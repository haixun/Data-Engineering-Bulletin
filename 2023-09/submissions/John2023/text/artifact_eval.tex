%\documentclass[sigplan, screen]{acmart}
%
%\settopmatter{printacmref=false} % Removes citation information below abstract
%\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
%\setcopyright{none}
%\pagestyle{plain} % removes running headers
%
%\usepackage{xspace}
%\usepackage{listings}
%\usepackage{hyperref}
%\usepackage{enumitem}
%\usepackage{xcolor}
%
%\definecolor{codegreen}{rgb}{0,0.6,0}
%\definecolor{codegray}{rgb}{0.5,0.5,0.5}
%\definecolor{codepurple}{rgb}{0.58,0,0.82}
%\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
%
%\lstdefinestyle{mystyle}{
%    backgroundcolor=\color{backcolour},   
%    commentstyle=\color{codegreen},
%    keywordstyle=\color{magenta},
%    numberstyle=\tiny\color{codegray},
%    stringstyle=\color{codepurple},
%    basicstyle=\ttfamily\footnotesize,
%    breakatwhitespace=false,         
%    breaklines=true,                 
%    captionpos=b,                    
%    keepspaces=true,                 
%    numbers=left,                    
%    numbersep=5pt,                  
%    showspaces=false,                
%    showstringspaces=false,
%    showtabs=false,                  
%    tabsize=2
%}
%
%\lstset{style=mystyle}
%
%\newcommand{\Hammer}{\emph{iQAN}\xspace}
%
%
%\begin{document}
%
%\title{Artifact Evaluation for \Hammer}
%% \special{papersize=8.5in,11in}
%\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% When adding this appendix to your paper, 
%% please remove above part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix
\section{Artifact Evaluation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Getting Started Guide}

\subsubsection{Availability}

The artifact is available on Zenodo (\url{https://doi.org/10.5281/zenodo.7322631}) and GitHub (\url{https://github.com/johnpzh/iQAN_AE}). Please note the artifact on GitHub does not contain the SIFT1M dataset because it exceeds the platform's file size limit. If using GitHub, please use the command to clone the repository into the local directory \verb|iQAN_AE|.
\begin{lstlisting}[language=bash]
git clone https://github.com/johnpzh/iQAN_AE.git
\end{lstlisting}

%
%\subsubsection{Remote Access}
%
%We also provide remote access to an Intel Xeon Phi machine (KNL) that is ready to run the artifact. We created three accounts named \verb|ppopp2023a|, \verb|ppopp2023b|, and \verb|ppopp2023c|. The password is \verb|ppopp2023AE|. Please use the following port number and address for login, and the username can be either one of those three.
%\begin{lstlisting}[language=bash]
%    ssh -Y -p 6180 rocco.cs.wm.edu -l ppopp2023a
%\end{lstlisting}
%
%The \verb|-Y| option enables X-forwarding to show figures remotely.
%
%After login, the artifact is under the home directory 
%\begin{lstlisting}[language=bash]
%    cd ~/iQAN_AE
%\end{lstlisting}
%
%If you use remote access, please directly go to step~\ref{subsubsec:build} to build the artifact and run the quick script.

\subsubsection{Hardware Environment}
The artifact is supposed to be run on an Intel CPU with at least 32 cores and support AVX2 intrinsics. It does not use hyperthreading.

To run the quick script, the artifact requires 2 GB disk space and 4 GB memory. To run all full scripts, it needs 300 GB disk space and 80 GB memory. The particular number of required space are shown in Table~\ref{tab:datasets}.

\subsubsection{Software Environment}
Some libraries and programs are required to build and run the artifact.

Libraries:
\begin{itemize}
    \item Boost C++. The artifact is tested on version 1.53. To install Boost C++ on Ubuntu, please use the command
    
\begin{lstlisting}[language=bash]
sudo apt install libboost-all-dev
\end{lstlisting}
    
    \item OpenMP. The artifact is tested on version 5.0.
    \item Matplotlib for plotting figures. The artifact is tested on version 3.1.2. To install Matplotlab in Python, please use the command
\begin{lstlisting}[language=bash]
python -m pip install -U matplotlib
\end{lstlisting}
\end{itemize}

Programs:
\begin{itemize}
    \item CMake for building the program (>= 3.9). The artifact is tested on 3.17.
    \item C++ Compiler, recommending Intel C++ Compiler (>= 19.0.1). The artifact is tested on GCC 4.8.5 and ICC 2021.4.0.
    \item Bash or Zsh for shell scripts.
    \item Python3 for Python scripts. The artifact is tested on 3.7.11.
\end{itemize}


\subsubsection{Build} \label{subsubsec:build}
A script is provided to build the artifact.

Under the project directory, please run this command to build the artifact:
\begin{lstlisting}[language=bash]
bash ./build.sh
\end{lstlisting}

If using the Intel ICC compiler, please run this command with the option \verb|icc|:
\begin{lstlisting}[language=bash]
bash ./build.sh icc
\end{lstlisting}

This command creates a directory \verb|cmake-build-release| and builds the program under it.


\subsubsection{Quick Run}
A quick script can run the artifact using the included dataset SIFT1M. Please note the script should be run under the build directory.
\begin{lstlisting}[language=bash]
cd cmake-build-release
bash ../scripts/run.quick.sh
\end{lstlisting}

This script runs \Hammer, NSG, and HNSW for six recall targets (Recall@100) 0.9, 0.95, 0.99, 0.995, 0.997, and 0.999 on the dataset SIFT1M. It takes about 30 minutes to finish depending on the machines (e.g., 1 hour on KNL). 
The script can generate a figure \verb|fig.quick.png| under the current directory \verb|cmake-build-release|. 
%One could use the following command to show the figure remotely. Please note the command may take one or two minutes to show the figure due to X-forwarding, and please make sure to use \verb|ssh -Y| to enable X-forwarding when logging in.
%\begin{lstlisting}[language=bash]
%xdg-open fig.quick.png
%\end{lstlisting}

The generated figure should reflect the same trend as Fig.~\ref{fig:eva_runtime_recall-1_KNL} in the paper, although the quick script only collects results for six recall targets. The figure provides a glimpse showing that \Hammer can achieve latency speedup over NSG and HNSW for given recall targets.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Step by Step Instructions}

\subsubsection{Prepare Datasets}

(If you use remote access, you do not need to prepare datasets as are ready to use.) The artifact is delivered with the included dataset SIFT1M, which is relatively smaller than other datasets used in the paper. 
Table~\ref{tab:ae_datasets} shows the sizes of all five datasets. 
% Besides SIFT1M, the artifact provides scripts to download four other datasets, including GIST1M, DEEP10M, SIFT100M, and DEEP100M. 
% For downloading a particular dataset, e.g., DEEP100M, please run the commands under the directory \verb|data/|:
% \begin{lstlisting}[language=bash]
    % bash ../scripts/get.deep100m.sh
    % \end{lstlisting}
% Please replace \verb|deep100m| in the command with other dataset names to download the corresponding dataset.
Besides SIFT1M, the artifact provides scripts to download five other datasets. To download a particular dataset, please run the commands under the directory \verb|data/|:
\begin{lstlisting}[language=bash]
cd data
bash ../scripts/get.xxx.sh
\end{lstlisting}
Here \verb|xxx| can be replaced by either \verb|gist1m|, \verb|deep1m|, \verb|deep10m|, \verb|sift100m|, or \verb|deep100m|.


For a given dataset, \Hammer uses a base file containing all data vectors, a query file containing all query vectors, a ground-truth file containing the real 100 nearest neighbors of all queries, and an NSG index file generated by using NSG implementation (\url{https://github.com/ZJULearning/nsg}). 
The NSG index can also be used by NSG searching method.
Additionally, the HNSW index is generated by using HNSWLib (\url{https://github.com/nmslib/hnswlib}), which is used by HNSW searching method.


\begin{table}[t]
    %\scriptsize
    \tiny
    \caption{Dataset sizes and running time.} \label{tab:ae_datasets}
    \begin{tabular}{c@{}r@{}r@{}r@{}r@{}r@{}r}
        \hline
        Datasets & \multicolumn{1}{c@{}}{SIFT1M} & \multicolumn{1}{c@{}}{GIST1M} & \multicolumn{1}{c@{}}{DEEP1M} & \multicolumn{1}{c@{}}{DEEP10M} & \multicolumn{1}{c@{}}{SIFT100M} & \multicolumn{1}{c@{}}{DEEP100M} \\ \hline
        base file (GB) & 0.5 & 3.6 & 0.3 & 3.7 & 49 & 37 \\
        NSG index (GB) & 0.1 & 0.08 & 0.1 & 1.7 & 19 & 14 \\
        HNSW index (GB) & 0.6 & 3.8 & 0.5 & 5 & 62 & 50 \\
        query file (MB) & 5 & 3.7 & 3.8 & 3.8 & 5 & 3.8 \\
        groundtruth file (MB) & 7.7 & 0.7 & 7.7 & 7.7 & 7.7 & 7.7 \\
        running time (hr) & 3.5 & 12 & 5 & 15 & 24 & 30 \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Run Scripts for Latency-vs-Recall Curves}

After a particular dataset is ready, one can run \Hammer, NSG, and HNSW upon the dataset by using the dataset's corresponding script provided by the artifact. 
% For example, for SIFT1M, please run
% \begin{lstlisting}[language=bash]
    % cd cmake-build-release
    % bash ../scripts/run.sift1m.sh
    % \end{lstlisting}

\begin{lstlisting}[language=bash]
cd cmake-build-release
bash ../scripts/run.xxx.sh
\end{lstlisting}
Here \verb|xxx| can be replaced by either \verb|sift1m|, \verb|gist1m|, \verb|deep10m|, \verb|sift100m|, or \verb|deep100m|.

This script runs \Hammer, NSG, and HNSW for nineteen recall targets from 0.9 to 0.999 on the corresponding dataset. The estimated running time of datasets is shown in Table~\ref{tab:datasets}. 
The script can generate a figure \verb|fig.xxx.png|, under the build directory \verb|cmake-build-release|. 
The figure shows the latency-vs-recall curves for \Hammer, NSG, and HNSW, which are the main results of the evaluation in the paper (Fig.~\ref{fig:eva_runtime_recall-1_KNL}). 
The shorter latency is better.
% They should show that \Hammer is able to achieve better latency performance than NSG and HNSW for most given recall targets, which is also the main claim in the paper. For a given recall target, the shorter latency is better.

\subsubsection{Run Scripts for Speedup}
To get the speedup results, please run
\begin{lstlisting}[language=bash]
cd cmake-build-release
bash ../scripts/run.speedup_xxx.sh
\end{lstlisting}
Here \verb|xxx| can be replaced by \verb|gist1m|, \verb|deep10m|, \verb|sift100m|, or \verb|deep100m|.

This script runs \Hammer recall targets 0.99, 0.995, and 0.999 on the corresponding dataset using 1 to 64 threads. 
The script can generate a figure \verb|fig.speedup.xxx.png| under the build directory \verb|cmake-build-release|. The figure shows \Hammer's speedup over its 1-thread latency (Fig.~\ref{fig:eval-speedup}). The larger speedup is better.

\subsubsection{Run Scripts for Graph-Size Scalability}
To get the graph-size scalability results, please run
\begin{lstlisting}[language=bash]
cd cmake-build-release
# First, download dataset DEEP1M if not yet
bash ../scripts/get.deep1m.sh 
bash ../scripts/run.graph_scale.sh
\end{lstlisting}

This script runs HNSW, NSG, and \Hammer for recall targets 0.9, 0.99, and 0.999 on the datasets DEEP1M, DEEP10M, and DEEP100M. 
A figure \verb|fig.graph_scale.png| will be generated under the build directory \verb|cmake-build-release|. The figure shows \Hammer's scalability over large graphs (Fig.~\ref{fig:eva_datasets_size_scale}). 


% \subsubsection{Explain Results}

% For other results or claims, the artifact does not provide direct scripts for several reasons. 
% First, other tests just use different settings from the main results.
% Second, it usually takes a long time to finish testing, which is not practical for artifact evaluation reviewing.
% Third, we try to keep the artifact as straightforward as possible and also be able to show the main results at the same time.

% % \subsubsection{Tune the Parameters}

% % As shown in \verb|sh.iqan_quick| \verb|.sh| and other \Hammer scripts, one can tune the program's behavior via the number of threads and the queue capacity ($L$). A larger $L$ allows \Hammer (and NSG) to explore more vertices before convergence, which enables higher recall targets but also longer search latency.

\subsubsection{Programs, Parameters, and Scripts}

The iQAN mainly uses the following files for its implementation for relatively small datasets such as SIFT1M, GIST1M, and DEEP10M, 
\begin{lstlisting}[language=bash]
app/PSS_v5_distance_threshold_profiling.cpp
core/Searching.202102022027.PSS_v5.dist_thresh.profiling.cpp
\end{lstlisting}

For large datasets such as SIFT100M and DEEP100M, iQAN mainly uses:
\begin{lstlisting}[language=bash]
app/PSS_v5_LG_distance_threshold_profiling.cpp
core/Searching.202102031939.PSS_v5.large_graph.dist_thresh.profiling.cpp
\end{lstlisting}

The two versions use the same search algorithm, while the small-dataset version uses a flatted graph format to improve the data locality. Correspondingly, NSG also use the same format for small dataset. For large datasets, the flatted format causes too large memory footprint. In that case, iQAN and NSG uses the standard graph format.

The iQAN takes the following parameters:
\begin{lstlisting}[language=bash]
PSS_v5_distance_threshold_profiling 
    <data_file> <query_file> <nsg_file> 
    <K> <place_holder> <true_NN_file> <num_threads> 
    <L_low> <L_up> <L_step> 
    <place_holder> <place_holder> <place_holder> 
    <X_low> <X_up> <X_step>
    <place_holder> <place_holder> <place_holder>
\end{lstlisting}

% \verb|PSS_v5_distance_threshold_profiling <data_file> <query_file> <nsg_file> <K> <place_holder> <true_NN_file> <num_threads> <L_low> <L_up> <L_step> <place_holder> <place_holder> <place_holder> <X_low> <X_up> <X_step> <place_holder> <place_holder> <place_holder>|

\begin{itemize}[leftmargin=0.1in]
    \item \verb|data_file|: the input file containing all vectors, such as \verb|sift_base.fvecs|
    \item \verb|query_file|: the input query file containing all query vectors, such as \verb|sift_query.fvecs|
    \item \verb|nsg_file|: the input NSG index file, such as \verb|sift.nsg|
    \item \verb|K|: the value \verb|K| as in Top-K, which is set as 100 for current implementation
    \item \verb|true_NN_file|: the input file contains the real top-100 neighbors' IDs for all queries, used for computing the recall
    \item \verb|num_threads|: the number of threads
    \item \verb|L_low|, \verb|L_up|, and \verb|L_step|: the settings for the capacity of queues (the value \verb|L|). A larger \verb|L| uses larger queues, which can improve the search accuracy but also increase the search latency. Here the program will run multiple times with different values of \verb|L| from \verb|L_low| to \verb|L_up| (inclusive) with step \verb|L_step|. For example, a setting of (100, 102, 1) lets the program run with \verb|L| as 100, 101, and 102. A user can then choose the expected output that satisfies the accuracy target and also achieves the shortest latency.
    \item \verb|X_low|, \verb|X_up|, and \verb|X_step|: the settings for the synchronization frequency (the value \verb|X|). A larger \verb|X| has less frequent synchronization (merging local queues) among threads, which can reduce the synchronization overhead, improve the search accuracy but also increase the distance computation overhead. Similar with \verb|L|, here the program will run multiple times with different values of \verb|X| from \verb|X_low| to \verb|X_up| (inclusive) with step \verb|X_step|.
    
\end{itemize}
Under the directory \verb|scripts/|, the shell script \verb|sh.iqan_xxx.|\\
\verb|sh| drives the program. First, it uses the python script \verb|test51.|\\
\verb|PSS_v5_dt_profiling_ranged_L.py| to provide input and format the output. Second, it uses the python script \verb|output_|\\
\verb|find_runtime_above_presicion.py| to select the output of best performance. The final output can then be used to generate figures by the script \verb|run.xxx.sh|.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% When adding this appendix to your paper, 
%% please remove below part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    
%\end{document}