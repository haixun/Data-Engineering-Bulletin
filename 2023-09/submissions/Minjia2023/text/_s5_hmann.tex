\section{HM-ANN: Efficient Billion-Point Vector Search via Heterogeneous Memory}
\label{minjia_sec:hmann}

This section describes a large-scale vector search solution built on top of Heterogeneous Memory. The HNSW index is modified to make it HM-aware, and the search algorithm is also modified to make the search more efficient. With these modifications, this solution enables fast and highly accurate billion-scale ANNS on HM.

\subsection{Design of \name}

The design of \algoname generalizes HNSW, whose hierarchical structure naturally fits into HM. Elements in the upper layers consume a small portion of the memory, making them good candidates to be placed in fast memory (small capacity); The bottom-most layer has all the elements and has the largest memory consumption, which makes it suitable to be placed in slow memory. Unlike HNSW, where the majority of search happens in the bottom-most layer, elements in the upper layers now have faster access speed, so it is a reasonable strategy to increase the access frequency of the upper layers. On the other hand, since accessing L0 is slower, it is preferable to have only a small portion of it to be accessed by each query. The key idea of \algoname is, therefore, to build high-quality upper layers and make most memory accesses happen in fast memory, in order to provide better navigation for search at L0 and reduce memory accesses in slow memory.

\textbf{Notations.} In the rest of the paper, we let $V$ denote the dataset with $N=|V|$ to build the graph; we refer the graph in the layer $i \in \{0, 1,...,l\}$ of \name as $G_i = (V_i, E_i)$ where $V_i$ is the vertex set and $E_i$ is the edge set. We refer $N_i$ as the number of elements in the layer $i$, and we have $N_i=|V_i|$. Because L0 contains all the elements in the database, we have $V_0 = V$ and $N_0 = N$. Based on the hierarchical structure of \name, we have $ V_i \subsetneq V_{i-1}$.  Similar to the existing effort~\cite{hnsw}, we introduce $M_i$ as the maximum number of established connections for each point $v$ in the layer $i$. For $v \in V$, we let $D(v) $ denote the degree of node $v$, and $D(v) = \sum_{u \in V} m(v,u)$ where $m(v,u)=1$ if there exists a link between node $v$ and node $u$.

\subsubsection{HM-aware Index Construction via Top-Down Insertions and Bottom-up Promotions}

To make the ANNS index aware of HM architecture, we generalize the HNSW construction algorithm to include two phases: a top-down insertion phase and a bottom-up promotion phase.

\textbf{Top-down insertions.}
The top-down insertion phase is the same as HNSW, where we incrementally build a hierarchical graph by iteratively inserting each vector $v$ in $V$ as a node in $G$. Each node will generate up to $M$ (i.e., the neighbor degree) out-going edges. Among those, $M - 1$ are short-range edges, which connect $v$ to its $M - 1$ nearest neighbors according to their pair-wise Euclidean distance to $v$. The rest is a long-range edge that connects $v$ to a randomly picked node, which may connect other isolated clusters. It is theoretically justified that graphs (e.g., L0) constructed by inserting these two types of edges guarantee to have the small world properties~\cite{nsg,small-world-dynamics,hnsw}.

\textbf{Bottom-up promotions.} The goal of the second phase is to build a high-quality projection of L0 elements into the layer $1$ (L1), such that the search in L0 can find the true nearest neighbors of the query with only a few hops. Ideally, \algoname wants to achieve the goal that performing 1-greedy search in L0 is sufficient to achieve high recall, so that the slowdown caused by accessing the slow memory is minimal. A straightforward way to project the L0 elements into L1 is to randomly select a subset of elements in L0 to be L1, similar to what HNSW already does to build upper layers. However, we observe that such an approach leads to poor index quality. As a result, many searches end up happening in L0 (slow memory), causing long search latency. 

\algoname uses a \textit{high-degree promotion strategy}. This strategy promotes elements with the highest degree in L0 into L1. 
From the layer $i$ ($i \ge 2$) to $i+1$, \algoname promotes high-degree nodes to the upper layer with a promotion rate of $1/M$, where $M$ is the maximum number of neighbors for each element (i.e., $M_i = M$, where $i=2...l$). A similar promotion rate setting is used in HNSW~\cite{hnsw} and typical skip list~\cite{skip_list}.
\algoname increases search quality in $L1$ by promoting more nodes from $L0$ to $L1$ and setting the maximum number of neighbors for each element in L1 to $2 \times M$ (i.e., $M_1=2 \times M$).  The number of nodes in upper layers ($N_i$, where $i=1..l$) is decided by available fast memory space. 

The high-degree promotion strategy is based on the following observation. The hub nodes of the graph at L0 are those nodes with a large number of connections (i.e., high degree). In the small world navigation algorithm, a higher degree node provides better navigability~\cite{swg}. Most of the shortest paths between nodes flow through hubs. In other words, the average length of the navigation path (i.e., number of hops) is the smallest, when the adjacent node with the highest degree is selected as the next hop. By promoting the high-degree nodes, the resulting L1 layer allows \algoname to effectively reduce the number of search steps in L0, compared with the random promotion strategy.

\subsubsection{\name Graph Search Algorithm}
\label{minjia_sec:searching}

\textbf{Fast memory search.} The search in fast memory begins at the entry point in the top layer and then performs a 1-greedy search from the top layer to layer 2, which is the same as in HNSW. To narrow down the search space in L0, \name performs the search in L1 with a search budget controlled by $efSearch_{L1}$. $efSearch_{L1}$ defines the size of dynamic candidate list in L1. Those candidates in the list are used as entry points for search in L0 (HNSW uses just one entry point), in order to improve search quality in L0. 

\textbf{Parallel L0 search.} In L0, \name evenly partitions the candidates from searching L1 and uses them as entry points to perform \emph{parallel multi-start 1-greedy search} with  $Thr$ threads in parallel. The top candidates from each search are collected to find the best candidates. Parallel search makes the best use of memory bandwidth and improves search quality without increasing search time. $Thr$ is determined by peak memory bandwidth constrained by hardware divided by memory bandwidth consumption by one thread, which is easy to calculate. 

Different from the SSD-based ANNS~\cite{diskann,grip}, the data in slow memory in \name can be directly accessed by processors, and there is no duplication between fast and slow memories. However, due to high latency and low bandwidth of slow memory, \name should still make memory accesses in fast memory as many as possible. \name implements a software-managed cache in fast memory to prefetch data from slow memory to fast memory before the memory access happens. In particular, \name reserves a space in fast memory ($\sim$2 GB) called \textit{migration space}. When searching L1, \name asynchronously copies neighbor elements of those candidates in $efSearch_{L1}$ and the neighbor elements' connections in L1 from slow memory to the migration space in fast memory. When the search in L0 happens, there is already a portion of to-be-accessed data placed in fast memory, which leads to shorter query time.
