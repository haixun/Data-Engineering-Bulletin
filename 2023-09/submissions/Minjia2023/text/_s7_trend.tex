\section{Research Opportunities}
\label{minjia_sec:future}

In this paper, we have concentrated on the computational and memory efficiency of large-scale vector search. Our \Hammer and HM-ANN designs achieve exceptional performance. However, large-scale vector search still requires further attention, and we expect that additional improvements can be made. This section explores the challenges and research opportunities associated with vector search.

\textbf{High-performance vector search through hierarchical parallelism.} Inference demand varies across different applications and scenarios, with some being latency-critical, and others being latency-sensitive or throughput-oriented. Additionally, the hardware resources available to each application may also vary significantly. To meet these diverse requirements and make efficient use of all computational resources, we can combine different parallelism approaches:
\begin{itemize}
    \item Distributed vector search. Libraries such as Milvus~\cite{milvus} allow the development of distributed versions of vector search systems using a cluster.
    \item Inter-query parallelism. Multithreading enables the use of multiple cores to improve the throughput of answering user requests.
    \item Intra-query parallelism. Methods such as \Hammer enable the system to speed up individual queries by leveraging the aggregated computational capacity of multiple cores.
\end{itemize}

These techniques can be combined hierarchically to meet given latency, throughput, and cost objectives. The research opportunity here is: \emph{Build systems for large-scale vector search that fully exploit parallelism at different levels, providing millisecond-level latency, high throughput, and low hardware cost.}
Furthermore, while techniques such as \Hammer optimize search efficiency by modifying the graph traversal process to leverage intra-query parallelism, the underlying index has not been specifically designed to take advantage of intra-query parallelism. Intuitively, a different index structure could achieve better search efficiency under intra-query parallelism if it naturally helps avoid redundant computations across different threads and also leads to better data locality. The research opportunity here is: \emph{Design vector search indices that maximize search efficiency through both inter- and intra-query parallelism. } 

\textbf{Highly concurrent vector search with addition and deletion.} Most existing vector search systems build indices offline and become read-only once deployed. In some applications, data capture may occur more frequently than query processing, or the application may need to index continuous data streams while serving query requests. In these cases, the index organization should be optimized for addition and update in addition to query performance. The complexity arises with concurrent read and update operations (e.g., addition, deletion), as it is challenging to achieve both correctness and speed simultaneously. Concurrent updates and reads can lead to data races, making it difficult to ensure correctness on shared-memory architectures. Lock-based synchronization can be used to coordinate access to shared-memory data, but while using one lock for the entire index simplifies reasoning about correctness, it also leads to serialized execution, severely impacting scalability. Another solution is to use fine-grained locking, where multiple locks are associated with the index. However, fine-grained locking increases the complexity of operations that access shared data, leading to issues such as deadlock, atomicity violation, and high locking overhead. The research opportunity here is: \emph{Build a highly concurrent vector search system that supports robust addition and deletion with high search efficiency}. 

\textbf{Automating index construction for vector search.} Several advances have been made in support of large-scale vector search, introducing novel algorithms and system optimizations~\cite{grip,diskann,hm-ann,spann}. However, applying these methods to large-scale datasets still requires a significant amount of engineering effort specific to the data, hardware environment, and performance objectives. For instance, deploying a large dataset to a given cloud VM requires a careful selection of vector search indices, with NVMe/SSD/remote storage, the number of cores to use, and multiple index-specific parameters. Correctly choosing the algorithm and tuning the parameters can deliver significant improvements in search performance but also depend on strong system expertise. Therefore, automating the selection and construction of vector search indices would help alleviate the burden on deployment engineers, but it also requires navigating a complex space of choices that grows exponentially with different algorithm choices, each with its own trade-offs, and data sizes and hardware resources. The research opportunity here is: \emph{Build a framework and optimization algorithm that automatically finds the optimal index construction strategy for a given dataset to deliver fast search speed with low cost.}

