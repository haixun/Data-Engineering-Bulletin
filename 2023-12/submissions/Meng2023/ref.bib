% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{fan2023recommender,
  title={Recommender systems in the era of large language models (llms)},
  author={Fan, Wenqi and Zhao, Zihuai and Li, Jiatong and Liu, Yunqing and Mei, Xiaowei and Wang, Yiqi and Tang, Jiliang and Li, Qing},
  journal={arXiv preprint arXiv:2307.02046},
  year={2023}
}

@article{wu2023survey,
  title={A Survey on Large Language Models for Recommendation},
  author={Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and others},
  journal={arXiv preprint arXiv:2305.19860},
  year={2023}
}

@article{lin2023can,
  title={How Can Recommender Systems Benefit from Large Language Models: A Survey},
  author={Lin, Jianghao and Dai, Xinyi and Xi, Yunjia and Liu, Weiwen and Chen, Bo and Li, Xiangyang and Zhu, Chenxu and Guo, Huifeng and Yu, Yong and Tang, Ruiming and others},
  journal={arXiv preprint arXiv:2306.05817},
  year={2023}
}

@inproceedings{li2021survey,
  title={A survey on representation learning for user modeling},
  author={Li, Sheng and Zhao, Handong},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={4997--5003},
  year={2021}
}

@article{he2023survey,
  title={A Survey on User Behavior Modeling in Recommender Systems},
  author={He, Zhicheng and Liu, Weiwen and Guo, Wei and Qin, Jiarui and Zhang, Yingxue and Hu, Yaochen and Tang, Ruiming},
  journal={arXiv preprint arXiv:2302.11087},
  year={2023}
}


@article{chen2023palr,
  title={PALR: Personalization Aware LLMs for Recommendation},
  author={Chen, Zheng},
  journal={arXiv preprint arXiv:2305.07622},
  year={2023}
}

@article{liu2023genre,
  title={A First Look at LLM-Powered Generative News Recommendation},
  author={Liu, Qijiong and Chen, Nuo and Sakai, Tetsuya and Wu, Xiao-Ming},
  journal={arXiv preprint arXiv:2305.06566},
  year={2023}
}

@article{liu2023llmrec,
  title={Llmrec: Benchmarking large language models on recommendation task},
  author={Liu, Junling and Liu, Chao and Zhou, Peilin and Ye, Qichen and Chong, Dading and Zhou, Kang and Xie, Yueqi and Cao, Yuwei and Wang, Shoujin and You, Chenyu and others},
  journal={arXiv preprint arXiv:2308.12241},
  year={2023}
}

@article{kang2023llms,
  title={Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction},
  author={Kang, Wang-Cheng and Ni, Jianmo and Mehta, Nikhil and Sathiamoorthy, Maheswaran and Hong, Lichan and Chi, Ed and Cheng, Derek Zhiyuan},
  journal={arXiv preprint arXiv:2305.06474},
  year={2023}
}

@article{salemi2023lamp,
  title={LaMP: When Large Language Models Meet Personalization},
  author={Salemi, Alireza and Mysore, Sheshera and Bendersky, Michael and Zamani, Hamed},
  journal={arXiv preprint arXiv:2304.11406},
  year={2023}
}

@article{mozes2023use,
  title={Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities},
  author={Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D},
  journal={arXiv preprint arXiv:2308.12833},
  year={2023}
}

@article{lipretrained,
  title={Pretrained Language Models for Text Generation: A Survey},
  author={Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Wen, Ji-Rong}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others}
}


@article{raffel2020t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2023llamaadapter,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}

@article{feng2023pretraining,
  title={From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models},
  author={Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.08283},
  year={2023}
}

@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{farid2018user,
  title={User profiling approaches, modeling, and personalization},
  author={Farid, Marina and Elgohary, Rania and Moawad, Ibrahim and Roushdy, Mohamed},
  booktitle={Proceedings of the 11th International Conference on Informatics \& Systems (INFOS 2018)},
  year={2018}
}


@article{goldberg1992using,
  title={Using collaborative filtering to weave an information tapestry},
  author={Goldberg, David and Nichols, David and Oki, Brian M and Terry, Douglas},
  journal={Communications of the ACM},
  volume={35},
  number={12},
  pages={61--70},
  year={1992},
  publisher={ACM New York, NY, USA}
}

@article{koren2009matrix,
  title={Matrix factorization techniques for recommender systems},
  author={Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  journal={Computer},
  volume={42},
  number={8},
  pages={30--37},
  year={2009},
  publisher={IEEE}
}

@inproceedings{singh2008relational,
  title={Relational learning via collective matrix factorization},
  author={Singh, Ajit P and Gordon, Geoffrey J},
  booktitle={Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={650--658},
  year={2008}
}

@inproceedings{tang2015line,
  title={Line: Large-scale information network embedding},
  author={Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
  booktitle={Proceedings of the 24th international conference on world wide web},
  pages={1067--1077},
  year={2015}
}

@inproceedings{perozzi2014deepwalk,
  title={Deepwalk: Online learning of social representations},
  author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={701--710},
  year={2014}
}

@inproceedings{grover2016node2vec,
  title={node2vec: Scalable feature learning for networks},
  author={Grover, Aditya and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={855--864},
  year={2016}
}

@inproceedings{dong2017metapath2vec,
  title={metapath2vec: Scalable representation learning for heterogeneous networks},
  author={Dong, Yuxiao and Chawla, Nitesh V and Swami, Ananthram},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={135--144},
  year={2017}
}

@inproceedings{kipf2016semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{velivckovic2018graph,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{hu2020heterogeneous,
  title={Heterogeneous graph transformer},
  author={Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Sun, Yizhou},
  booktitle={Proceedings of the web conference 2020},
  pages={2704--2710},
  year={2020}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{fan2020graph,
  title={A graph neural network framework for social recommendations},
  author={Fan, Wenqi and Ma, Yao and Li, Qing and Wang, Jianping and Cai, Guoyong and Tang, Jiliang and Yin, Dawei},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={34},
  number={5},
  pages={2033--2047},
  year={2020},
  publisher={IEEE}
}

@inproceedings{ying2018pinsage,
  title={Graph convolutional neural networks for web-scale recommender systems},
  author={Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L and Leskovec, Jure},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={974--983},
  year={2018}
}

@article{alghamdi2015survey,
  title={A Survey of Topic Modeling in Text Mining},
  author={Alghamdi, Rubayyi and Alfalqi, Khalid},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={6},
  number={1},
  year={2015},
  publisher={Science and Information (SAI) Organization Limited}
}

@inproceedings{hu2017user,
  title={A user profile modeling method based on Word2Vec},
  author={Hu, Jianqiao and Jin, Feng and Zhang, Guigang and Wang, Jian and Yang, Yi},
  booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
  pages={410--414},
  year={2017},
  organization={IEEE}
}

@inproceedings{sun2019bert4rec,
  title={BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer},
  author={Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
  booktitle={Proceedings of the 28th ACM international conference on information and knowledge management},
  pages={1441--1450},
  year={2019}
}

@inproceedings{feng2022heterogeneity,
  title={Heterogeneity-aware twitter bot detection with relational graph transformers},
  author={Feng, Shangbin and Tan, Zhaoxuan and Li, Rui and Luo, Minnan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={4},
  pages={3977--3985},
  year={2022}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@inproceedings{chien2021node,
  title={Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction},
  author={Chien, Eli and Chang, Wei-Cheng and Hsieh, Cho-Jui and Yu, Hsiang-Fu and Zhang, Jiong and Milenkovic, Olgica and Dhillon, Inderjit S},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{yin2023heterogeneous,
  title={Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM},
  author={Yin, Bin and Xie, Junjie and Qin, Yu and Ding, Zixiang and Feng, Zhichao and Li, Xiang and Lin, Wei},
  journal={arXiv preprint arXiv:2308.03333},
  year={2023}
}


@article{gao2023chat,
  title={Chat-rec: Towards interactive and explainable llms-augmented recommender system},
  author={Gao, Yunfan and Sheng, Tao and Xiang, Youlin and Xiong, Yun and Wang, Haofen and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2303.14524},
  year={2023}
}

@article{runfeng2023lkpnr,
  title={LKPNR: LLM and KG for Personalized News Recommendation Framework},
  author={Runfeng, Xie and Xiangyang, Cui and Zhou, Yan and Xin, Wang and Zhanwei, Xuan and Kai, Zhang and others},
  journal={arXiv preprint arXiv:2308.12028},
  year={2023}
}

@article{chen2023exploring,
  title={Exploring the potential of large language models (llms) in learning on graphs},
  author={Chen, Zhikai and Mao, Haitao and Li, Hang and Jin, Wei and Wen, Hongzhi and Wei, Xiaochi and Wang, Shuaiqiang and Yin, Dawei and Fan, Wenqi and Liu, Hui and others},
  journal={arXiv preprint arXiv:2307.03393},
  year={2023}
}

@article{labonne2023spam,
  title={Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection},
  author={Labonne, Maxime and Moran, Sean},
  journal={arXiv preprint arXiv:2304.01238},
  year={2023}
}

@article{li2023teach,
  title={Teach LLMs to Personalize--An Approach inspired by Writing Education},
  author={Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Wang, Yaqing and Hombaiah, Spurthi Amba and Liang, Yi and Bendersky, Michael},
  journal={arXiv preprint arXiv:2308.07968},
  year={2023}
}

@article{chen2023netgpt,
  title={NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services},
  author={Chen, Yuxuan and Li, Rongpeng and Zhao, Zhifeng and Peng, Chenghui and Wu, Jianjun and Hossain, Ekram and Zhang, Honggang},
  journal={arXiv preprint arXiv:2307.06148},
  year={2023}
}

@article{wang2023can,
  title={Can Language Models Solve Graph Problems in Natural Language?},
  author={Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.10037},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{li2023exploring,
  title={Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights},
  author={Li, Ruyu and Deng, Wenhao and Cheng, Yu and Yuan, Zheng and Zhang, Jiaqi and Yuan, Fajie},
  journal={arXiv preprint arXiv:2305.11700},
  year={2023}
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis}
}

@article{pu2023summarization,
  title={Summarization is (Almost) Dead},
  author={Pu, Xiao and Gao, Mingqi and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2309.09558},
  year={2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{page1998pagerank,
  title={The PageRank Citation Ranking: Bringing Order to the Web},
  author={PAGE, L},
  booktitle={Proc. of the 7\^{}< th> WWW Conf., 1998},
  year={1998}
}

@article{chen2023large,
  title={When large language models meet personalization: Perspectives of challenges and opportunities},
  author={Chen, Jin and Liu, Zheng and Huang, Xu and Wu, Chenwang and Liu, Qi and Jiang, Gangwei and Pu, Yuanhao and Lei, Yuxuan and Chen, Xiaolong and Wang, Xingmei and others},
  journal={arXiv preprint arXiv:2307.16376},
  year={2023}
}

@article{schafer2001commerce,
  title={E-commerce recommendation applications},
  author={Schafer, J Ben and Konstan, Joseph A and Riedl, John},
  journal={Data mining and knowledge discovery},
  volume={5},
  pages={115--153},
  year={2001},
  publisher={Springer}
}

@inproceedings{zhao2015commerce,
  title={E-commerce recommendation with personalized promotion},
  author={Zhao, Qi and Zhang, Yi and Friedman, Daniel and Tan, Fangfang},
  booktitle={Proceedings of the 9th ACM Conference on Recommender Systems},
  pages={219--226},
  year={2015}
}

@inproceedings{sarwar2000analysis,
  title={Analysis of recommendation algorithms for e-commerce},
  author={Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
  booktitle={Proceedings of the 2nd ACM Conference on Electronic Commerce},
  pages={158--167},
  year={2000}
}

@inproceedings{berkovsky2005entertainment,
  title={Entertainment personalization mechanism through cross-domain user modeling},
  author={Berkovsky, Shlomo and Kuflik, Tsvi and Ricci, Francesco},
  booktitle={International Conference on Intelligent Technologies for Interactive Entertainment},
  pages={215--219},
  year={2005},
  organization={Springer}
}

@inproceedings{abel2011analyzing,
  title={Analyzing user modeling on twitter for personalized news recommendations},
  author={Abel, Fabian and Gao, Qi and Houben, Geert-Jan and Tao, Ke},
  booktitle={User Modeling, Adaption and Personalization: 19th International Conference, UMAP 2011, Girona, Spain, July 11-15, 2011. Proceedings 19},
  pages={1--12},
  year={2011},
  organization={Springer}
}

@article{christensen2011entertainment,
  title={Entertainment recommender systems for group of users},
  author={Christensen, Ingrid A and Schiaffino, Silvia},
  journal={Expert systems with applications},
  volume={38},
  number={11},
  pages={14127--14135},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{natkin2006user,
  title={User model in multiplayer mixed reality entertainment applications},
  author={Natkin, St{\'e}phane and Yan, Chen},
  booktitle={Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology},
  pages={74--es},
  year={2006}
}

@inproceedings{tao2012tums,
  title={Tums: twitter-based user modeling service},
  author={Tao, Ke and Abel, Fabian and Gao, Qi and Houben, Geert-Jan},
  booktitle={The Semantic Web: ESWC 2011 Workshops: ESWC 2011 Workshops, Heraklion, Greece, May 29-30, 2011, Revised Selected Papers 8},
  pages={269--283},
  year={2012},
  organization={Springer}
}

@inproceedings{abel2013twitter,
  title={Twitter-based user modeling for news recommendations},
  author={Abel, Fabian and Gao, Qi and Houben, Geert-Jan and Tao, Ke},
  booktitle={Twenty-Third International Joint Conference on Artificial Intelligence},
  year={2013},
  organization={Citeseer}
}

@article{shi2016survey,
  title={A survey of heterogeneous information network analysis},
  author={Shi, Chuan and Li, Yitong and Zhang, Jiawei and Sun, Yizhou and Philip, S Yu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={1},
  pages={17--37},
  year={2016},
  publisher={IEEE}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{sun2023head,
  title={Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? AKA Will LLMs Replace Knowledge Graphs?},
  author={Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
  journal={arXiv preprint arXiv:2308.10168},
  year={2023}
}

@article{pan2023unifying,
  title={Unifying Large Language Models and Knowledge Graphs: A Roadmap},
  author={Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  journal={arXiv preprint arXiv:2306.08302},
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{fang2023chatgpt,
  title={ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection},
  author={Fang, Yihao and Li, Xianzhi and Thomas, Stephen W and Zhu, Xiaodan},
  journal={arXiv preprint arXiv:2308.13517},
  year={2023}
}

@article{liu2023chatgpt,
  title={Is chatgpt a good recommender? a preliminary study},
  author={Liu, Junling and Liu, Chao and Lv, Renjie and Zhou, Kang and Zhang, Yan},
  journal={arXiv preprint arXiv:2304.10149},
  year={2023}
}




@article{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}

@article{zhang2018network,
  title={Network representation learning: A survey},
  author={Zhang, Daokun and Yin, Jie and Zhu, Xingquan and Zhang, Chengqi},
  journal={IEEE transactions on Big Data},
  volume={6},
  number={1},
  pages={3--28},
  year={2018},
  publisher={IEEE}
}

@article{feng2022twibot,
  title={TwiBot-22: Towards graph-based Twitter bot detection},
  author={Feng, Shangbin and Tan, Zhaoxuan and Wan, Herun and Wang, Ningnan and Chen, Zilong and Zhang, Binchi and Zheng, Qinghua and Zhang, Wenqian and Lei, Zhenyu and Yang, Shujie and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35254--35269},
  year={2022}
}

@inproceedings{chen2019semi,
  title={Semi-supervised User Profiling with Heterogeneous Graph Attention Networks.},
  author={Chen, Weijian and Gu, Yulong and Ren, Zhaochun and He, Xiangnan and Xie, Hongtao and Guo, Tong and Yin, Dawei and Zhang, Yongdong},
  booktitle={IJCAI},
  volume={19},
  pages={2116--2122},
  year={2019}
}

@inproceedings{yan2021relation,
  title={Relation-aware Heterogeneous Graph for User Profiling},
  author={Yan, Qilong and Zhang, Yufeng and Liu, Qiang and Wu, Shu and Wang, Liang},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={3573--3577},
  year={2021}
}

@inproceedings{yu2020identifying,
  title={Identifying referential intention with heterogeneous contexts},
  author={Yu, Wenhao and Yu, Mengxia and Zhao, Tong and Jiang, Meng},
  booktitle={Proceedings of The Web Conference 2020},
  pages={962--972},
  year={2020}
}

@inproceedings{wang2020calendar,
  title={Calendar graph neural networks for modeling time structures in spatiotemporal user behaviors},
  author={Wang, Daheng and Jiang, Meng and Syed, Munira and Conway, Oliver and Juneja, Vishal and Subramanian, Sriram and Chawla, Nitesh V},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2581--2589},
  year={2020}
}

@inproceedings{fan2019graph,
  title={Graph neural networks for social recommendation},
  author={Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  booktitle={The world wide web conference},
  pages={417--426},
  year={2019}
}

@inproceedings{he2020lightgcn,
  title={Lightgcn: Simplifying and powering graph convolution network for recommendation},
  author={He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, Yongdong and Wang, Meng},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={639--648},
  year={2020}
}

@inproceedings{settles2008active,
  title={Active learning with real annotation costs},
  author={Settles, Burr and Craven, Mark and Friedland, Lewis},
  booktitle={Proceedings of the NIPS workshop on cost-sensitive learning},
  volume={1},
  year={2008},
  organization={Vancouver, CA:}
}

@article{leong2009text,
  title={Text-based over-representation analysis of microarray gene lists with annotation bias},
  author={Leong, Hui Sun and Kipling, David},
  journal={Nucleic acids research},
  volume={37},
  number={11},
  pages={e79--e79},
  year={2009},
  publisher={Oxford University Press}
}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{goertzel2014artificial,
  title={Artificial general intelligence: concept, state of the art, and future prospects},
  author={Goertzel, Ben},
  journal={Journal of Artificial General Intelligence},
  volume={5},
  number={1},
  pages={1},
  year={2014},
  publisher={De Gruyter Poland}
}

@article{dumais2004latent,
  title={Latent semantic analysis},
  author={Dumais, Susan T},
  journal={Annual Review of Information Science and Technology (ARIST)},
  volume={38},
  pages={189--230},
  year={2004}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}

@incollection{schafer2007collaborative,
  title={Collaborative filtering recommender systems},
  author={Schafer, J Ben and Frankowski, Dan and Herlocker, Jon and Sen, Shilad},
  booktitle={The adaptive web: methods and strategies of web personalization},
  pages={291--324},
  publisher={Springer}
}

@article{lee2000algorithms,
  title={Algorithms for non-negative matrix factorization},
  author={Lee, Daniel and Seung, H Sebastian},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{wang2022language,
  title={What language model architecture and pretraining objective works best for zero-shot generalization?},
  author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={22964--22984},
  year={2022},
  organization={PMLR}
}

@article{yao2023beyond,
  title={Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models},
  author={Yao, Yao and Li, Zuchao and Zhao, Hai},
  journal={arXiv preprint arXiv:2305.16582},
  year={2023}
}

@article{besta2023graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  journal={arXiv preprint arXiv:2308.09687},
  year={2023}
}

@article{yang2023large,
  title={Large language models as optimizers},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={arXiv preprint arXiv:2309.03409},
  year={2023}
}

@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}

@article{kosinski2023theory,
  title={Theory of mind may have spontaneously emerged in large language models},
  author={Kosinski, Michal},
  journal={arXiv preprint arXiv:2302.02083},
  year={2023}
}

@article{wang2023survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2308.11432},
  year={2023}
}

@article{xi2023rise,
  title={The Rise and Potential of Large Language Model Based Agents: A Survey},
  author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal={arXiv preprint arXiv:2309.07864},
  year={2023}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@inproceedings{zeng2022glm,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@article{openai2023gpt,
  title={GPT-4 technical report},
  author={OpenAI, R},
  journal={arXiv},
  pages={2303--08774},
  year={2023}
}

@techreport{manyika2023overview,
  title={An overview of Bard: an early experiment with generative AI},
  author={Manyika, James},
  year={2023},
  institution={Tech. rep., Technical report, Google AI}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{huang2023survey,
  title={A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation},
  author={Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and others},
  journal={arXiv preprint arXiv:2305.11391},
  year={2023}
}

@article{yao2023editing,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

@article{russell2021human,
  title={Human-compatible artificial intelligence},
  author={Russell, Stuart},
  journal={Human-like machine intelligence},
  pages={3--23},
  year={2021},
  publisher={Oxford University Press Oxford}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

@article{ji2023chatgpt,
  title={Is ChatGPT a Good Personality Recognizer? A Preliminary Study},
  author={Ji, Yu and Wu, Wen and Zheng, Hong and Hu, Yi and Chen, Xi and He, Liang},
  journal={arXiv preprint arXiv:2307.03952},
  year={2023}
}

@article{chiu2021detecting,
  title={Detecting hate speech with gpt-3},
  author={Chiu, Ke-Li and Collins, Annie and Alexander, Rohan},
  journal={arXiv preprint arXiv:2103.12407},
  year={2021}
}

@article{friedman2023leveraging,
  title={Leveraging Large Language Models in Conversational Recommender Systems},
  author={Friedman, Luke and Ahuja, Sameer and Allen, David and Tan, Terry and Sidahmed, Hakim and Long, Changbo and Xie, Jun and Schubiner, Gabriel and Patel, Ajay and Lara, Harsh and others},
  journal={arXiv preprint arXiv:2305.07961},
  year={2023}
}

@misc{liu2023once,
      title={ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models}, 
      author={Qijiong Liu and Nuo Chen and Tetsuya Sakai and Xiao-Ming Wu},
      year={2023},
      eprint={2305.06566},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{tan2023botpercent,
  title={BotPercent: Estimating Twitter bot populations from groups to crowds},
  author={Tan, Zhaoxuan and Feng, Shangbin and Sclar, Melanie and Wan, Herun and Luo, Minnan and Choi, Yejin and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2302.00381},
  year={2023}
}

@article{jiang2016suspicious,
  title={Suspicious behavior detection: Current trends and future directions},
  author={Jiang, Meng and Cui, Peng and Faloutsos, Christos},
  journal={IEEE intelligent systems},
  volume={31},
  number={1},
  pages={31--39},
  year={2016},
  publisher={IEEE}
}

@article{xi2023towards,
  title={Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models},
  author={Xi, Yunjia and Liu, Weiwen and Lin, Jianghao and Zhu, Jieming and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Zhang, Rui and Yu, Yong},
  journal={arXiv preprint arXiv:2306.10933},
  year={2023}
}

@article{lyu2023llm,
  title={LLM-Rec: Personalized Recommendation via Prompting Large Language Models},
  author={Lyu, Hanjia and Jiang, Song and Zeng, Hanqing and Xia, Yinglong and Luo, Jiebo},
  journal={arXiv preprint arXiv:2307.15780},
  year={2023}
}

@article{du2023enhancing,
  title={Enhancing Job Recommendation through LLM-based Generative Adversarial Networks},
  author={Du, Yingpeng and Luo, Di and Yan, Rui and Liu, Hongzhi and Song, Yang and Zhu, Hengshu and Zhang, Jie},
  journal={arXiv preprint arXiv:2307.10747},
  year={2023}
}

@inproceedings{peng2023gpt,
  title={Are GPT Embeddings Useful for Ads and Recommendation?},
  author={Peng, Wenjun and Xu, Derong and Xu, Tong and Zhang, Jianjin and Chen, Enhong},
  booktitle={International Conference on Knowledge Science, Engineering and Management},
  pages={151--162},
  year={2023},
  organization={Springer}
}

@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}


@article{li2023llm4jobs,
  title={LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models},
  author={Li, Nan and Kang, Bo and De Bie, Tijl},
  journal={arXiv preprint arXiv:2309.09708},
  year={2023}
}

@article{mysore2023large,
  title={Large Language Model Augmented Narrative Driven Recommendations},
  author={Mysore, Sheshera and McCallum, Andrew and Zamani, Hamed},
  journal={arXiv preprint arXiv:2306.02250},
  year={2023}
}

@article{li2023gpt4rec,
  title={GPT4Rec: A generative framework for personalized recommendation and user interests interpretation},
  author={Li, Jinming and Zhang, Wentao and Wang, Tian and Xiong, Guanglei and Lu, Alan and Medioni, Gerard},
  journal={arXiv preprint arXiv:2304.03879},
  year={2023}
}

@article{li2023prompt,
  title={Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations},
  author={Li, Pan and Wang, Yuyan and Chi, Ed H and Chen, Minmin},
  journal={arXiv preprint arXiv:2306.01475},
  year={2023}
}

@article{li2023pulsar,
  title={PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients' Problems and Data Augmentation with Black-box Large Language Models},
  author={Li, Hao and Wu, Yuping and Schlegel, Viktor and Batista-Navarro, Riza and Nguyen, Thanh-Tung and Kashyap, Abhinav Ramesh and Zeng, Xiaojun and Beck, Daniel and Winkler, Stefan and Nenadic, Goran},
  journal={arXiv preprint arXiv:2306.02754},
  year={2023}
}

@article{zheng2022augesc,
  title={Augesc: Large-scale data augmentation for emotional support conversation with pre-trained language models},
  author={Zheng, Chujie and Sabour, Sahand and Wen, Jiaxin and Huang, Minlie},
  journal={arXiv preprint arXiv:2202.13047},
  year={2022}
}

@inproceedings{acharya2023llm,
  title={LLM Based Generation of Item-Description for Recommendation System},
  author={Acharya, Arkadeep and Singh, Brijraj and Onoe, Naoyuki},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={1204--1207},
  year={2023}
}

@article{bang2022enabling,
  title={Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values},
  author={Bang, Yejin and Yu, Tiezheng and Madotto, Andrea and Lin, Zhaojiang and Diab, Mona and Fung, Pascale},
  journal={arXiv preprint arXiv:2210.07652},
  year={2022}
}

@article{peng2023generating,
  title={Generating Efficient Training Data via LLM-based Attribute Manipulation},
  author={Peng, Letian and Zhang, Yuwei and Shang, Jingbo},
  journal={arXiv preprint arXiv:2307.07099},
  year={2023}
}

@article{borisov2022language,
  title={Language models are realistic tabular data generators},
  author={Borisov, Vadim and Se{\ss}ler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={arXiv preprint arXiv:2210.06280},
  year={2022}
}

@article{tang2023does,
  title={Does synthetic data generation of llms help clinical text mining?},
  author={Tang, Ruixiang and Han, Xiaotian and Jiang, Xiaoqian and Hu, Xia},
  journal={arXiv preprint arXiv:2303.04360},
  year={2023}
}

@article{viswanathan2023prompt2model,
  title={Prompt2Model: Generating Deployable Models from Natural Language Instructions},
  author={Viswanathan, Vijay and Zhao, Chenyang and Bertsch, Amanda and Wu, Tongshuang and Neubig, Graham},
  journal={arXiv preprint arXiv:2308.12261},
  year={2023}
}

@article{golde2023fabricator,
  title={Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs},
  author={Golde, Jonas and Haller, Patrick and Hamborg, Felix and Risch, Julian and Akbik, Alan},
  journal={arXiv preprint arXiv:2309.09582},
  year={2023}
}

@article{zheng2023building,
  title={Building Emotional Support Chatbots in the Era of LLMs},
  author={Zheng, Zhonghua and Liao, Lizi and Deng, Yang and Nie, Liqiang},
  journal={arXiv preprint arXiv:2308.11584},
  year={2023}
}

@article{wang2023umass_bionlp,
  title={UMASS\_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?},
  author={Wang, Junda and Yao, Zonghai and Mitra, Avijit and Osebe, Samuel and Yang, Zhichao and Yu, Hong},
  journal={arXiv preprint arXiv:2306.16931},
  year={2023}
}

@article{schlegel2023pulsar,
  title={PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records},
  author={Schlegel, Viktor and Li, Hao and Wu, Yuping and Subramanian, Anand and Nguyen, Thanh-Tung and Kashyap, Abhinav Ramesh and Beck, Daniel and Zeng, Xiaojun and Batista-Navarro, Riza Theresa and Winkler, Stefan and others},
  journal={arXiv preprint arXiv:2307.02006},
  year={2023}
}

@article{su2023fake,
  title={Fake News Detectors are Biased against Texts Generated by Large Language Models},
  author={Su, Jinyan and Zhuo, Terry Yue and Mansurov, Jonibek and Wang, Di and Nakov, Preslav},
  journal={arXiv preprint arXiv:2309.08674},
  year={2023}
}

@article{veselovsky2023generating,
  title={Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science},
  author={Veselovsky, Veniamin and Ribeiro, Manoel Horta and Arora, Akhil and Josifoski, Martin and Anderson, Ashton and West, Robert},
  journal={arXiv preprint arXiv:2305.15041},
  year={2023}
}

@article{lingo2023exploring,
  title={Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT},
  author={Lingo, Ryan},
  journal={arXiv preprint arXiv:2306.13700},
  year={2023}
}

@article{leite2023detecting,
  title={Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision},
  author={Leite, Jo{\~a}o A and Razuvayevskaya, Olesya and Bontcheva, Kalina and Scarton, Carolina},
  journal={arXiv preprint arXiv:2309.07601},
  year={2023}
}

@article{zhang2023recommendation,
  title={Recommendation as instruction following: A large language model empowered recommendation approach},
  author={Zhang, Junjie and Xie, Ruobing and Hou, Yupeng and Zhao, Wayne Xin and Lin, Leyu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.07001},
  year={2023}
}


@article{zheng2023generative,
  title={Generative job recommendations with large language model},
  author={Zheng, Zhi and Qiu, Zhaopeng and Hu, Xiao and Wu, Likang and Zhu, Hengshu and Xiong, Hui},
  journal={arXiv preprint arXiv:2307.02157},
  year={2023}
}

@article{wang2023zero,
  title={Zero-Shot Next-Item Recommendation using Large Pretrained Language Models},
  author={Wang, Lei and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2304.03153},
  year={2023}
}

@article{christakopoulou2023large,
  title={Large Language Models for User Interest Journeys},
  author={Christakopoulou, Konstantina and Lalama, Alberto and Adams, Cj and Qu, Iris and Amir, Yifat and Chucri, Samer and Vollucci, Pierce and Soldo, Fabio and Bseiso, Dina and Scodel, Sarah and others},
  journal={arXiv preprint arXiv:2305.15498},
  year={2023}
}

@inproceedings{sanner2023large,
  title={Large Language Models are Competitive Near Cold-start Recommenders for Language-and Item-based Preferences},
  author={Sanner, Scott and Balog, Krisztian and Radlinski, Filip and Wedin, Ben and Dixon, Lucas},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={890--896},
  year={2023}
}

@article{zhiyuli2023bookgpt,
  title={BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model},
  author={Zhiyuli, Aakas and Chen, Yanfang and Zhang, Xuan and Liang, Xun},
  journal={arXiv preprint arXiv:2305.15673},
  year={2023}
}

@article{bao2023tallrec,
  title={Tallrec: An effective and efficient tuning framework to align large language model with recommendation},
  author={Bao, Keqin and Zhang, Jizhi and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
  journal={arXiv preprint arXiv:2305.00447},
  year={2023}
}

@article{wu2023exploring,
  title={Exploring large language model for graph data understanding in online job recommendations},
  author={Wu, Likang and Qiu, Zhaopeng and Zheng, Zhi and Zhu, Hengshu and Chen, Enhong},
  journal={arXiv preprint arXiv:2307.05722},
  year={2023}
}

@article{hou2023large,
  title={Large language models are zero-shot rankers for recommender systems},
  author={Hou, Yupeng and Zhang, Junjie and Lin, Zihan and Lu, Hongyu and Xie, Ruobing and McAuley, Julian and Zhao, Wayne Xin},
  journal={arXiv preprint arXiv:2305.08845},
  year={2023}
}

@article{li2023preliminary,
  title={A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News},
  author={Li, Xinyi and Zhang, Yongfeng and Malthouse, Edward C},
  journal={arXiv preprint arXiv:2306.10702},
  year={2023}
}

@article{lin2023rella,
  title={ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation},
  author={Lin, Jianghao and Shan, Rong and Zhu, Chenxu and Du, Kounianhua and Chen, Bo and Quan, Shigang and Tang, Ruiming and Yu, Yong and Zhang, Weinan},
  journal={arXiv preprint arXiv:2308.11131},
  year={2023}
}

@article{rao2023can,
  title={Can chatgpt assess human personalities? a general evaluation framework},
  author={Rao, Haocong and Leung, Cyril and Miao, Chunyan},
  journal={arXiv preprint arXiv:2303.01248},
  year={2023}
}

@article{tie2023automatic,
  title={Automatic Personalized Impression Generation for PET Reports Using Large Language Models},
  author={Tie, Xin and Shin, Muheon and Pirasteh, Ali and Ibrahim, Nevein and Huemann, Zachary and Castellino, Sharon M and Kelly, Kara M and Garrett, John and Hu, Junjie and Cho, Steve Y and others},
  journal={arXiv preprint arXiv:2309.10066},
  year={2023}
}

@article{dai2023uncovering,
  title={Uncovering ChatGPT's Capabilities in Recommender Systems},
  author={Dai, Sunhao and Shao, Ninglu and Zhao, Haiyuan and Yu, Weijie and Si, Zihua and Xu, Chen and Sun, Zhongxiang and Zhang, Xiao and Xu, Jun},
  journal={arXiv preprint arXiv:2305.02182},
  year={2023}
}

@article{di2023evaluating,
  title={Evaluating ChatGPT as a Recommender System: A Rigorous Approach},
  author={Di Palma, Dario and Biancofiore, Giovanni Maria and Anelli, Vito Walter and Narducci, Fedelucio and Di Noia, Tommaso and Di Sciascio, Eugenio},
  journal={arXiv preprint arXiv:2309.03613},
  year={2023}
}

@article{ghanadian2023chatgpt,
  title={ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations},
  author={Ghanadian, Hamideh and Nejadgholi, Isar and Osman, Hussein Al},
  journal={arXiv preprint arXiv:2306.09390},
  year={2023}
}


@article{wang2023rethinking,
  title={Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models},
  author={Wang, Xiaolei and Tang, Xinyu and Zhao, Wayne Xin and Wang, Jingyuan and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.13112},
  year={2023}
}

@article{wang2023recmind,
  title={Recmind: Large language model powered agent for recommendation},
  author={Wang, Yancheng and Jiang, Ziyan and Chen, Zheng and Yang, Fan and Zhou, Yingxue and Cho, Eunah and Fan, Xing and Huang, Xiaojiang and Lu, Yanbin and Yang, Yingzhen},
  journal={arXiv preprint arXiv:2308.14296},
  year={2023}
}

@article{huang2023recommender,
  title={Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations},
  author={Huang, Xu and Lian, Jianxun and Lei, Yuxuan and Yao, Jing and Lian, Defu and Xie, Xing},
  journal={arXiv preprint arXiv:2308.16505},
  year={2023}
}

@article{wang2023recagent,
  title={RecAgent: A Novel Simulation Paradigm for Recommender Systems},
  author={Wang, Lei and Zhang, Jingsen and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2306.02552},
  year={2023}
}

@article{hu2023unlocking,
  title={Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System},
  author={Hu, Zhiyuan and Feng, Yue and Luu, Anh Tuan and Hooi, Bryan and Lipani, Aldo},
  journal={arXiv preprint arXiv:2306.09821},
  year={2023}
}


@article{kong2023large,
  title={Large Language Model as a User Simulator},
  author={Kong, Chuyi and Fan, Yaxin and Wan, Xiang and Jiang, Feng and Wang, Benyou},
  journal={arXiv preprint arXiv:2308.11534},
  year={2023}
}

@article{zhang2023graph,
  title={Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT},
  author={Zhang, Jiawei},
  journal={arXiv preprint arXiv:2304.11116},
  year={2023}
}

@article{robinson2022leveraging,
  title={Leveraging large language models for multiple choice question answering},
  author={Robinson, Joshua and Rytting, Christopher Michael and Wingate, David},
  journal={arXiv preprint arXiv:2210.12353},
  year={2022}
}

@article{sun2023text,
  title={Text Classification via Large Language Models},
  author={Sun, Xiaofei and Li, Xiaoya and Li, Jiwei and Wu, Fei and Guo, Shangwei and Zhang, Tianwei and Wang, Guoyin},
  journal={arXiv preprint arXiv:2305.08377},
  year={2023}
}

@article{zhang2022would,
  title={How would stance detection techniques evolve after the launch of chatgpt?},
  author={Zhang, Bowen and Ding, Daijun and Jing, Liwen},
  journal={arXiv preprint arXiv:2212.14548},
  year={2022}
}

@article{hu2023ladder,
  title={Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection},
  author={Hu, Kairui and Yan, Ming and Zhou, Joey Tianyi and Tsang, Ivor W and Chong, Wen Haw and Yap, Yong Keong},
  journal={arXiv preprint arXiv:2308.16763},
  year={2023}
}

@article{pena2023leveraging,
  title={Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs},
  author={Pe{\~n}a, Alejandro and Morales, Aythami and Fierrez, Julian and Serna, Ignacio and Ortega-Garcia, Javier and Puente, I{\~n}igo and Cordova, Jorge and Cordova, Gonzalo},
  journal={arXiv preprint arXiv:2306.02864},
  year={2023}
}

@article{mu2023navigating,
  title={Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science},
  author={Mu, Yida and Wu, Ben P and Thorne, William and Robinson, Ambrose and Aletras, Nikolaos and Scarton, Carolina and Bontcheva, Kalina and Song, Xingyi},
  journal={arXiv preprint arXiv:2305.14310},
  year={2023}
}

@article{parikh2023exploring,
  title={Exploring Zero and Few-shot Techniques for Intent Classification},
  author={Parikh, Soham and Vohra, Quaizar and Tumbade, Prashil and Tiwari, Mitul},
  journal={arXiv preprint arXiv:2305.07157},
  year={2023}
}

@inproceedings{clavie2023large,
  title={Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification},
  author={Clavi{\'e}, Benjamin and Ciceu, Alexandru and Naylor, Frederick and Souli{\'e}, Guillaume and Brightwell, Thomas},
  booktitle={International Conference on Applications of Natural Language to Information Systems},
  pages={3--17},
  year={2023},
  organization={Springer}
}

@article{qin2023read,
  title={Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media},
  author={Qin, Wei and Chen, Zetong and Wang, Lei and Lan, Yunshi and Ren, Weijieying and Hong, Richang},
  journal={arXiv preprint arXiv:2305.05138},
  year={2023}
}

@inproceedings{deng2023llms,
  title={What do llms know about financial markets? a case study on reddit market sentiment analysis},
  author={Deng, Xiang and Bashlovkina, Vasilisa and Han, Feng and Baumgartner, Simon and Bendersky, Michael},
  booktitle={Companion Proceedings of the ACM Web Conference 2023},
  pages={107--110},
  year={2023}
}

@article{ferrara2023social,
  title={Social bot detection in the age of ChatGPT: Challenges and opportunities},
  author={Ferrara, Emilio},
  journal={First Monday},
  year={2023}
}

@article{qi2023evaluating,
  title={Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media},
  author={Qi, Hongzhi and Zhao, Qing and Song, Changwei and Zhai, Wei and Luo, Dan and Liu, Shuo and Yu, Yi Jing and Wang, Fan and Zou, Huijing and Yang, Bing Xiang and others},
  journal={arXiv preprint arXiv:2309.03564},
  year={2023}
}

@article{zhu2023clickbait,
  title={Clickbait Detection via Large Language Models},
  author={Zhu, Yi and Wang, Han and Wang, Ye and Li, Yun and Yuan, Yunhao and Qiang, Jipeng},
  journal={arXiv preprint arXiv:2306.09597},
  year={2023}
}

@article{nguyen2023fine,
  title={Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts},
  author={Nguyen, Thanh Thi and Wilson, Campbell and Dalins, Janis},
  journal={arXiv preprint arXiv:2308.14683},
  year={2023}
}

@article{kheiri2023sentimentgpt,
  title={SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning},
  author={Kheiri, Kiana and Karimi, Hamid},
  journal={arXiv preprint arXiv:2307.10234},
  year={2023}
}

@article{yu2023temporal,
  title={Temporal Data Meets LLM--Explainable Financial Time Series Forecasting},
  author={Yu, Xinli and Chen, Zheng and Ling, Yuan and Dong, Shujing and Liu, Zongyi and Lu, Yanbin},
  journal={arXiv preprint arXiv:2306.11025},
  year={2023}
}

@article{zhang2023explaining,
  title={Explaining Agent Behavior with Large Language Models},
  author={Zhang, Xijia and Guo, Yue and Stepputtis, Simon and Sycara, Katia and Campbell, Joseph},
  journal={arXiv preprint arXiv:2309.10346},
  year={2023}
}

@article{jiang2023balanced,
  title={Balanced and Explainable Social Media Analysis for Public Health with Large Language Models},
  author={Jiang, Yan and Qiu, Ruihong and Zhang, Yi and Zhang, Peng-Fei},
  journal={arXiv preprint arXiv:2309.05951},
  year={2023}
}

@article{bao2023exploring,
  title={Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models},
  author={Bao, Qiming and Leinonen, Juho and Peng, Alex Yuxuan and Zhong, Wanjun and Pistotti, Tim and Huang, Alice and Denny, Paul and Witbrock, Michael and Liu, Jiamou},
  journal={arXiv preprint arXiv:2309.10444},
  year={2023}
}

@article{lin2023sparks,
  title={Sparks of artificial general recommender (agr): Early experiments with chatgpt},
  author={Lin, Guo and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2305.04518},
  year={2023}
}

@article{jiang2023personallm,
  title={Personallm: Investigating the ability of gpt-3.5 to express personality traits and gender differences},
  author={Jiang, Hang and Zhang, Xiajie and Cao, Xubo and Kabbara, Jad and Roy, Deb},
  journal={arXiv preprint arXiv:2305.02547},
  year={2023}
}

@article{lakkaraju2023can,
  title={Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes},
  author={Lakkaraju, Kausik and Vuruma, Sai Krishna Revanth and Pallagani, Vishal and Muppasani, Bharath and Srivastava, Biplav},
  journal={arXiv preprint arXiv:2307.07422},
  year={2023}
}

@article{hassan2023chatgpt,
  title={ChatGPT as your Personal Data Scientist},
  author={Hassan, Md Mahadi and Knipper, Alex and Santu, Shubhra Kanti Karmaker},
  journal={arXiv preprint arXiv:2305.13657},
  year={2023}
}

@article{tu2023characterchat,
  title={CharacterChat: Learning towards Conversational AI with Personalized Social Support},
  author={Tu, Quan and Chen, Chuanqi and Li, Jinpeng and Li, Yanran and Shang, Shuo and Zhao, Dongyan and Wang, Ran and Yan, Rui},
  journal={arXiv preprint arXiv:2308.10278},
  year={2023}
}

@article{he2023large,
  title={Large language models as zero-shot conversational recommenders},
  author={He, Zhankui and Xie, Zhouhang and Jha, Rahul and Steck, Harald and Liang, Dawen and Feng, Yesu and Majumder, Bodhisattwa Prasad and Kallus, Nathan and McAuley, Julian},
  journal={arXiv preprint arXiv:2308.10053},
  year={2023}
}

@article{bae2022building,
  title={Building a role specified open-domain dialogue system leveraging large-scale language models},
  author={Bae, Sanghwan and Kwak, Donghyun and Kim, Sungdong and Ham, Donghoon and Kang, Soyoung and Lee, Sang-Woo and Park, Woomyoung},
  journal={arXiv preprint arXiv:2205.00176},
  year={2022}
}

@article{li2023chatdoctor,
  title={ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge},
  author={Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
  journal={Cureus},
  volume={15},
  number={6},
  year={2023},
  publisher={Cureus}
}

@article{foosherian2023enhancing,
  title={Enhancing Pipeline-Based Conversational Agents with Large Language Models},
  author={Foosherian, Mina and Purwins, Hendrik and Rathnayake, Purna and Alam, Touhidul and Teimao, Rui and Thoben, Klaus-Dieter},
  journal={arXiv preprint arXiv:2309.03748},
  year={2023}
}

@article{chen2023llm,
  title={LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation},
  author={Chen, Siyuan and Wu, Mengyue and Zhu, Kenny Q and Lan, Kunyao and Zhang, Zhiling and Cui, Lyuchun},
  journal={arXiv preprint arXiv:2305.13614},
  year={2023}
}

@article{svikhnushina2023approximating,
  title={Approximating Human Evaluation of Social Chatbots with Prompting},
  author={Svikhnushina, Ekaterina and Pu, Pearl},
  journal={arXiv preprint arXiv:2304.05253},
  year={2023}
}

@article{fan2023uncovering,
  title={Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study},
  author={Fan, Yaxin and Jiang, Feng},
  journal={arXiv preprint arXiv:2305.08391},
  year={2023}
}

@article{finch2023leveraging,
  title={Leveraging Large Language Models for Automated Dialogue Analysis},
  author={Finch, Sarah E and Paek, Ellie S and Choi, Jinho D},
  journal={arXiv preprint arXiv:2309.06490},
  year={2023}
}

@article{huynh2023understanding,
  title={Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation},
  author={Huynh, Jessica and Jiao, Cathy and Gupta, Prakhar and Mehri, Shikib and Bajaj, Payal and Chaudhary, Vishrav and Eskenazi, Maxine},
  journal={arXiv preprint arXiv:2301.12004},
  year={2023}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@article{lin2023llm,
  title={LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models},
  author={Lin, Yen-Ting and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:2305.13711},
  year={2023}
}

@article{mendoncca2023simple,
  title={Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation},
  author={Mendon{\c{c}}a, John and Pereira, Patr{\'\i}cia and Carvalho, Jo{\~a}o Paulo and Lavie, Alon and Trancoso, Isabel},
  journal={arXiv preprint arXiv:2308.16797},
  year={2023}
}

@article{shi2023llm,
  title={LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation},
  author={Shi, Xiaoming and Xu, Jie and Ding, Jinru and Pang, Jiali and Liu, Sichen and Luo, Shuqing and Peng, Xingwei and Lu, Lu and Yang, Haihong and Hu, Mingtao and others},
  journal={arXiv preprint arXiv:2308.07635},
  year={2023}
}

@article{bhattacharjee2023llms,
  title={LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain Black-box Text Classifiers?},
  author={Bhattacharjee, Amrita and Moraffah, Raha and Garland, Joshua and Liu, Huan},
  journal={arXiv preprint arXiv:2309.13340},
  year={2023}
}

@article{yang2023anatomy,
  title={Anatomy of an AI-powered malicious social botnet},
  author={Yang, Kai-Cheng and Menczer, Filippo},
  journal={arXiv preprint arXiv:2307.16336},
  year={2023}
}


@article{cohen2023enhancing,
  title={Enhancing social network hate detection using back translation and GPT-3 augmentations during training and test-time},
  author={Cohen, Seffi and Presil, Dan and Katz, Or and Arbili, Ofir and Messica, Shvat and Rokach, Lior},
  journal={Information Fusion},
  pages={101887},
  year={2023},
  publisher={Elsevier}
}

@article{matwin2021survey,
  title={Survey of generative methods for social media analysis},
  author={Matwin, Stan and Milios, Aristides and Pra{\l}at, Pawe{\l} and Soares, Amilcar and Th{\'e}berge, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2112.07041},
  year={2021}
}

@article{wang2023evaluating,
  title={Evaluating GPT-3 Generated Explanations for Hateful Content Moderation},
  author={Wang, Han and Hee, Ming Shan and Awal, Md Rabiul and Choo, Kenny Tsu Wei and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2305.17680},
  year={2023}
}

@inproceedings{del2023respectful,
  title={Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech},
  author={Del Arco, Flor Miriam Plaza and Nozza, Debora and Hovy, Dirk},
  booktitle={The 7th Workshop on Online Abuse and Harms (WOAH)},
  pages={60--68},
  year={2023}
}

@article{das2023evaluating,
  title={Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection},
  author={Das, Mithun and Pandey, Saurabh Kumar and Mukherjee, Animesh},
  journal={arXiv preprint arXiv:2305.13276},
  year={2023}
}

@inproceedings{ayoobi2023looming,
  title={The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention},
  author={Ayoobi, Navid and Shahriar, Sadat and Mukherjee, Arjun},
  booktitle={Proceedings of the 34th ACM Conference on Hypertext and Social Media},
  pages={1--10},
  year={2023}
}

@article{pavlyshenko2023analysis,
  title={Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model},
  author={Pavlyshenko, Bohdan M},
  journal={arXiv preprint arXiv:2309.04704},
  year={2023}
}

@article{pan2023risk,
  title={On the Risk of Misinformation Pollution with Large Language Models},
  author={Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13661},
  year={2023}
}

@article{hanley2023machine,
  title={Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites},
  author={Hanley, Hans WA and Durumeric, Zakir},
  journal={arXiv preprint arXiv:2305.09820},
  year={2023}
}

@article{chavan2022large,
  title={Large Language Models for Multi-label Propaganda Detection},
  author={Chavan, Tanmay and Kane, Aditya},
  journal={arXiv preprint arXiv:2210.08209},
  year={2022}
}

@article{jiang2023disinformation,
  title={Disinformation Detection: An Evolving Challenge in the Age of LLMs},
  author={Jiang, Bohan and Tan, Zhen and Nirmal, Ayushi and Liu, Huan},
  journal={arXiv preprint arXiv:2309.15847},
  year={2023}
}

@article{chen2023can,
  title={Can LLM-Generated Misinformation Be Detected?},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv preprint arXiv:2309.13788},
  year={2023}
}

@article{khalil2023will,
  title={Will ChatGPT get you caught? Rethinking of plagiarism detection},
  author={Khalil, Mohammad and Er, Erkan},
  journal={arXiv preprint arXiv:2302.04335},
  year={2023}
}

@article{pegoraro2023chatgpt,
  title={To ChatGPT, or not to ChatGPT: That is the question!},
  author={Pegoraro, Alessandro and Kumari, Kavita and Fereidooni, Hossein and Sadeghi, Ahmad-Reza},
  journal={arXiv preprint arXiv:2304.01487},
  year={2023}
}

@article{yu2023gpt,
  title={GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance},
  author={Yu, Xiao and Qi, Yuang and Chen, Kejiang and Chen, Guoqiang and Yang, Xi and Zhu, Pengyuan and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2305.12519},
  year={2023}
}

@article{dhaini2023detecting,
  title={Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text},
  author={Dhaini, Mahdi and Poelman, Wessel and Erdogan, Ege},
  journal={arXiv preprint arXiv:2309.07689},
  year={2023}
}

@article{tang2023science,
  title={The science of detecting llm-generated texts},
  author={Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
  journal={arXiv preprint arXiv:2303.07205},
  year={2023}
}

@article{liu2022coco,
  title={CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning},
  author={Liu, Xiaoming and Zhang, Zhaohan and Wang, Yichen and Lan, Yu and Shen, Chao},
  journal={arXiv preprint arXiv:2212.10341},
  year={2022}
}

@article{shen2023hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2303.17580},
  year={2023}
}

@article{feng2023cook,
  title={CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge},
  author={Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.09955},
  year={2023}
}

@article{bhattacharjee2023fighting,
  title={Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?},
  author={Bhattacharjee, Amrita and Liu, Huan},
  journal={arXiv preprint arXiv:2308.01284},
  year={2023}
}

@article{yang2023chatgpt,
  title={Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text},
  author={Yang, Lingyi and Jiang, Feng and Li, Haizhou},
  journal={arXiv preprint arXiv:2307.11380},
  year={2023}
}

@article{cai2023evade,
  title={Evade ChatGPT detectors via a single space},
  author={Cai, Shuyang and Cui, Wanyun},
  journal={arXiv preprint arXiv:2307.02599},
  year={2023}
}

@article{shukla2023catch,
  title={Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers},
  author={Shukla, Aishwarya Deep and Agarwal, Laksh and Mein, Jie and Agarwal, Ritu and others},
  journal={arXiv preprint arXiv:2304.09948},
  year={2023}
}

@article{orenstrakh2023detecting,
  title={Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases},
  author={Orenstrakh, Michael Sheinman and Karnalim, Oscar and Suarez, Carlos Anibal and Liut, Michael},
  journal={arXiv preprint arXiv:2307.07411},
  year={2023}
}

@article{wahle2022large,
  title={How large language models are transforming machine-paraphrased plagiarism},
  author={Wahle, Jan Philip and Ruas, Terry and Kirstein, Frederic and Gipp, Bela},
  journal={arXiv preprint arXiv:2210.03568},
  year={2022}
}

@article{tian2023chatgpt,
  title={Is ChatGPT the Ultimate Programming Assistant--How far is it?},
  author={Tian, Haoye and Lu, Weiqi and Li, Tsz On and Tang, Xunzhu and Cheung, Shing-Chi and Klein, Jacques and Bissyand{\'e}, Tegawend{\'e} F},
  journal={arXiv preprint arXiv:2304.11938},
  year={2023}
}

@article{mo2023roll,
  title={Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System},
  author={Mo, Lingbo and Chen, Shijie and Chen, Ziru and Deng, Xiang and Lewis, Ashley and Singh, Sunit and Stevens, Samuel and Tai, Chang-You and Wang, Zhen and Yue, Xiang and others},
  journal={arXiv preprint arXiv:2307.16081},
  year={2023}
}

@article{yue2023disc,
  title={DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services},
  author={Yue, Shengbin and Chen, Wei and Wang, Siyuan and Li, Bingxuan and Shen, Chenchen and Liu, Shujun and Zhou, Yuxuan and Xiao, Yao and Yun, Song and Lin, Wei and others},
  journal={arXiv preprint arXiv:2309.11325},
  year={2023}
}

@article{liu2023fingpt,
  title={FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},
  author={Liu, Xiao-Yang and Wang, Guoxuan and Zha, Daochen},
  journal={arXiv preprint arXiv:2307.10485},
  year={2023}
}

@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@article{chakrabarty2023creativity,
  title={Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers},
  author={Chakrabarty, Tuhin and Padmakumar, Vishakh and Brahman, Faeze and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2309.12570},
  year={2023}
}

@article{yang2023zhongjing,
  title={Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue},
  author={Yang, Songhua and Zhao, Hanjia and Zhu, Senbin and Zhou, Guangyu and Xu, Hongfei and Jia, Yuxiang and Zan, Hongying},
  journal={arXiv preprint arXiv:2308.03549},
  year={2023}
}

@article{cao2023diaggpt,
  title={DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue},
  author={Cao, Lang},
  journal={arXiv preprint arXiv:2308.08043},
  year={2023}
}

@article{hudevcek2023llms,
  title={Are LLMs All You Need for Task-Oriented Dialogue?},
  author={Hude{\v{c}}ek, Vojt{\v{e}}ch and Du{\v{s}}ek, Ond{\v{r}}ej},
  journal={arXiv preprint arXiv:2304.06556},
  year={2023}
}

@article{cho2022personalized,
  title={A personalized dialogue generator with implicit user persona detection},
  author={Cho, Itsugun and Wang, Dongyang and Takahashi, Ryota and Saito, Hiroaki},
  journal={arXiv preprint arXiv:2204.07372},
  year={2022}
}

@article{kwon2023textit,
  title={WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue},
  author={Kwon, Deuksin and Lee, Sunwoo and Kim, Ki Hyun and Lee, Seojin and Kim, Taeyoon and Davis, Eric},
  journal={arXiv preprint arXiv:2306.03361},
  year={2023}
}

@article{sun2023fictional,
  title={Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs},
  author={Sun, Yuqian and Wang, Hanyi and Chan, Pok Man and Tabibi, Morteza and Zhang, Yan and Lu, Huan and Chen, Yuheng and Lee, Chang Hee and Asadipour, Ali},
  journal={arXiv preprint arXiv:2309.11478},
  year={2023}
}

@article{yang2023refgpt,
  title={RefGPT: Reference-> Truthful \& Customized Dialogues Generation by GPTs and for GPTs},
  author={Yang, Dongjie and Yuan, Ruifeng and Fan, YuanTao and Yang, YiFei and Wang, Zili and Wang, Shushen and Zhao, Hai},
  journal={arXiv preprint arXiv:2305.14994},
  year={2023}
}

@article{joshi2023let,
  title={From" Let's Google" to" Let's ChatGPT": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education},
  author={Joshi, Ishika and Budhiraja, Ritvik and Tanna, Pranav Deepak and Jain, Lovenya and Deshpande, Mihika and Srivastava, Arjun and Rallapalli, Srinivas and Akolekar, Harshal D and Challa, Jagat Sesh and Kumar, Dhruv},
  journal={arXiv preprint arXiv:2309.10694},
  year={2023}
}

@article{ziems2023can,
  title={Can Large Language Models Transform Computational Social Science?},
  author={Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  journal={arXiv preprint arXiv:2305.03514},
  year={2023}
}

@article{wu2023large,
  title={Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting},
  author={Wu, Patrick Y and Tucker, Joshua A and Nagler, Jonathan and Messing, Solomon},
  journal={arXiv preprint arXiv:2303.12057},
  year={2023}
}

@article{dan2023educhat,
  title={EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education},
  author={Dan, Yuhao and Lei, Zhikai and Gu, Yiyang and Li, Yong and Yin, Jianghao and Lin, Jiaju and Ye, Linhao and Tie, Zhiyan and Zhou, Yougen and Wang, Yilei and others},
  journal={arXiv preprint arXiv:2308.02773},
  year={2023}
}

@article{yan2023practical,
  title={Practical and ethical challenges of large language models in education: A systematic literature review},
  author={Yan, Lixiang and Sha, Lele and Zhao, Linxuan and Li, Yuheng and Martinez-Maldonado, Roberto and Chen, Guanliang and Li, Xinyu and Jin, Yueqiao and Ga{\v{s}}evi{\'c}, Dragan},
  journal={arXiv preprint arXiv:2303.13379},
  year={2023}
}

@article{tu2023should,
  title={What Should Data Science Education Do with Large Language Models?},
  author={Tu, Xinming and Zou, James and Su, Weijie J and Zhang, Linjun},
  journal={arXiv preprint arXiv:2307.02792},
  year={2023}
}

@article{sharma2023performance,
  title={Performance of chatgpt on usmle: Unlocking the potential of large language models for ai-assisted medical education},
  author={Sharma, Prabin and Thapa, Kisan and Dhakal, Prastab and Upadhaya, Mala Deep and Adhikari, Santosh and Khanal, Salik Ram},
  journal={arXiv preprint arXiv:2307.00112},
  year={2023}
}

@inproceedings{elkins2023useful,
  title={How Useful are Educational Questions Generated by Large Language Models?},
  author={Elkins, Sabina and Kochmar, Ekaterina and Serban, Iulian and Cheung, Jackie CK},
  booktitle={International Conference on Artificial Intelligence in Education},
  pages={536--542},
  year={2023},
  organization={Springer}
}

@article{ochieng2023large,
  title={Are Large Language Models Fit For Guided Reading?},
  author={Ochieng, Peter},
  journal={arXiv preprint arXiv:2305.10645},
  year={2023}
}

@article{levine2021inductive,
  title={The inductive bias of in-context learning: Rethinking pretraining example design},
  author={Levine, Yoav and Wies, Noam and Jannai, Daniel and Navon, Dan and Hoshen, Yedid and Shashua, Amnon},
  journal={arXiv preprint arXiv:2110.04541},
  year={2021}
}

@article{si2023measuring,
  title={Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations},
  author={Si, Chenglei and Friedman, Dan and Joshi, Nitish and Feng, Shi and Chen, Danqi and He, He},
  journal={arXiv preprint arXiv:2305.13299},
  year={2023}
}

@article{olga2023generative,
  title={Generative AI: Implications and Applications for Education},
  author={Olga, Anastasia and Saini, Akash and Zapata, Gabriela and Searsmith, Duane and Cope, Bill and Kalantzis, Mary and Castro, Vania and Kourkoulou, Theodora and Jones, John and da Silva, Rodrigo Abrantes and others},
  journal={arXiv preprint arXiv:2305.07605},
  year={2023}
}

@article{caines2023application,
  title={On the application of Large Language Models for language teaching and assessment technology},
  author={Caines, Andrew and Benedetto, Luca and Taslimipoor, Shiva and Davis, Christopher and Gao, Yuan and Andersen, Oeistein and Yuan, Zheng and Elliott, Mark and Moore, Russell and Bryant, Christopher and others},
  journal={arXiv preprint arXiv:2307.08393},
  year={2023}
}

@article{phung2023generative,
  title={Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors},
  author={Phung, Tung and P{\u{a}}durean, Victor-Alexandru and Cambronero, Jos{\'e} and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
  journal={International Journal of Management},
  volume={21},
  number={2},
  pages={100790},
  year={2023}
}

@article{koyuturk2023developing,
  title={Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy},
  author={Koyuturk, Cansu and Yavari, Mona and Theophilou, Emily and Bursic, Sathya and Donabauer, Gregor and Telari, Alessia and Testa, Alessia and Boiano, Raffaele and Gabbiadini, Alessandro and Hernandez-Leo, Davinia and others},
  journal={arXiv preprint arXiv:2306.10645},
  year={2023}
}

@inproceedings{bhat2022towards,
  title={Towards automated generation and evaluation of questions in educational domains},
  author={Bhat, Shravya and Nguyen, Huy A and Moore, Steven and Stamper, John and Sakr, Majd and Nyberg, Eric},
  booktitle={Proceedings of the 15th International Conference on Educational Data Mining},
  volume={701},
  year={2022}
}

@article{fu2023enhancing,
  title={Enhancing psychological counseling with large language model: A multifaceted decision-support system for non-professionals},
  author={Fu, Guanghui and Zhao, Qing and Li, Jianqiang and Luo, Dan and Song, Changwei and Zhai, Wei and Liu, Shuo and Wang, Fan and Wang, Yan and Cheng, Lijuan and others},
  journal={arXiv preprint arXiv:2308.15192},
  year={2023}
}

@article{belyaeva2023multimodal,
  title={Multimodal LLMs for health grounded in individual-specific data},
  author={Belyaeva, Anastasiya and Cosentino, Justin and Hormozdiari, Farhad and McLean, Cory Y and Furlotte, Nicholas A},
  journal={arXiv preprint arXiv:2307.09018},
  year={2023}
}

@article{liu2023large,
  title={Large Language Models are Few-Shot Health Learners},
  author={Liu, Xin and McDuff, Daniel and Kovacs, Geza and Galatzer-Levy, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  journal={arXiv preprint arXiv:2305.15525},
  year={2023}
}

@article{xu2023mental,
  title={Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data},
  author={Xu, Xuhai and Yao, Bingshen and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K and Wang, Dakuo},
  journal={arXiv preprint arXiv:2307.14385},
  year={2023}
}

@article{lai2023psy,
  title={Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models},
  author={Lai, Tin and Shi, Yukun and Du, Zicong and Wu, Jiajie and Fu, Ken and Dou, Yichao and Wang, Ziqi},
  journal={arXiv preprint arXiv:2307.11991},
  year={2023}
}

@article{wang2023large,
  title={Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding},
  author={Wang, Yuqing and Zhao, Yun and Petzold, Linda},
  journal={arXiv preprint arXiv:2304.05368},
  year={2023}
}

@article{yuan2023llm,
  title={LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability},
  author={Yuan, Jiayi and Tang, Ruixiang and Jiang, Xiaoqian and Hu, Xia},
  journal={arXiv preprint arXiv:2303.16756},
  year={2023}
}

@article{peters2023large,
  title={Large Language Models Can Infer Psychological Dispositions of Social Media Users},
  author={Peters, Heinrich and Matz, Sandra},
  journal={arXiv preprint arXiv:2309.08631},
  year={2023}
}

@article{liu2023pharmacygpt,
  title={Pharmacygpt: The ai pharmacist},
  author={Liu, Zhengliang and Wu, Zihao and Hu, Mengxuan and Zhao, Bokai and Zhao, Lin and Zhang, Tianyi and Dai, Haixing and Chen, Xianyan and Shen, Ye and Li, Sheng and others},
  journal={arXiv preprint arXiv:2307.10432},
  year={2023}
}

@article{zhang2023potential,
  title={The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant},
  author={Zhang, Jingqing and Sun, Kai and Jagadeesh, Akshay and Ghahfarokhi, Mahta and Gupta, Deepa and Gupta, Ashok and Gupta, Vibhor and Guo, Yike},
  journal={arXiv preprint arXiv:2307.08152},
  year={2023}
}

@article{wei2023leveraging,
  title={Leveraging large language models to power chatbots for collecting user self-reported data},
  author={Wei, Jing and Kim, Sungdong and Jung, Hyunhoon and Kim, Young-Ho},
  journal={arXiv preprint arXiv:2301.05843},
  year={2023}
}

@article{zhang2023chatgpt,
  title={Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation},
  author={Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
  journal={arXiv preprint arXiv:2305.07609},
  year={2023}
}

@article{ji2023genrec,
  title={Genrec: Large language model for generative recommendation},
  author={Ji, Jianchao and Li, Zelong and Xu, Shuyuan and Hua, Wenyue and Ge, Yingqiang and Tan, Juntao and Zhang, Yongfeng},
  journal={arXiv e-prints},
  pages={arXiv--2307},
  year={2023}
}

@article{wang2023generative,
  title={Generative recommendation: Towards next-generation recommender paradigm},
  author={Wang, Wenjie and Lin, Xinyu and Feng, Fuli and He, Xiangnan and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2304.03516},
  year={2023}
}

@article{zhang2023siren,
  title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{yu2022survey,
  title={A survey of knowledge-enhanced text generation},
  author={Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  journal={ACM Computing Surveys},
  volume={54},
  number={11s},
  pages={1--38},
  year={2022},
  publisher={ACM New York, NY}
}

@article{adlakha2023evaluating,
  title={Evaluating correctness and faithfulness of instruction-following models for question answering},
  author={Adlakha, Vaibhav and BehnamGhader, Parishad and Lu, Xing Han and Meade, Nicholas and Reddy, Siva},
  journal={arXiv preprint arXiv:2307.16877},
  year={2023}
}

@article{liu2021token,
  title={A token-level reference-free hallucination detection benchmark for free-form text generation},
  author={Liu, Tianyu and Zhang, Yizhe and Brockett, Chris and Mao, Yi and Sui, Zhifang and Chen, Weizhu and Dolan, Bill},
  journal={arXiv preprint arXiv:2104.08704},
  year={2021}
}

@article{min2023factscore,
  title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
  author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2305.14251},
  year={2023}
}

@article{feng2023factkb,
  title={Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge},
  author={Feng, Shangbin and Balachandran, Vidhisha and Bai, Yuyang and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.08281},
  year={2023}
}

@inproceedings{huang2022large,
  title={Are Large Pre-Trained Language Models Leaking Your Personal Information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2038--2047},
  year={2022}
}

@article{liu2023agentbench,
  title={Agentbench: Evaluating llms as agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  journal={arXiv preprint arXiv:2308.03688},
  year={2023}
}

@article{yu2023large,
  title={Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias},
  author={Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
  journal={arXiv preprint arXiv:2306.15895},
  year={2023}
}

@article{li2023you,
  title={Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models},
  author={Li, Zekun and Peng, Baolin and He, Pengcheng and Yan, Xifeng},
  journal={arXiv preprint arXiv:2308.10819},
  year={2023}
}

@article{rao2023tricking,
  title={Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks},
  author={Rao, Abhinav and Vashistha, Sachin and Naik, Atharva and Aditya, Somak and Choudhury, Monojit},
  journal={arXiv preprint arXiv:2305.14965},
  year={2023}
}

@article{das2018personalized,
  title={Personalized privacy assistants for the internet of things: Providing users with notice and choice},
  author={Das, Anupam and Degeling, Martin and Smullen, Daniel and Sadeh, Norman},
  journal={IEEE Pervasive Computing},
  volume={17},
  number={3},
  pages={35--46},
  year={2018},
  publisher={IEEE}
}

@article{ye2023natural,
  title={Natural language is all a graph needs},
  author={Ye, Ruosong and Zhang, Caiqi and Wang, Runhui and Xu, Shuyuan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2308.07134},
  year={2023}
}


@article{richardson2023integrating,
  title={Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models},
  author={Richardson, Chris and Zhang, Yao and Gillespie, Kellen and Kar, Sudipta and Singh, Arshdeep and Raeesy, Zeynab and Khan, Omar Zia and Sethy, Abhinav},
  journal={arXiv preprint arXiv:2310.20081},
  year={2023}
}

@article{he2023explanations,
  title={Explanations as Features: LLM-Based Features for Text-Attributed Graphs},
  author={He, Xiaoxin and Bresson, Xavier and Laurent, Thomas and Hooi, Bryan},
  journal={arXiv preprint arXiv:2305.19523},
  year={2023}
}

@article{jang2023personalized,
  title={Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging},
  author={Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2310.11564},
  year={2023}
}

@article{liu2023gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}

@article{morris2023text,
  title={Text Embeddings Reveal (Almost) As Much As Text},
  author={Morris, John X and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander M},
  journal={arXiv preprint arXiv:2310.06816},
  year={2023}
}

@article{huang2023harnessing,
  title={Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in Generation, Detection and Explanation},
  author={Huang, Yue and Sun, Lichao},
  journal={arXiv preprint arXiv:2310.05046},
  year={2023}
}

@article{lu2023large,
  title={Large Language Models can be Guided to Evade AI-Generated Text Detection},
  author={Lu, Ning and Liu, Shengcai and He, Rui and Tang, Ke},
  journal={arXiv preprint arXiv:2305.10847},
  year={2023}
}

@article{ziems2023explaining,
  title={Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection},
  author={Ziems, Noah and Liu, Gang and Flanagan, John and Jiang, Meng},
  journal={arXiv preprint arXiv:2310.19658},
  year={2023}
}

@article{zhang2023auto,
  title={Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models},
  author={Zhang, Zhihan and Wang, Shuohang and Yu, Wenhao and Xu, Yichong and Iter, Dan and Zeng, Qingkai and Liu, Yang and Zhu, Chenguang and Jiang, Meng},
  journal={arXiv preprint arXiv:2310.13127},
  year={2023}
}

@article{yu2023pre,
  title={Pre-training Language Models for Comparative Reasoning},
  author={Yu, Mengxia and Zhang, Zhihan and Yu, Wenhao and Jiang, Meng},
  journal={arXiv preprint arXiv:2305.14457},
  year={2023}
}

@article{ziems2023large,
  title={Large Language Models are Built-in Autoregressive Search Engines},
  author={Ziems, Noah and Yu, Wenhao and Zhang, Zhihan and Jiang, Meng},
  journal={arXiv preprint arXiv:2305.09612},
  year={2023}
}

@inproceedings{jiang2014detecting,
  title={Detecting suspicious following behavior in multimillion-node social networks},
  author={Jiang, Meng and Cui, Peng and Beutel, Alex and Faloutsos, Christos and Yang, Shiqiang},
  booktitle={Proceedings of the 23rd International Conference on World Wide Web},
  pages={305--306},
  year={2014}
}

@inproceedings{jiang2016catchtartan,
  title={Catchtartan: Representing and summarizing dynamic multicontextual behaviors},
  author={Jiang, Meng and Faloutsos, Christos and Han, Jiawei},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={945--954},
  year={2016}
}

@inproceedings{zhao2021action,
  title={Action sequence augmentation for early graph-based anomaly detection},
  author={Zhao, Tong and Ni, Bo and Yu, Wenhao and Guo, Zhichun and Shah, Neil and Jiang, Meng},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={2668--2678},
  year={2021}
}

@article{wang2021modeling,
  title={Modeling co-evolution of attributed and structural information in graph sequence},
  author={Wang, Daheng and Zhang, Zhihan and Ma, Yihong and Zhao, Tong and Jiang, Tianwen and Chawla, Nitesh and Jiang, Meng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  publisher={IEEE}
}

@inproceedings{jiang2012social,
  title={Social contextual recommendation},
  author={Jiang, Meng and Cui, Peng and Liu, Rui and Yang, Qiang and Wang, Fei and Zhu, Wenwu and Yang, Shiqiang},
  booktitle={Proceedings of the 21st ACM international conference on Information and knowledge management},
  pages={45--54},
  year={2012}
}

@article{jiang2014scalable,
  title={Scalable recommendation with social contextual information},
  author={Jiang, Meng and Cui, Peng and Wang, Fei and Zhu, Wenwu and Yang, Shiqiang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={26},
  number={11},
  pages={2789--2802},
  year={2014},
  publisher={IEEE}
}

@inproceedings{jiang2014catchsync,
  title={Catchsync: catching synchronized behavior in large directed graphs},
  author={Jiang, Meng and Cui, Peng and Beutel, Alex and Faloutsos, Christos and Yang, Shiqiang},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={941--950},
  year={2014}
}
