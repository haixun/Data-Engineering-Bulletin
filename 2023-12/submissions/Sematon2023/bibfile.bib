@misc{lupton2017self,
  title     = {Self-tracking, health and medicine},
  author    = {Lupton, Deborah},
  journal   = {Health Sociology Review},
  volume    = {26},
  number    = {1},
  pages     = {1--5},
  year      = {2017},
  publisher = {Taylor \& Francis}
}


@inproceedings{10.1145/3025453.3025838,
  author    = {Koesten, Laura M. and Kacprzak, Emilia and Tennison, Jenifer F. A. and Simperl, Elena},
  title     = {The Trials and Tribulations of Working with Structured Data: A Study on Information Seeking Behaviour},
  year      = {2017},
  isbn      = {9781450346559},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3025453.3025838},
  doi       = {10.1145/3025453.3025838},
  abstract  = {Structured data such as databases, spreadsheets and web tables is becoming critical in every domain and professional role. Yet we still do not know much about how people interact with it. Our research focuses on the information seeking behaviour of people looking for new sources of structured data online, including the task context in which the data will be used, data search, and the identification of relevant datasets from a set of possible candidates. We present a mixed-methods study covering in-depth interviews with 20 participants with various professional backgrounds, supported by the analysis of search logs of a large data portal. Based on this study, we propose a framework for human structured-data interaction and discuss challenges people encounter when trying to find and assess data that helps their daily work. We provide design recommendations for data publishers and developers of online data platforms such as data catalogs and marketplaces. These recommendations highlight important questions for HCI research to improve how people engage and make use of this incredibly useful online resource.},
  booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages     = {1277–1289},
  numpages  = {13},
  keywords  = {data search, human data interaction, data portal},
  location  = {Denver, Colorado, USA},
  series    = {CHI '17}
}

@inproceedings{liu2019task,
  title     = {Task, information seeking intentions, and user behavior: Toward a multi-level understanding of Web search},
  author    = {Liu, Jiqun and Mitsui, Matthew and Belkin, Nicholas J and Shah, Chirag},
  booktitle = {Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
  pages     = {123--132},
  year      = {2019}
}

@article{10.1145/357489.357513,
  author     = {Bell, Gordon},
  title      = {A Personal Digital Store},
  year       = {2001},
  issue_date = {Jan. 2001},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {44},
  number     = {1},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/357489.357513},
  doi        = {10.1145/357489.357513},
  journal    = {Commun. ACM},
  month      = {Jan},
  pages      = {86–91},
  numpages   = {6}
}

@article{birch2021data,
  title     = {Data as asset? The measurement, governance, and valuation of digital personal data by Big Tech},
  author    = {Birch, Kean and Cochrane, DT and Ward, Callum},
  journal   = {Big Data \& Society},
  volume    = {8},
  number    = {1},
  pages     = {20539517211017308},
  year      = {2021},
  publisher = {Sage Publications Sage UK: London, England}
}


@article{INR-033,
  url     = {http://dx.doi.org/10.1561/1500000033},
  year    = {2014},
  volume  = {8},
  journal = {Foundations and Trends in Information Retrieval},
  title   = {LifeLogging: Personal Big Data},
  doi     = {10.1561/1500000033},
  issn    = {1554-0669},
  number  = {1},
  pages   = {1-125},
  author  = {Cathal Gurrin and Alan F. Smeaton and Aiden R. Doherty}
}


@article{barnard2011exploring,
  title     = {Exploring the basis and boundary conditions of {SenseCam}-facilitated recollection},
  author    = {Barnard, Philip J and Murphy, Fionnuala C and Carthery-Goulart, Maria Teresa and Ramponi, Cristina and Clare, Linda},
  journal   = {Memory},
  volume    = {19},
  number    = {7},
  pages     = {758--767},
  year      = {2011},
  publisher = {Taylor \& Francis}
}

@article{berry2007use,
  title     = {The use of a wearable camera, {SenseCam}, as a pictorial diary to improve autobiographical memory in a patient with limbic encephalitis: A preliminary report},
  author    = {Berry, Emma and Kapur, Narinder and Williams, Lyndsay and Hodges, Steve and Watson, Peter and Smyth, Gavin and Srinivasan, James and Smith, Reg and Wilson, Barbara and Wood, Ken},
  journal   = {Neuropsychological Rehabilitation},
  volume    = {17},
  number    = {4-5},
  pages     = {582--601},
  year      = {2007},
  publisher = {Taylor \& Francis}
}

@article{harvey2016remembering,
  title     = {Remembering through lifelogging: {A} survey of human memory augmentation},
  author    = {Harvey, Morgan and Langheinrich, Marc and Ward, Geoff},
  journal   = {Pervasive and Mobile Computing},
  volume    = {27},
  pages     = {14--26},
  year      = {2016},
  publisher = {Elsevier}
}

@article{signal2017children,
  title     = {Children’s everyday exposure to food marketing: an objective analysis using wearable cameras},
  author    = {Signal, Louise N and Stanley, James and Smith, Moira and Barr, MB and Chambers, Tim J and Zhou, Jiang and Duane, Aaron and Gurrin, Cathal and Smeaton, Alan F and McKerchar, Christina and others},
  journal   = {International Journal of Behavioral Nutrition and Physical Activity},
  volume    = {14},
  pages     = {1--11},
  year      = {2017},
  publisher = {Springer}
}

[12]
@article{wilson2018use,
  title     = {The use of a wearable camera to explore daily functioning of older adults living with persistent pain: {M}ethodological reflections and recommendations},
  author    = {Wilson, Gemma and Jones, Derek and Schofield, Patricia and Martin, Denis J},
  journal   = {Journal of Rehabilitation and Assistive Technologies Engineering},
  volume    = {5},
  pages     = {2055668318765411},
  year      = {2018},
  publisher = {SAGE Publications Sage UK: London, England}
}

[13]
@article{nguyen2016recognition,
  title     = {Recognition of activities of daily living with egocentric vision: {A} review},
  author    = {Nguyen, Thi-Hoa-Cuc and Nebel, Jean-Christophe and Florez-Revuelta, Francisco},
  journal   = {Sensors},
  volume    = {16},
  number    = {1},
  pages     = {72},
  year      = {2016},
  publisher = {MDPI}
}
[14]
@article{everson2019can,
  title     = {Can wearable cameras be used to validate school-aged children’s lifestyle behaviours?},
  author    = {Everson, Bethan and Mackintosh, Kelly A and McNarry, Melitta A and Todd, Charlotte and Stratton, Gareth},
  journal   = {Children},
  volume    = {6},
  number    = {2},
  pages     = {20},
  year      = {2019},
  publisher = {MDPI}
}

[15]
@article{zhou2019use,
  title     = {The use of wearable cameras in assessing children's dietary intake and behaviours in {C}hina},
  author    = {Zhou, Qianling and Wang, Di and {Ní Mhurchu}, Cliona  and Gurrin, Cathal and Zhou, Jiang and Cheng, Yu and Wang, Haijun},
  journal   = {Appetite},
  volume    = {139},
  pages     = {1--7},
  year      = {2019},
  publisher = {Elsevier}
}


@article{doherty2012experiences,
  title     = {Experiences of aiding autobiographical memory using the {SenseCam}},
  author    = {Doherty, Aiden R and Pauly-Takacs, Katalin and Caprani, Niamh and Gurrin, Cathal and Moulin, Chris JA and O'Connor, Noel E and Smeaton, Alan F},
  journal   = {Human--Computer Interaction},
  volume    = {27},
  number    = {1-2},
  pages     = {151--174},
  year      = {2012},
  publisher = {Taylor \& Francis}
}
@article{tran2023comparing,
  title     = {Comparing Interactive Retrieval Approaches at the {Lifelog Search Challenge} 2021},
  author    = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Dang-Nguyen, Duc-Tien and Heller, Silvan and Spiess, Florian and Loko{\v{c}}, Jakub and Pe{\v{s}}ka, Ladislav and Nguyen, Thao-Nhu and Khan, Omar Shahbaz and Duane, Aaron and others},
  journal   = {IEEE Access},
  volume    = {11},
  pages     = {30982--30995},
  year      = {2023},
  publisher = {IEEE}
}


@inproceedings{tran2023vaisl,
  title        = {{VAISL: V}isual-Aware Identification of Semantic Locations in Lifelog},
  author       = {Tran, Ly-Duyen and Nie, Dongyun and Zhou, Liting and Nguyen, Binh and Gurrin, Cathal},
  booktitle    = {International Conference on Multimedia Modeling},
  pages        = {659--670},
  year         = {2023},
  organization = {Springer}
}


@article{tran2023mysceal,
  title     = {Mysc{\'e}al: a deeper analysis of an interactive lifelog search engine},
  author    = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Nguyen, Binh T and Zhou, Liting},
  journal   = {Multimedia Tools and Applications},
  pages     = {1--18},
  year      = {2023},
  publisher = {Springer}
}

@incollection{tran2023myeachtra,
  title     = {{MyEachtra}: Event-Based Interactive Lifelog Retrieval System for LSC’23},
  author    = {Tran, Ly-Duyen and Nguyen, Binh and Zhou, Liting and Gurrin, Cathal},
  booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
  pages     = {24--29},
  year      = {2023},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA}
}

@inproceedings{emysceal2022,
  author    = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Nguyen, Binh and Lee, Hyowon and Zhou, Liting and Gurrin, Cathal},
  title     = {{E-Mysc\'{e}al}: Embedding-Based Interactive Lifelog Retrieval System for LSC'22},
  year      = {2022},
  isbn      = {9781450392396},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3512729.3533012},
  doi       = {10.1145/3512729.3533012},
  booktitle = {Proceedings of the 5th Annual on Lifelog Search Challenge},
  pages     = {32–37},
  numpages  = {6},
  keywords  = {interactive retrieval system, human factors, lifelog},
  location  = {Newark, NJ, USA},
  series    = {LSC '22}
}

@incollection{nguyen2023lifeseeker,
  title     = {{E-LifeSeeker: A}n Interactive Lifelog Search Engine for LSC’23},
  author    = {Nguyen, Thao-Nhu and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal and Tran, Minh-Triet and Nguyen, Thanh Binh and Healy, Graham and Caputo, Annalina and Smyth, Sinéad},
  booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
  pages     = {13--17},
  year      = {2023},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}


@article{devlin2018bert,
  title   = {{BERT}: {P}re-training of deep bidirectional transformers for language understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}

@incollection{alam2023memento,
  title     = {Memento 3.0: {A}n Enhanced Lifelog Search Engine for {LSC’23}},
  author    = {Alam, Naushad and Graham, Yvette and Gurrin, Cathal},
  booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
  pages     = {41--46},
  year      = {2023},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA}
}


@article{janssen2020personal,
  title   = {Personal information management systems: a user-centric privacy utopia?},
  author  = {Janssen, Heleen and Cobbe, Jennifer and Singh, Jatinder},
  journal = {Published in Internet Policy Review (18 December 2020)},
  volume  = {9},
  number  = {4},
  pages   = {1--25},
  year    = {2020}
}

@inproceedings{jesus2020solid,
  title        = {{SOLID and PeaaS: Your Phone as a Store for Personal Data}},
  author       = {Jes{\'u}s-Azabal, Manuel and Berrocal, Javier and Laso, Sergio and Murillo, Juan Manuel and Garcia-Alonso, Jose},
  booktitle    = {Current Trends in Web Engineering: ICWE 2020 International Workshops, KDWEB, Sem4Tra, and WoT4H, Helsinki, Finland, June 9--12, 2020, Revised Selected Papers 20},
  pages        = {5--10},
  year         = {2020},
  organization = {Springer}
}


@article{s23031477,
  author         = {Fallatah, Khalid U. and Barhamgi, Mahmoud and Perera, Charith},
  title          = {{Personal Data Stores (PDS): A Review}},
  journal        = {Sensors},
  volume         = {23},
  year           = {2023},
  number         = {3},
  article-number = {1477},
  url            = {https://www.mdpi.com/1424-8220/23/3/1477},
  pubmedid       = {36772514},
  issn           = {1424-8220},
  abstract       = {Internet services have collected our personal data since their inception. In the beginning, the personal data collection was uncoordinated and was limited to a few selected data types such as names, ages, birthdays, etc. Due to the widespread use of social media, more and more personal data has been collected by different online services. We increasingly see that Internet of Things (IoT) devices are also being adopted by consumers, making it possible for companies to capture personal data (including very sensitive data) with much less effort and autonomously at a very low cost. Current systems architectures aim to collect, store, and process our personal data in the cloud with very limited control when it comes to giving back to citizens. However, Personal Data Stores (PDS) have been proposed as an alternative architecture where personal data will be stored within households, giving us complete control (self-sovereignty) over our data. This paper surveys the current literature on Personal Data Stores (PDS) that enable individuals to collect, control, store, and manage their data. In particular, we provide a comprehensive review of related concepts and the expected benefits of PDS platforms. Further, we compare and analyse existing PDS platforms in terms of their capabilities and core components. Subsequently, we summarise the major challenges and issues facing PDS platforms&rsquo; development and widespread adoption.},
  doi            = {10.3390/s23031477}
}


@article{tuovinen2022privacy,
  title     = {Privacy-aware sharing and collaborative analysis of personal wellness data: {P}rocess model, domain ontology, software system and user trial},
  author    = {Tuovinen, Lauri and Smeaton, Alan F},
  journal   = {Plos ONE},
  volume    = {17},
  number    = {4},
  pages     = {e0265997},
  year      = {2022},
  publisher = {Public Library of Science San Francisco, CA USA}
}


@article{lewis2020retrieval,
  title   = {Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author  = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {9459--9474},
  year    = {2020}
}

@article{krizhevsky2012imagenet,
  title   = {Imagenet classification with deep convolutional neural networks},
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {25},
  year    = {2012}
}
d
@article{vgg,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@inproceedings{szegedy2015going,
  title     = {Going deeper with convolutions},
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {1--9},
  year      = {2015}
}

@inproceedings{resnet,
  title     = {Deep residual learning for image recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {770--778},
  year      = {2016}
}
@article{krishna_visual_2016,
  title      = {Visual {Genome}: {Connecting} {Language} and {Vision} {Using} {Crowdsourced} {Dense} {Image} {Annotations}},
  shorttitle = {Visual {Genome}},
  abstract   = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.},
  journal    = {arXiv:1602.07332 [cs]},
  author     = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Li, Fei-Fei},
  month      = Feb,
  year       = {2016},
  note       = {arXiv: 1602.07332},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: 44 pages, 37 figures}
}

@inproceedings{deng2009imagenet,
  title        = {Imagenet: A large-scale hierarchical image database},
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages        = {248--255},
  year         = {2009},
  organization = {IEEE}
}

@article{kuznetsova2020open,
  title     = {The open images dataset v4: {U}nified image classification, object detection, and visual relationship detection at scale},
  author    = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
  journal   = {International Journal of Computer Vision},
  volume    = {128},
  number    = {7},
  pages     = {1956--1981},
  year      = {2020},
  publisher = {Springer}
}


@article{zhou2014learning,
  title   = {Learning deep features for scene recognition using places database},
  author  = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {27},
  year    = {2014}
}

@inproceedings{radford2021learning,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International Conference on Machine Learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PMLR}
}

@incollection{spiess2023best,
  title     = {The Best of Both Worlds: {L}ifelog Retrieval with a Desktop-Virtual Reality Hybrid System},
  author    = {Spiess, Florian and Gasser, Ralph and Schuldt, Heiko and Rossetto, Luca},
  booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
  pages     = {65--68},
  year      = {2023},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@inproceedings{tran_lifelog_2018,
  title     = {Lifelog Moment Retrieval with Visual Concept Fusion and Text-based Query Expansion},
  author    = {Tran, Minh-Triet and Truong, Thanh-Dat and Duy, Tung Dinh and Vo-Ho, Viet-Khoa and Luong, Quoc-An and Nguyen, Vinh-Tiep},
  booktitle = {CLEF (Working Notes)},
  year      = {2018}
}

@inproceedings{alsina2018interactive,
  title     = {An interactive lifelog search engine for {LSC} 2018},
  author    = {Alsina, Adri{\`a} and Gir{\'o}, Xavier and Gurrin, Cathal},
  booktitle = {Proceedings of the 2018 ACM Workshop on The Lifelog Search Challenge},
  pages     = {30--32},
  year      = {2018},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@inproceedings{nguyen2019two,
  title     = {A two-level lifelog search engine at the {LSC} 2019},
  author    = {Nguyen Van Khan, Isadora and Shrestha, Pranita and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
  booktitle = {Proceedings of the ACM Workshop on Lifelog Search Challenge},
  pages     = {19--23},
  year      = {2019}
}

@inproceedings{zhou_baseline_2017,
  location  = {New York, {NY}, {USA}},
  title     = {A Baseline Search Engine for Personal Life Archives},
  isbn      = {978-1-4503-5503-2},
  series    = {{LTA} '17},
  abstract  = {In lifelogging, as the volume of personal life archive data is ever increasing, we have to consider how to take advantage of a tool to extract or exploit valuable information from these personal life archives. In this work we motivate the need for, and present, a baseline search engine for personal life archives, which aims to make the personal life archive searchable, organizable and easy to be updated. We also present some preliminary results, which illustrate the feasibility of the baseline search engine as a tool for getting insights from personal life archives.},
  pages     = {21--24},
  booktitle = {Proceedings of the 2nd Workshop on Lifelogging Tools and Applications},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  author    = {Zhou, Liting and Dang-Nguyen, Duc-Tien and Gurrin, Cathal},
  date      = {2017-10-23},
  keywords  = {lifelogging, personal life archive, search engine},
year={2017}
}

@incollection{rossetto2023multi,
  title     = {{Multi-Mode Clustering for Graph-Based Lifelog Retrieval}},
  author    = {Rossetto, Luca and Inel, Oana and Lange, Svenja and Ruosch, Florian and Wang, Ruijie and Bernstein, Abraham},
  booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
  pages     = {36--40},
  year      = {2023},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@misc{bruton2019classification,
  title  = {{Classification of Everyday Living Version 1.0}},
  author = {Bruton, Paul and Langford, Joss and Reed, Matthew and Snelling, David},
  year   = {2019}
}

@article{vrandevcic2014wikidata,
  title     = {Wikidata: a free collaborative knowledgebase},
  author    = {Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal   = {Communications of the ACM},
  volume    = {57},
  number    = {10},
  pages     = {78--85},
  year      = {2014},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@inproceedings{dogariu_multimedia_nodate,
  title     = {{Multimedia Lab @ {ImageCLEF} 2018 Lifelog Moment Retrieval Task}},
  abstract  = {This paper describes the participation of the Multimedia Lab team at the {ImageCLEF} 2018 Lifelog Moment Retrieval Task. Our method makes use of visual information, text information and metadata. Our approach consists of the following steps: we reduce the number of images to analyze by eliminating the ones that are blurry or do not meet certain metadata criteria, extract relevant concepts with several Convolutional Neural Networks, perform K-means clustering on the Oriented Gradients and Color Histograms features and rerank the remaining images according to a relevance score computed between each image concept and the queried topic.},
  pages     = {13},
  author    = {Dogariu, Mihai and Ionescu, Bogdan},
  langid    = {english},
  booktitle = {CLEF (Working Notes)},
  year      = {2018}
}

@article{johnson2019billion,
  title     = {Billion-scale similarity search with {GPUs}},
  author    = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal   = {IEEE Transactions on Big Data},
  volume    = {7},
  number    = {3},
  pages     = {535--547},
  year      = {2019},
  publisher = {IEEE}
}

@inproceedings{tran2022exploration,
  title     = {An Exploration into the Benefits of the {CLIP} model for Lifelog Retrieval},
  author    = {Tran, Ly-Duyen and Alam, Naushad and Graham, Yvette and Vo, Linh Khanh and Diep, Nghiem Tuong and Nguyen, Binh and Zhou, Liting and Gurrin, Cathal},
  booktitle = {Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
  pages     = {15--22},
  year      = {2022}
}

@incollection{khan2021exquisitor,
  title     = {Exquisitor at the lifelog search challenge 2021: {R}elationships between semantic classifiers},
  author    = {Khan, Omar Shahbaz and Duane, Aaron and J{\'o}nsson, Bj{\"o}rn {\TH}{\'o}r and Zah{\'a}lka, Jan and Rudinac, Stevan and Worring, Marcel},
  booktitle = {Proceedings of the 4th Annual on Lifelog Search Challenge},
  pages     = {3--6},
  year      = {2021},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@incollection{lokovc2021enhanced,
  title     = {{Enhanced SOMHunter for known-item search in lifelog data}},
  author    = {Loko{\v{c}}, Jakub and Mejzlik, Franti{\v{s}}ek and Vesel{\`y}, Patrik and Sou{\v{c}}ek, Tom{\'a}{\v{s}}},
  booktitle = {Proceedings of the 4th Annual on Lifelog Search Challenge},
  pages     = {71--73},
  year      = {2021},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@incollection{schoeffmann2023lifexplore,
  title     = {{LifeXplore at the Lifelog Search Challenge 2023}},
  author    = {Schoeffmann, Klaus},
  booktitle = {Proceedings of the 6th Annual ACM Lifelog Search Challenge},
  pages     = {53--58},
  year      = {2023},
  publisher = {Association for Computing Machinery},  address   = {New York, NY, USA},
}

@inproceedings{tan-etal-2023-timelineqa,
    title = "{T}imeline{QA}: A Benchmark for Question Answering over Timelines",
    author = "Tan, Wang-Chiew  and
      Dwivedi-Yu, Jane  and
      Li, Yuliang  and
      Mathias, Lambert  and
      Saeidi, Marzieh  and
      Yan, Jing Nathan  and
      Halevy, Alon",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.6",
    doi = "10.18653/v1/2023.findings-acl.6",
    pages = "77--91",
    abstract = "Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information. We create and publicly release TimelineQA, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA with several state-of-the-art QA models. Our experiments reveal that for atomic queries, an extractive QA system significantly out-performs a state-of-the-art retrieval-augmented QA system. For multi-hop queries involving aggregates, we show that the best result is obtained with a state-of-the-art table QA technique, assuming the ground truth set of episodes for deriving the answer is available.",
}