\documentclass[11pt]{article}

\usepackage{deauthor,times,graphicx}
%\usepackage{url}

\begin{document}

{\it Data-centric Responsible AI} is becoming increasingly critical as AI is widely used in our everyday lives. In addition to simply improving model performance, it is important to make sure the trained model is trustworthy and responsible in the sense that it is fair, robust, private, secure, explainable, aligns with values, and more. Moreover, AI is only as good as its data, so we must take a data-centric approach and improve the data itself to fundamentally solve these problems. Recently applications like Large Language Models (LLMs) have remarkable performance largely because of the large amounts of data they are trained on, so data-centric research is only going to become more important in the future. This issue is thus timely and contains recent solutions by leading experts in this field.

The first three papers propose Data-centric Responsible AI methods that can be applied at different stages in the machine learning pipeline. The paper {\it Coverage-based Data-centric Approaches for
Responsible and Trustworthy AI} by Shahbazi et al. proposes data coverage methods to identify and resolve misrepresentation of minorities in data. The goal is to identify and resolve insufficient data coverage and generate data-centric reliability warnings to help data scientists determine if a prediction is reliable. Recent generative AI and foundation models can benefit from these techniques to effectively augment datasets with synthetic data. 
Next, the paper {\it Overcoming Data Biases: Towards Enhanced Accuracy and Reliability in Machine Learning} by Zhu and Salimi explores how causal modeling can improve the data cleaning, preparation, and quality management for machine learning. Causal reasoning can effectively identify and correct data biases resulting from missing data, confounding variables, and measurement errors and thus improve the fairness and accuracy of machine learning models.
Finally, the paper {\it Fairness and Robustness in Answering Preference Queries} by Roy outlines algorithmic challenges and directions for systematically changing the original aggregated output to satisfy different criteria related to fairness and robustness. The author considers different scenarios on how users provide their input preferences and how the individual preferences get aggregated.

The next two papers explore interesting domains that provide inspiration to further advance Data-centric Responsible AI. 
The paper {\it On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective} by Wang et al. performs a thorough evaluation of the robustness of ChatGPT and other LLMs from an adversarial and out-of-distribution perspective. While LLMs are receiving significant attention nowadays, their robustness to unexpected inputs is still understudied, which is a concern especially for safety-critical applications. The authors leverage multiple recent datasets for adversarial robustness and show that ChatGPT performs better and others, but also has much room for improvement.
The paper {\it Red Onions, Soft Cheese and Data:
From Food Safety to Data Traceability for Responsible AI} by Grafberger et al. makes the interesting analogy that data traceability for Responsible AI is akin to ensuring food safety. In particular, the U.S. Food and Drug Administration (FDA) detects outbreaks of foodborne illnesses, discovers contaminated food, and conducts traceback investigations through the food supply chain to determine the root cause and issue a comprehensive product recall. Taking inspiration from this process, the authors propose a data-centric vision for Responsible AI that involves prediction monitoring, data tracing, and identifying contaminated data and pipeline steps through audits.


Overall, these works represent the state-of-the-art data management approaches for Data-centric Responsible AI from various angles. We are just scratching the surface, and the data management community is well positioned to eventually realize this vision. 


% How cool would it be if we can record and recall every moment in our lives? Then we will never worry about remembering the answers to questions such as ``Where did I put my keys?'', ``I had some very good Korean hotpot a while back but which restaurant was that?'', 
% %``I must have seen this lady before but when and where?'', 
% or ``Does this skirt have higher quality and lower price than the similar-looking one I saw at Macy's yesterday?'' Asking such questions has been a dream for decades and dates back to Vannevar Bush's MEMEX (MEMory \& EXpansion) vision in 1945. Now with the rapid advancements in AI technology, especially in 2023, the Year of AI, are we getting closer to the dream of capturing and organizing personal information smoothly and accessing them effortlessly?

% To realize this dream, we face four challenges. First and foremost, we need to be able to {\bf capture personal information}. Scopewise, do we capture mainly digital footprints of a user (messages, browsing and purchase history, and app usage logs), or also physical aspects of a user (what she sees and experiences, and the time and location of those) through special devices? For the latter, what should be the frequency of capturing of data so it answers our needs? Second, we need to be able to {\bf integrate personal data}, often from different sources, organize them in a meaningful way and find a way to store the potentially large volume of captured data. Understanding of the semantics and effective aggregation are the keys, but achieving them is non-trivial. Third, we shall be able to effectively {\bf leverage personal data} to make life easier or memorable for the owner in future; afterall, the data is worthless unless we can effectively use it. The usage includes helping users easily retrieve past memory, and offering recommendations of books, articles, and products that they may enjoy. Solution to this challenge also decides the solutions to previous challenges, such as how we would like to present and store the data, how much data we really need to capture, and whether we shall have an aging mechanism to ensure efficient usage of the past data and limit its effect on future recommendations. Last but not least, {\bf privacy} guarantees are a must-have for trustworthy personal information management, to use it for good, not for harm.

% This issue collects a set of papers around personal information management shedding lights on how to address the aforementioned challenges. We start with a paper by {\bf Tran et al.}, describing lifelogging of user activities from both online and offline in the real world, and sharing the learnings from the annual Lifelog Search Challenge Benchmarking Workshops. The logged data include both our digital footprints, and information collected by devices and sensors to track heart rate, sleep quality, locations, and so on. In the second paper, {\bf Kalokyri et al.} focused on digital data and described the YourDigtalSelf project. The project collects users' digital traces from different applications, integrates them, reconstructs meaningful episodes from them, and supports memory recall using the episodes. The next two papers focus on particular domains and discuss how to build a personal knowledge graph to support applications: {\bf Chakraborty et al.} described Personal Research Knowledge Graph to organize important information for scientists, such as grants, lab equipments, papers, journal and conferences; {\bf Nidhi et al.} described G-VARS for gun violence susceptibility analysis, through information collected on behavior, mental health, access to firearms, social networks and online activities. We next come to how to apply personal information in applications. {\bf Sun et al.} described QALinkPlus, a novel graph-based method that constructs an entity co-occurrence graph derived from QA datasets, describes context QA-specific subgraphs, and leverages this to improve QA, especially personalized QA. The last paper in this issue is an insightful survey User Modeling in the Era of LLMs, where {\bf Tan and Meng} surveyed user modeling leveraging LLM techniques, and pointed our new research directions for personalized search and recommendation.

% Overall, the above papers represent an interesting sample of the ongoing work on the new trend of personal information management. We hope that this special issue will further help and inspire the research community in its quest to solve this challenging problem. We would like to thank all the authors for their valuable contributions, as well as Haixun Wang for giving us the opportunity to put together this special issue, and Nurendra Choudhary for his help in its publication.
\end{document}
