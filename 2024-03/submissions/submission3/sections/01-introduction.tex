% !TEX root = ../main.tex
\section{The Need for a Data-Centric Perspective on Responsible~AI}
\label{sec:intro}

Software systems that learn from data with AI and machine learning (ML) are becoming ubiquitous and are increasingly used to automate impactful decisions. The risks arising from this widespread use are garnering attention from policymakers, scientists, and the media, and lead to the question of what data management research can contribute to reduce the dangers and malfunctions of data-driven AI/ML applications. 

\header{AI/ML malfunctions threaten vulnerable populations} In recent years, we have been regularly alarmed by media reports about the harm potential of faulty AI/ML systems in devastating real-world incidents. Examples include failures of automated decision-making systems, e.g., an eight-month pregnant woman in Detroit was mistakenly arrested based on a faulty prediction from a facial recognition system, held in jail for several hours and needed medical care upon her release~\cite{aiface2023}. Another example is that one of the largest health insurers in the US allegedly applies a faulty AI model with a 90\% error rate to deny critical health care services to elderly patients~\cite{aihealth2023}. The recent rise of generative AI produces new types of harm as well. A recent study of AI detection tools, for example, found that these systems are biased against non-native English speakers~\cite{aicheating2023} and often falsely accuse international students of cheating. Furthermore, an AI supermarket meal planner recently went rogue and suggested a recipe that would create chlorine gas~\cite{airecipe2023}.


\header{Technical bias in ML applications} The reasons that data-driven systems are susceptible to producing unfair, harmful outcomes are multi-faceted~\cite{stoyanovich2022responsible,whang2021responsible,groth2013transparency}, as we are ultimately dealing with socio-technological systems~\cite{birhane2021large}, which suffer from various types of bias~\cite{friedman1996bias}. In this work, we focus on \textit{technical bias}, which arises from the design decisions and operations in a technical system itself. Such bias is not well understood, especially in the context of large end-to-end systems, which include data preparation and data cleaning stages, deployed models and feedback loops. Recent research on technical bias identifies issues such as the lack of sufficient, representative training data for certain demographic groups~\cite{lin2020identifying,asudeh2019assessing,chen2018my}, biased training data with undesirable stereotypes~\cite{birhane2021multimodal} or unintended side effects from automated data cleaning operations~\cite{guha2024automated,tae2019data,shahbazi2023through}.


\header{Existing and upcoming regulation} The dangers arising from data-driven AI/ML applications have been recognised by regulators and lawmakers several years ago already, and led to the introduction of regulation all over the world. The ``General Data Protection Regulation'' (GPDR) in Europe, for example, grants citizens the right to find out what information an organisation has about them and to issue deletion requests for their data as part of the ``right-to-be-forgotten''~\cite{GDPRart17,GDPRrec74}. The upcoming European AI Act~\cite{euaiact} will be the first comprehensive regulation for the application of AI/ML in Europe. This act is expected to outlaw the usage of ML in selected application areas and to strongly regulate its application in certain other areas. It defines different levels of risk in AI usage scenarios and imposes a set of comprehensive technical requirements, such as ``logging of activity to ensure traceability of results'', ``detailed documentation providing all information necessary on the system and its purpose for authorities to assess its compliance'', and ``appropriate human oversight measures to minimize risk''. We note that outside Europe, similar regulations are being adopted~\cite{cppa,digichina}.

\header{The need for a data-centric perspective} Unfortunately, as evidenced by the media reports cited previously, we currently lack the ability to efficiently implement technical measures to detect and mitigate the harms present in AI/ML applications. This is confirmed by a recent survey study with industry practitioners~\cite{holstein2019improving}, which outlines several alarming shortcomings in addressing fairness and bias issues. The interviewed practitioners report that academic research on de-biasing models falls short of addressing their concerns and often falsely ``view[s] the training data as fixed'', while they ``consider data collection, rather than model development, as the most important place to intervene''. At the same time, only ``65\% of survey respondents [...] reported that their teams have some control over data collection and curation'', and the study also finds a high demand for future research to ``support [...] practitioners in [...] curating high-quality datasets''. Another example of the dire situation in the industry is a recent court case against Facebook~\cite{facebookdata}, where two veteran engineers of the company told the court that the company does not keep track of the exact locations where personal data is stored and processed.

In the research community, several widely used training datasets for computer vision, such as LAION-5B~\cite{schuhmann2022laion} or TinyImages~\cite{torralba200880}, have been taken offline after the discovery of highly problematic content in them~\cite{birhane2021large,birhane2023hate}. Moreover, it is unlikely, though, that all models that had been trained on these problematic datasets have been retracted as well. For the current wave of closed, proprietary pretrained models available behind commercial APIs, the situation is even worse, as we do not even have a way to determine what data they have been trained on.

\header{Paper inspiration} In order to find inspiration for the outlined questions and challenges, we take a look into safety measures outside of the computer science domain, as our societies have had to deal with the dangers of complex and distributed technical processes for a long time already. In particular, we discuss how the U.S. Food and Drug Administration (FDA) combats the outbreaks of foodborne illnesses (\Cref{sec:inspiration}). We ask ourselves what we can learn from the millennial pursuit of food safety. What type of technical and regulatory frameworks exist such that we trust what we put on the table for our family every day? We use the FDA's established processes as an inspiration for a data-centric vision towards responsible~AI in \Cref{sec:vision}, with the goal to obtain the same level of trust for our data products that we have for our food.






