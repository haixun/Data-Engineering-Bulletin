% !TEX root = ../main.tex
\subsection{Prediction Monitoring}
\label{sec:vision-monitoring}

As detailed in \Cref{sec:inspiration}, the FDA monitors a database of DNA profiles of bacteria for geographic patterns to detect outbreaks. This raises the question of whether large institutions or companies could use similar methods to detect fairness issues with deployed models and ML pipelines early. In the following, we outline three directions which we deem crucial for this endeavour.

\header{Identifiable predictions} The ``end product'' of AI/ML applications are predictions on unseen data, which are received by end-users or downstream applications in an organisation. Any detection of problems with the application or its data, as well as any potential audit has to start from these predictions, similar to how disease detectives need to determine the type of food that people consumed before they became sick. However, in current systems, predictions are often rather ephemeral. As a first step towards auditable AI/ML applications, their predictions should come with unique identifiers, analogous to the TLC of food in food supply chains. Such identifiers should be assigned in a way that allows for the retrospective identification of the state of the AI/ML application (e.g., the software version and currently deployed model version, etc.) from which a prediction was generated. Based on these identifiers, users and downstream consumers could raise concerns about a particular prediction, and an investigating party (e.g., a dedicated responsible AI team in a large organisation) could start an audit of the system.

\headerl{State-of-the-art} In MLOps, the benefits of identifiable predictions are being recognised among industry practitioners~\cite{tensorflowPatterns21,howToMonitor23}. However, current approaches require high expertise and custom implementations~\cite{tensorflowPatterns21}. Even rudimentary tasks such as tracking the corresponding code and model versions are challenging~\cite{traceabilityAndReproducibility23}. To fully benefit from identifiable predictions, e.g., for rectifying erroneous predictions, it is essential to integrate prediction identifiers with the associated metadata and provenance records encompassing ancillary pipeline stages such as data preprocessing. However, the current implementation complexity leads us to believe that the adoption of these techniques in practice is rather low.
  
\headerl{Open questions and challenges} Enhancing and maintaining traceability and reproducibility in ML applications requires that practitioners manually integrate, configure, and orchestrate various disparate systems~\cite{traceabilityAndReproducibility23,tensorflowPatterns21}. The resulting one-off solutions require further time- and cost-intensive development effort to enable monitoring and output explanation. We argue that standardised interfaces would be essential to seamlessly integrate existing and new ML operations techniques with identifiable predictions.
%
We will also discuss further provenance-related challenges for fine-grained data tracing in end-to-end ML pipelines in Section~\ref{sec:vision-tracing}. 


\header{Detecting and collecting predictions with fairness issues} Even with identifiable predictions, an open question is how to reliably detect fairness issues of an ML application at deployment time. Ideally, such issues should already be caught by pre-deployment evaluations, but media reports and industry surveys show that this is rarely the case. Furthermore, it would be crucial to have a ``database'' of common issues and examples of unfair / unreliable predictions in production ML deployments, e.g., at a company-wide level. Given a comprehensive catalog of such issues and an efficient way to monitor live predictions for fairness, we could build automatable detection mechanisms similar to the outbreak detection techniques in PulseNet (\Cref{sec:inspiration}).

\headerl{State-of-the-art} A lot of recent work has focused on detecting changes in the overall distribution of the predictions or changes between the training and serving data~\cite{nigenda2022amazon}. At serving time, systems like Tensorflow Serving~\cite{olston2017tensorflow} for example employ so-called ``canary models'' to detect cases where the predictions differ between previous and newly deployed models, and several techniques analyse the distribution of the predicted labels to detect changes in the data~\cite{lipton2018detecting,schelter2020learning}. However, none of these techniques have a particular focus on determining fairness issues, which may occur in small subsets of the data only. 

Orthogonal to that, several techniques to debug prediction data offline have been developed, e.g., to detect slices of the data where a model works less well~\cite{chung2019slice,sagadeeva2021sliceline}. These approaches require simultaneous access to the model, the featurised prediction data and additional demographic side data however, which makes their application difficult in practice, especially for teams not owning the underlying AI/ML application.


\headerl{Open questions and challenges} A major difficulty in monitoring a deployed system for fairness is that the group membership information for individual predictions must be known to maintain corresponding fairness metrics. Such group membership information (e.g., about the race or gender identity of the persons involved in the predictions) is very sensitive and private information, to which a deployed serving system should ideally not even have access. Furthermore, regulation like the EU AI Act enforces strict rules for which parts of an AI/ML application such data can be used for at all. We envision that large organisation may want to create dedicated infrastructure for such cases, where predictions with identifiers from different applications are collected, the corresponding fairness metrics are maintained and SliceFinder-like algorithms~\cite{chung2019slice} are run continuously to look for subsets of the prediction data with potential issues.

A large corpus of real-world predictions from ML systems with fairness issues would also greatly enhance the ability of the academic community to work on these problems. However, it is difficult to collect such a corpus of predictions and issues due to the inherent sensitive, privacy-critical nature of the data. There are some ongoing efforts to (manually) create a comprehensive repository of ``AI incidents''~\cite{mcgregor2021preventing}, yet the underlying technical details and prediction data of the incidents are not available.


\header{Monitoring generative models for representational harms} A large part of the existing fairness literature focuses on so-called ``allocative harms'' in automated decision-making systems, which decide upon access to certain resources such as job interviews, loans or medical prioritisation~\cite{stoyanovich2022responsible,holstein2019improving}. It is difficult to choose an appropriate fairness metric for such cases, as such a choice always implies a values-based decision and trade-offs~\cite{narayanan21fairness}. On the technical side however, computing these metrics is straightforward (given access to the required data), as one essentially only has to maintain separate confusion matrices for the predictions for the groups of interest~\cite{guha2024automated}. With the rise of generative models however, we are being faced with so-called ``representational harms''~\cite{holstein2019improving}, which occur for example when generative models reproduce sexist or racist stereotypes in the images or text that they generate. 


\headerl{State-of-the-art} There is a large body of targeted studies in the NLP community, where researchers uncovered a variety of biases and stereotypes in pretrained language models. Examples include sexist stereotypes and gender bias~\cite{lucy2021gender,sheng2019woman}, anti-muslim bias \cite{abid2021persistent}, and undesirable biases towards mentions of disability~\cite{hutchinson2020social}. It is however unclear how to translate the detection capabilities of these customly designed studies into monitoring techniques for deployed real-world systems. A first interesting step in this direction is the recently proposed Spade~\cite{shankar2024spade} system, which learns assertions for safeguarding LLM outputs based on the version history of prompt edits. 

\headerl{Open questions and challenges} Due to the unpredictable nature of large generative models, generating adequate assertions or ``data unit tests'' to check for bias in their output remains a complex challenge. Having too few assertions potentially might make a system miss biased outputs, leading to unfair outcomes, while having too many assertions could slow down the system and lead to many false alarms. We expect that future approaches will generate data unit tests from predefined templates, based on manually defined assertion criteria. An orthogonal approach are so-called ``safety classifiers'' \cite{xu2021bot, dinan2022safetykit,markov2023holistic}, where a secondary model is employed to assess the outputs of a primary model for safety. Prior to the deployment phase, data will be collected where generative models are intentionally probed to induce errors, which will then be used to train a classifier to detect biased behavior.


