% !TEX root = ../main.tex
\subsection{Tracing Data Through End-to-End AI/ML Applications}
\label{sec:vision-tracing}

Complex food supply chains span the globe and a single ingredient (like red onions in the example from~\Cref{sec:inspiration}) may end up in multiple end products. This makes tracing such ingredients a complicated and expensive undertaking. The FDA addresses this challenge with targeted tracing requirements which focus on only retaining tracing data for high-risk ingredients on the food traceability list (\Cref{sec:inspiration}). While tracking the provenance of data in data processing systems is a decades-old research area~\cite{tan2007provenance}, there is still little practical adoption of these techniques in real-world systems, mainly due to the incurred performance overhead of comprehensively tracking provenance through all kinds of queries, especially when they contain aggregations~\cite{amsterdamer2011provenance}.
%
Similar to the FDA's list of high-risk ingredients, the EU AI Act~\cite{euaiact} defines high-risk AI application domains, such as CV-sorting software for recruitment procedures, credit scoring denying citizens the opportunity to obtain a loan or the verification of the authenticity of travel documents. In the following, we discuss ideas for efficiently applying provenance tracking to the data pipelines in such scenarios.


\header{Selective and focused provenance tracking} As already mentioned, tracking fully fine-grained semiring provenance~\cite{green2007provenance,amsterdamer2011provenance} for every input row imposes a high performance overhead. In the food supply chain, provenance tracking focuses on predefined ``Critical Tracking Events'', which are the points in the supply chain that are crucial later for audits. We need to adopt such a methodology as well for data pipelines, which would enable us to restrict the provenance tracking efforts to data exchange and transformation operations, which actually impact the information required to audit an AI/ML application later. Furthermore, for each high-risk AI application scenario, we could define the tracking granularity, the key transformations to focus on and the information required per transformation event.  The minimum granularity of the provenance should be tailored for each use-case. For demographic data, provenance at the level of individuals might be sufficient, for facial recognition applications, more fine-grained provenance at the level of individual images may be required, however.


\headerl{State-of-the-art} In recent years, several techniques have been proposed to model ML pipeline operations and to apply database-style provenance tracking for Python code, for example via runtime instrumentation as part of mlinspect~\cite{grafberger2022data} or via static analysis as part of Vamsa~\cite{namaki2020vamsa}. These approaches have been extended in various ways, e.g., for data debugging via Shapley values~\cite{karlavs2022data} or pipeline screening during continuous integration~\cite{schelter2023proactively}. A drawback of these methods is that they rely on heuristics and well-written, declarative code to be able to infer the semantics of the pipeline operations, which leaves it unclear whether they can reliably be applied to low-quality code as well. Another family of systems, which include Amazon's ExperimentTracker~\cite{Schelter2017} and mltrace~\cite{shankar2022observability}, uses a more robust approach for provenance tracking as they require manually annotated code. Unfortunately, this puts a heavy burden on developers, who will, in our experience, often forego the additional effort of putting detailed annotations on their code under time pressure. We expect that even coming up with high-level ``traceability plans'' for large AI/ML applications will be challenging in practice, since these applications often orchestrate different systems and libraries with workflow managers like Apache Airflow~\cite{airflow}.

\headerl{Open questions and challenges} In our eyes, the biggest challenge in this space is to find ways to reduce the implementation-, annotation-, and runtime overhead for provenance tracking in ML pipelines, while guaranteeing a high level of correctness and robustness. For industry applications, we can neither rely on trying to handle arbitrary code nor on forcing developers to always manually annotate their code. An interesting middle ground may be the use of pipeline templates, as pioneered by the mlflow recipes project~\cite{zaharia2018accelerating}, which forces developers to modularise their code into pipeline steps with known semantics and predefined inputs and outputs, but still gives them the freedom to write arbitrary code inside the steps. Unfortunately though, the real-world adoption of these templating approaches is unclear at the moment. Nevertheless, such templates might be a natural point to implement general robust provenance tracking. Analogous to the traceability plans required for food chain tracking, we could define traceability templates for high-risk AI scenarios, with steps, provenance tracking, and logging requirements specific to the particular use case. 

To reduce the runtime overhead of provenance tracking, it may be worthwhile to take a deeper look at several common aggregation operations in ML pipelines, like one-hot-encoding a particular column or normalising a feature. While these operations technically conduct a global aggregation followed by a map transformation (in dataflow terms), we may be able to ignore the aggregation part for tracking provenance, as we already know that they do not remove rows and introduce an all-to-all provenance relationship onto the transformed feature values. Similar techniques are already applied in DataScope~\cite{karlavs2022data} and ArgusEyes~\cite{schelter2023proactively} to approximate ML pipelines as queries in the positive relational algebra. A future challenge here is to define a restricted subset of operations for ML pipelines, which still allows the implementation of a large class of ML applications, but drastically simplifies provenance tracking.


Identifiable predictions, as discussed in Section~\ref{sec:vision-monitoring}, also present new challenges with respect to ML provenance research. Existing experiment tracking tools like mlflow~\cite{zaharia2018accelerating} already link predictions to high-level artifacts such as models and source code. However, we think that record-level provenance is required to effectively reconstruct the necessary data for a prediction. Given a prediction identifier, we would like to be able to automatically retrieve all relevant inference inputs, data preprocessing steps, the model version employed for inference, and, if necessary, all information about the training pipeline and its input data. While existing research partially addresses provenance tracking and versioning in static pipelines with static input data, further challenges remain for pipelines in dynamic production environments with continuously trained models~\cite{baylor19tfxContinous} and evolving retrieval corpora~\cite{Guo16traditionalIR,chend23dynamicCorpora,bleifuss2018change}, where provenance has to be maintained incrementally.


Another open question is the impact of data cleaning and integration operations on the fairness of AI/ML applications. Several experimental studies indicate that data wrangling and integration operations such as missing value imputation, outlier removal, or entity matching can sometimes negatively impact the fairness of models trained on the resulting data~\cite{guha2023automated,li2021cleanml,tae2019data,shahbazi2023through}. However, we currently lack a detailed understanding of this impact, especially since the outcome seems to heavily depend on the chosen fairness metric and group definition. Furthermore, determining such impact is hard in practice without access to the downstream models.

An orthogonal challenge in this area is the tension between detailed provenance tracking and the protection of private user data. Provenance tracking requires storing information about the intermediate outputs of pipeline operations and must additionally maintain sensitive metadata such as demographic group memberships of certain records to be able to quantify the fairness impact of different operations. In many cases, such sensitive metadata may not be accessible in inference systems at prediction time, for example, and measures must be taken to ensure that these sensitive attributes are only used for testing models but not for training them~\cite{euaiact}. To the best of our knowledge, current ML platforms lack support for such use cases.


\header{Provenance of data in pretrained and fine-tuned models} Academic ``textbook'' ML commonly assumes that a single dataset is used to create a particular ML model, which implies that we would only need to track the provenance of this source data through the corresponding ML pipeline. However, this assumption has never held up for real-world deployments, which typically leverage a variety of data sources as input for a pipeline and often apply ML already as part of the preprocessing of this data. Twitter's recommender system for example aggregates multiple input networks (representing likes, follows etc on the platform) into a common network dataset called RealGraph~\cite{kamath2014realgraph}, via a dedicated classifier that estimates the interaction probability between different users of the network. Several recommendation algorithms consume this aggregated dataset instead of the raw input datasets and the provenance of an interaction such as a like or follow is unclear after the transformation. This problem is exacerbated nowadays due to the prevalence of large pretrained models, which are downloaded from repositories such as HuggingFace and tailored to a particular ML use case via fine-tuning. In the majority of cases, the connection to the underlying training data becomes unclear after fine-tuning, as the current infrastructure does not keep track of the relationships between models. It would, for example, be difficult to identify all computer vision models that originate from the recently retracted LAION dataset. The situation is even worse for non-open source models created by commercial companies, where the underlying training data is not known for the base model already.

\headerl{State-of-the-art} Common methods to voluntarily document the origin of data and ML models are datasheets~\cite{gebru2021datasheets} and model cards~\cite{mitchell2019model}. These are a form of manually created, semi-structured documentation, which is, for example, in use at the popular model and data repository HuggingFace.
%
Tensorflow ML Metadata~\cite{Katsiapis2020tfx} is a library for recording and retrieving metadata associated with ML workflows. The Model Card Toolkit~\cite{fang2020introducing} supports the creation of Model Cards and can also use metadata from ML Metadata to prepopulate information such as class distributions and performance metrics. DAG Cards~\cite{tagliabue2021dag}, inspired by model cards, have also been proposed as a form of documentation, which can be automatically generated from ML pipeline code~\cite{berg2019open}. Experiment tracking tools like mlflow~\cite{zaharia2018accelerating} can log metadata as a starting point for creating documentation for ML models. OpenML~\cite{rijn2013openml} is a popular platform for sharing datasets, ML tasks, workflows, and experimentation runs. While it supports documentation like a dataset description for dataset uploads~\cite{OpenMLDatasetUpload}, it does not enforce their quality and prioritises a frictionless user experience over documentation completeness. However, OpenML automatically analyses uploaded datasets to compute additional data quality statistics. For ML pipelines, it relies on extensions for popular libraries like scikit-learn that can automatically create a serialisable description~\cite{OpenMLDocs}. Systems like Macaroni~\cite{li2023macaroni} allow querying the existing metadata in open repositories, based on a unified representation~\cite{li2023metadata}.

\headerl{Open questions and challenges} The main drawback of model cards and datasheets is that creating and maintaining helpful documentation still mostly depends on the goodwill of the parties involved in the creation of the models and the data. Most importantly, this documentation is not machine-readable in a way that would make it easy to audit and/or verify the claims made about the provided models and data. As discussed, models are nowadays often downloaded and fine-tuned programmatically (e.g., via the popular transformers library from HuggingFace~\cite{huggingtransformers}). Such packages and the underlying infrastructure pose a direct opportunity to automate provenance tracking and to record the relationships between models. The semi-automated metadata collection tools can export implementation details for reproducing experiments, however, they still put the burden to extract information about the ML pipelines and models onto the users. Recently proposed approaches such as mlwhatif~\cite{grafberger2023mlwhatif} might be a starting point to automatically extract meaningful metadata, e.g., for nutritional labels in ranking~\cite{yang2018nutritional,stoyanovich2019nutritional}.


Another recent trend are parameter-efficient fine-tuning methods~\cite{hu2021lora,li2021prefix,liu2023gpt,lester2021power}, which do not create a full model copy, but only learn a continuous prompt or an ``adapter'' to the model. In such cases, we would need to track provenance on the level of these prompts and adapters (which might later even be further combined~\cite{shah2023ziplora}). A final challenge with tracking the provenance of data in generative models is that many large datasets commonly used for these models (e.g., LAION~\cite{schuhmann2022laion} or gitschemas~\cite{dohmen2022gitschemas}) for generative models consist of links to resources on the web, which are often crawled and filtered to build a custom dataset. This filtering process must also be taken into account for provenance.


