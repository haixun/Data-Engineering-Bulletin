% !TEX root = ../main.tex
\subsection{Identifying ``Contaminated'' Data and Pipeline Steps Through Audits}
\label{sec:vision-audit}

It is still unclear how to efficiently and comprehensively audit AI/ML applications; see~\cite {birhane2024ai,sandvig2014auditing} for a discussion on the current state of this endeavor. Due to our data-centric perspective, we focus on issues and directions for quantitative data audits only.
%
As discussed in \Cref{sec:inspiration}, traceback investigations in the food supply chain allow disease detectives to audit these supply chains, identify the point of contamination, and ultimately remove the source of contamination by issuing comprehensive recalls for all affected end products. How can we audit AI/ML applications in a similar manner, based on the provenance information from \Cref{sec:vision-tracing}? Ideally, we would like to be able to quickly identify ``contaminated'' data and intermediate outputs, which, for example, contain unwanted stereotypes or has been rendered unrepresentative due to biased filtering operations. Once such contaminated data is identified, an audit would furthermore need to determine which models and predictions were affected and need to be retracted and/or recomputed. Furthermore, such data-centric audits should be able to answer a larger set of related questions about the robustness and regulatory compliance of an AI/ML application. Examples of such questions are what data and features were used by the application and whether this usage was in line with legal requirements (e.g., from the EU AI Act~\cite{euaiact}), or whether the application follows the timely data deletion requirements imposed by the right to be forgotten from GDPR~\cite{GDPRart17}. Furthermore, audits should be able to assess whether an application is robust enough against potential errors and changes in the data, and whether appropriate measures have been taken to quantify and control the fairness of its predictions. 


\headerl{State-of-the-art} The validation of ML data in popular ML platforms such as Google TFX~\cite{baylor2017tfx} or Amazon SageMaker~\cite{nigenda2022amazon} relies on libraries such as Tensorflow Data Validation (TFDV)~\cite{breck2019data} and Deequ~\cite{schelter2018automating,schelter2019differential}, which generate validation rules based on heuristics and data profiling. Related approaches are to ``lint'' ML data based on well-known practical issues~\cite{hynes2017data}. Follow-up work to these approaches~\cite{redyuk2021automating,shankar2023automatic} applies a technique called ``partition summarisation'' to learn to spot data with potential quality issues by applying anomaly detection based on the statistics of previously observed data partitions.

There has been extensive research on cleaning datasets, e.g., ~\cite{Ziawasch2016errors,mahdavi2019raha,jager2021benchmark,narayan2022can}. Furthermore, the data-centric AI community started developing related techniques that jointly consider the ML model and data to address inaccuracy, bias, and fragility in real-world ML applications and are tackling tasks such as training set selection and data acquisition~\cite{mazumder2023dataperf}. Many of the techniques in this space rely on data influence estimation techniques~\cite{hammoudeh2022training}, in particular on (an estimate of) the leave-one-out error or data Shapley value~\cite{ghorbani2019data}, which is either computed via extensive retraining or influence functions~\cite{pmlr-v70-koh17a}. Such techniques are the basis of several recently proposed data debugging methods like Rain~\cite{wu2020complaint}, Gopher~\cite{pradhan2022interpretable} or DataScope~\cite{karlavs2022data}. 
%
A related line of work tackles ML pipelines and employs light-weight provenance tracking and automatic instrumentation of Python code to assess technical bias introduced by sudden distribution shifts~\cite{grafberger2022data,grafberger2021demo}, data leakage and fairness issues~\cite{schelter2022screening,schelter2023proactively}, as well as robustness to erroneous input data~\cite{grafberger2023mlwhatif,grafberger2023demo}. 


\headerl{Open questions and challenges} 
Unfortunately, neither TFDV nor Deequ have a particular focus on identifying fairness and bias issues in the data, and require a relatively high user expertise and knowledge of the underlying domain to adjust and filter the suggested validation rules. It would be crucial to find ways to guide users in designing compliance- and fairness-related data unit tests with these libraries. 

Furthermore, the existing methods for estimating the influence of training samples are extremely restricted in terms of efficiency, scalability or applicability. In general, there exist two families of methods: Retraining-based methods are applicable to any model class, but require extensive retraining of the ML model on a large number of subsets of the data. Even retraining a model a few hundred times for a large dataset is infeasible in practice. The second family are gradient-based methods, which require no retraining but are only applicable to certain model classes due to assumptions of convexity~\cite{pmlr-v70-koh17a} or linearity~\cite{yeh2018representer}, and are still rather compute-heavy, as they often require to compute a ``Hessian vector product'' for each combination of a training and validation sample~\cite{hammoudeh2022training}. Some exciting progress has been made in terms of scalability, e.g., on efficiently computing the Data Shapley value~\cite{ghorbani2019data} for kNN proxy models~\cite{jia2019towards}. However, these techniques are only applicable to certain utility functions but, for example, not to common ranking-based metrics in information retrieval.

The work from the data-centric AI community is promising. However, challenges such as ML pipelines with complex data preprocessing operations are often overlooked, and automatically applying these techniques to ML pipelines is still an open challenge~\cite{grafberger2023towards}. The approaches for the holistic screening of ML training pipelines rely on well-written code, which is often an unrealistic assumption in practice. 

On the engineering side, we should strive to design a standardised API for provenance-based data auditing and incident investigation, which could be integrated into popular projects such as Google TFX, mlflow recipes, or SageMaker. Based on such an API, the academic and open source community could develop general auditing software to greatly reduce the costs of such audits.
