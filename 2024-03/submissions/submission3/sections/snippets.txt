\begin{itemize}[leftmargin=*,noitemsep]
  \item On the regulation side, the right to explanation has been a heavily debated topics that already led to several laws. In the US, under the Equal Credit Opportunity Act, applicants have a right to an explanation for denied credit requests. In France, the Digital Republic Act also introduced a provision for "decisions taken on the basis of an algorithmic treatment" and gives citizens the right to learn about rules that define that treatment and its “principal characteristics” (except in domains like national security or defence). GDPR also contains regulations regarding a right to explanation, but the extends to which have been heavily debated. \todo {look what to cite \url{https://en.wikipedia.org/wiki/Right_to_explanation}} 
  \item But even outside regulations, similar techniques can be used within a company for debugging and incident investigation. 
  Explainability of ML decisions is also important for experts, in critical domains like depression detection whose ML applications are designed from the ground up to be as interpretable as possible \cite{nguyen2022improving}. But making ML models interpretable is still an active topic of research, and considering whole pipelines instead of models makes it even more difficult.
  \item Nutritional labels~\cite{yang2018nutritional} have also been proposed as a way to provide information about AI/ML applications to end-users. The authors also draw an analogy to the food industry, where is impossible for consumers to derive certain information about food short of setting up a chemistry lab. Similar nutritional labels for AI/ML applications can provide a starting point for consumers to learn about what an ML/AI system is doing. Like in the food industry, this is sufficient as long as everything is going well, but if something goes wrong, there needs to be a way to investigate.
  \item Experiment tracking tools like mlwflow help with making experiments repeatable and log information like details of how a model was created. This logging information can be a starting point for outbreak investigations, but still requires a lot of manual work to actually debug specific predictions.
  \item Making predictions identifiable, as described above, requires provenance tracking in inference pipelines. The provenance community has started looking into applying database-style provenance to ML pipelines, as we will discuss in Section~\ref{sec:vision-tracing}. \todo{ cite: mlinspect/ArgusEyes can track data points the model is being evaluated on on the whole test set, a similar approach can be used in production scenarios. Supporting Better Insights of Data Science Pipelines with Fine-grained Provenance, maybe Deep Learning Provenance Data Integration: a Practical Approach (need to look at it first), Spade.}
\end{itemize}


\headerl{Open questions and challenges} \todo{(Stefan) Please draft the paragraph open questions and challenges here}
\begin{itemize}[leftmargin=*,noitemsep]
  \item Connection to pipeline / data / model provenance
  % Introduction
  
  \item We need systems and techniques to both make the predictions identifiable, and then to use those for investigation and to create explanations.
  % Extension on provenance tracking: in production it is more complicated
  \item Such a system has to use the prediction identifier to retrieve all data to the ML inference pipeline that was directly used as input, it needs to be able to reconstruct in detail how that data was processed, and provide the exact model version that was used to run inference on the featurized data. If necessary, the information how a model was created needs to be retrievable, along with information about the pipeline that was used for training, what input data that pipeline got run on, etc. 
  \item Doing this with a static pipeline with static input data is partly solved by existing research already, as we discuss in Section~\ref{sec:vision-tracing}. However, making predictions identifiable implies tracking provenance continuously in production. E.g., in recommendation use cases, predictions might be made based on similarity with potentially constantly changing retrieval corpus. If a LLM is used with retrieval augmentation, the retrieval corpus might be constantly changing. Some ML models might also get continuously trained on new incoming data. How to do versioning in an efficient way that still allows cheap reconstruction of past predictions? What if the whole pipeline is updated from time to time? 
  % How to generate useful explanations, also relates to next Subsection on Detecting and collecting predictions with fairness issues
  \item counterfactual explanations etc. \todo{cite "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR" \url{https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/hjlt31&id=862&men_tab=srchresults} or look for other/more papers}   \todo{More about explanability.}
  % Vision: allowing interactive investigation for identifiable predictions, maybe even for end-users, or at least make it as efficient as possible
  \item The provenance tracking has to be developed while keeping in mind how the identifiable predictions and the corresponding information is used. One could imagine scenarios where end-users is given access to part of this information, to understand how their data is processed. However, this requires to make access to data around identifiable predictions low-latency while still keeping the overhead of the production inference low. What do access patterns look like? Given a user interface for users to investigate which data, e.g., a recommendation algorithm for advertisements, used, can we enable users to immediately revoke permissions to use certain data? 
  \item The right to forget is also relevant here. If a user decides to delete some of their data, to what degree should their identifiable predictions still be reconstructable?
  \todo{Should we move this whole Section behind the provenance Section 3.2?}
  % Should the next Section really be fairness only? Or should we discuss somewhere that we can also use this to screen for general issues such as data quality? E.g., when data quality issues are detected, the identifiable predictions can be used for determining affected groups. It is not just the slice finder example below, but it seems more minor, do we still want to mention it?
  \item Next, we will discuss questions around using these identifiable predictions to detect issues such as fairness.
\end{itemize}  