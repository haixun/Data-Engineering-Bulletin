\section{Introduction}

Differential privacy (DP)~\cite{dwork2006calibrating} emerged in 2006 as a groundbreaking concept for protecting individual privacy in data analysis. DP offers a powerful privacy-preserving approach by mathematically ensuring that data releases reveal minimal information about any single person. This has led to the development of numerous DP mechanisms and systems like PINQ~\cite{mcsherry2009pinq}, FLEX~\cite{johnson2018towards}, PrivateSQL~\cite{kotsogiannis2019privatesql}, GoogleDP~\cite{amin2022plume}, and Chorus~\cite{johnson2020chorus}.
However, despite its theoretical elegance and strong privacy guarantees, DP's practical deployments lag behind its potential. While a few pioneering cases exist, such as the 2020 US Census disclosure~\cite{abowd2018census,hawes2020census}, widespread adoption remains limited. Companies like Amazon, Snowflake, Google, LinkedIn, Uber, and Apple, and startups like Tumlut Labs and Transcend are exploring DP in data management products or statistical learning scenarios~\cite{amazon_DP,snowflake_DP,google_DP_big_query,wilson2020differentially,houssiau2022google_map,rogers2020linkedin,DingKY17microsoft_DP, openDP,tumult, transcend}, but these integrations are often experimental and face challenges in production environments~\cite{tang2017privacy,GadottiHAM22apple_DP_attack,yu2022thwarting}, suggesting a divergence between theory and practice.

In theory, DP relies on a privacy budget, represented by the parameters $(\epsilon,\delta)$, which controls the overall privacy guarantee. By carefully injecting controllable noise, it can be proved that a mechanism can only reveal bounded information about any individual in a \textit{static} dataset, and thus, this mechanism satisfies the notion of DP. However, translating this theory into practical systems presents several challenges. 
First, even with a simple use case that only focuses on the \emph{central DP} setting\footnote{Assuming the existence of a \emph{trusted} curator runs a (data analytics) system with DP guarantees for a curated sensitive dataset.}, the system has to interact with private data and data analysts and has to maintain the system budget at least correctly and faithfully. 
Some systems resort to large or frequently reset budgets~\cite{rogers2020linkedin,amazon_DP}, which may jeopardize long-term privacy. Second, DP systems often assume static datasets. However, real-world data can be dynamic, integrating information from various sources~\cite{nicolas2023cohere} and undergoing regular updates~\cite{growingdb18}. The individuals in the dataset may have different privacy awareness~\cite{jorgensen2015personalized}.
These complexities require additional considerations to maintain privacy guarantees. Third, DP systems need to cater to analysts with varying levels of privacy expertise. Non-expert analysts may struggle to interpret noisy results or choose the most appropriate DP mechanism for their queries~\cite{ge2019apex}, especially for complex queries, like nested subqueries or batched workloads~\cite{nissim2007smooth,dong2021residual}. As a result, implementations of the DP system can fail to deliver an optimized privacy-utility trade-off or the expected privacy guarantees.


Recent DP systems have addressed one specific challenge mentioned above by tracking fine-grained information such as the data blocks~\cite{LecuyerSVG019sage,nicolas2023cohere} or the noise used for previous queries~\cite{mazmudar2022cache,zhang2022DProvSQL}. 
Inspired by these works, we propose a broader approach to building usable end-to-end DP systems by leveraging the data provenance framework in databases~\cite{cheney2009provenance}, in which tracing and propagating \textit{proper provenance metadata} turns out to be useful for understanding queries, integrating data, and debugging inconsistencies. Similarly, in DP systems, we envision privacy provenance --- metadata that tracks the DP mechanisms and benefits the users of the systems. 
 
In this work, we analyze different components of a DP system and explore how proper provenance metadata can offer benefits, including improved user understanding, enhanced utility optimization, and dynamic privacy management. We start with three types of privacy provenance: \textit{why-DP-provenance}, which explains the private/noisy outputs to the data analysts; \textit{how-DP-provenance}, which uses metadata for tighter privacy in running DP mechanisms; and \textit{where-DP-provenance}, which tracks privacy budget consumption over dynamic data sources to satisfy different resolutions of privacy definitions. While the concept of privacy provenance is a recent development~\cite{dprovdb}, the idea of leveraging additional data structures in DP algorithm design has been explored in prior work~\cite{johnson2020chorus,mcsherry2009pinq,gaboardi2016psi,NanayakkaraB0HR22visualizing}. We surveyed all the relevant DP works in our privacy provenance framework and provided discussion in this direction.  
Note that our characterization of privacy provenance in this work is not meant to be exhaustive. We hope this work stimulates further exploration and discussion in developing more usable and optimized systems for DP. 
This work also aims to complement existing visions on DP (including recent surveys~\cite{cummings2024advancing_DP_survey,near2021differential}). 
By introducing a system-oriented perspective through the lens of privacy provenance, we hope to pave the way for the development of more usable and optimized DP systems in the future.


\stitle{Article Roadmap.}
The remainder of this article is organized as follows.
Section 2 summarizes the preliminaries of differential privacy and provenance in databases.
In Section 3, we provide a systematic view of the complexity of the DP systems and propose a taxonomy of fine-grained privacy provenance.
We survey several systems that feature the usages of fine-grained privacy provenance in Section 4 and discuss challenges and future directions in Section 5.
We conclude this article in Section 6.


