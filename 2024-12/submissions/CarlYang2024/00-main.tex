\documentclass[]{article}

\usepackage{deauthor}


% \usepackage{amsmath}
% \usepackage{graphicx}
% \usepackage{wrapfig}
% \usepackage{xspace}
% \usepackage{xcolor}
% \usepackage{colortbl}
% \usepackage{titlesec}
% \usepackage{hyperref}
% \usepackage{cleveref}
% \usepackage{ulem}
% \usepackage[numbers,sort&compress]{natbib}
% \usepackage{tcolorbox}

% \newcommand{\ie}[0]{\textit{i.e.}}
% \newcommand{\eg}[0]{\textit{e.g.}}
% \newcommand{\etc}[0]{\textit{etc.}}
% \newcommand{\wrt}[0]{\textit{w.r.t.}}
% \newcommand{\aka}[0]{\textit{a.k.a.}}

% \newcommand{\ours}[0]{\texttt{KLC}\xspace}
% \newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
% \newcommand{\carl}[1]{\textcolor{cyan}{Carl: #1}}

% % \newcommand{\modify}[1]{\textcolor{blue}{#1}}
% \DeclareRobustCommand{\modify}[1]{{\color{blue}#1}}  
% \newcommand{\delete}[1]{\textcolor{red}{\sout{#1}}}
% % \DeclareRobustCommand{\modify}[1]{{\color{blue}#1}}  
% % change into the following to hide
% %\newcommand{\modify}[1]{#1}
% %\newcommand{\delete}[1]{}


% \input{math_commands.tex}

\begin{document}

\title{Knowledge Graph and Large Language Model Co-learning via Structure-oriented \mbox{Retrieval Augmented Generation}}
%\title{KG and LLM Co-learning via Structure-oriented RAG}

\author{Carl Yang\thanks{Corresponding author (j.carlyang@emory.edu); Department of Computer Science, Emory University, Atlanta, GA 30322, USA}, Ran Xu, Linhao Luo, Shirui Pan}

\maketitle

\begin{abstract}
Recent years have witnessed major technical breakthroughs in AI-- facilitated by tremendous data and high-performance computers, large language models (LLMs) have brought disruptive progress to information technology from accessing data to performing analysis. While demonstrating unprecedented capabilities, LLMs have been found unreliable in tasks requiring factual knowledge and rigorous reasoning. Despite recent works discussing the hallucination problem of LLMs, systematic studies on empowering LLMs with the ability to plan, reason, and ground with explicit knowledge are still lacking. 
On the other hand, real-world data are enormous and complex, coming from different sources and bearing various modalities. Data professionals have spent tremendous efforts collecting and curating countless datasets with different schemas and standards. Transforming the separate datasets into unified knowledge graphs (KGs) can facilitate their integrative analysis and utilization, but these processes would often require strong domain expertise and significant human labor. 
{In this paper, we discuss recent progress and promise in the co-learning of KGs and LLMs, through LLM-aided KG construction, KG-guided LLM enhancement, and knowledge-aware multi-agent federation, particularly emphasizing a structure-oriented retrieval augmented generation (SRAG) paradigm, towards fully utilizing the value of complex data, unleashing the power of generative models, and expediting next-generation trustworthy AI.} 
\end{abstract}


\input{submissions/CarlYang2024/01-intro}
\input{submissions/CarlYang2024/02-llm4kg}
\input{submissions/CarlYang2024/03-kg4llm}
\input{submissions/CarlYang2024/04-fedmas}
\input{submissions/CarlYang2024/05-con}


\section*{Acknowledgements}
This research was partially supported by the National Science Foundation under Award Number 2319449 and Award Number 2312502, as well as the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of the National Science Foundation, National Institutes of Health, or the U.S. government.
The authors wish to thank the editors and reviewers for their valuable efforts and suggestions.  

%% \ackrule

\bibliographystyle{IEEEtran} 
\bibliography{submissions/CarlYang2024/carlyang,submissions/CarlYang2024/linhao}

%\section*{Biographies}

%\textbf{P. W. Wachulak} received the degree${\ldots}$ \\[6pt]
%\textbf{M. C. Marconi} received the degree${\ldots}$ \\[6pt]
%\textbf{R. A. Bartels} received the degree${\ldots}$ \\[6pt]
%\textbf{C. S. Menoni} received the degree${\ldots}$ \\[6pt]
%\textbf{J. J. Rocca} received the degree${\ldots}$



\end{document}