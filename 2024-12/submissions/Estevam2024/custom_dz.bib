

@book{kahneman_thinking_2012,
	address = {London},
	title = {Thinking, fast and slow},
	isbn = {9780141033570  0141033576},
	abstract = {Psychologist Daniel Kahneman reveals the truth about our intuitions and rationality to teach us how to better our lives. He explores the fascinating flaws and marvels of human behaviour and reveals to us the common errors in people's beliefs.},
	language = {English},
	publisher = {Penguin},
	author = {Kahneman, Daniel},
	year = {2012},
}

@article{semnani2023wikichat,
  title={WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia},
  author={Semnani, Sina J and Yao, Violet Z and Zhang, Heidi C and Lam, Monica S},
  journal={arXiv preprint arXiv:2305.14292},
  year={2023}
}


@inproceedings{maynez2020faithfulness,
  title={On Faithfulness and Factuality in Abstractive Summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1906--1919},
  year={2020},
  organization={Association for Computational Linguistics}
}

@inproceedings{
sclar2024quantifying,
title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=RIu5lyNXjT}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{sahoo2024systematic,
  title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications},
  author={Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman},
  journal={arXiv preprint arXiv:2402.07927},
  year={2024}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Houlsby, Neil and Giurgiu, Albin and Jastrzebski, Stanislaw and Morrone, Benjamin and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019}
}

@article{qin2023towards,
  title={Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper},
  author={Qin, Zhen and others},
  journal={arXiv preprint arXiv:2311.13126},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@inproceedings{thorne-etal-2018-fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}

@article{aly2021feverous,
  title={Feverous: Fact extraction and verification over unstructured and structured information},
  author={Aly, Rami and Guo, Zhijiang and Schlichtkrull, Michael and Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Cocarascu, Oana and Mittal, Arpit},
  journal={arXiv preprint arXiv:2106.05707},
  year={2021}
}

@article{crawford2024bmw,
  title={BMW Agents--A Framework For Task Automation Through Multi-agent Collaboration},
  author={Crawford, Noel and Duffy, Edward B and Evazzade, Iman and Foehr, Torsten and Robbins, Gregory and Saha, Debbrata Kumar and Varma, Jiya and Ziolkowski, Marcin},
  journal={arXiv preprint arXiv:2406.20041},
  year={2024}
}

@article{peterson1977petri,
  title={Petri nets},
  author={Peterson, James L},
  journal={ACM Computing Surveys (CSUR)},
  volume={9},
  number={3},
  pages={223--252},
  year={1977},
  publisher={ACM New York, NY, USA}
}

@inproceedings{10.1145/3586183.3606763,
author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
title = {Generative Agents: Interactive Simulacra of Human Behavior},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606763},
doi = {10.1145/3586183.3606763},
abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {22},
keywords = {Human-AI interaction, agents, generative AI, large language models},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{cmdbench,
author = {Feng, Yanlin and Rahman, Sajjadur and Feng, Aaron and Chen, Vincent and Kandogan, Eser},
title = {CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems},
year = {2024},
isbn = {9798400706943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665601.3669846},
doi = {10.1145/3665601.3669846},
abstract = {Compound AI systems (CASs) that employ LLMs as agents to accomplish knowledge-intensive tasks via interactions with tools and data retrievers have garnered significant interest within database and AI communities. While these systems have the potential to supplement typical analysis workflows of data analysts in enterprise data platforms, unfortunately, CASs are subject to the same data discovery challenges that analysts have encountered over the years — silos of multimodal data sources, created across teams and departments within an organization, make it difficult to identify appropriate data sources for accomplishing the task at hand. Existing data discovery benchmarks do not model such multimodality and multiplicity of data sources. Moreover, benchmarks of CASs prioritize only evaluating end-to-end task performance. To catalyze research on evaluating the data discovery performance of multimodal data retrievers in CASs within a real-world setting, we propose CMDBench, a benchmark modeling the complexity of enterprise data platforms. We adapt existing datasets and benchmarks in open-domain — from question answering and complex reasoning tasks to natural language querying over structured data — to evaluate coarse- and fine-grained data discovery and task execution performance. Our experiments reveal the impact of data retriever design on downstream task performance — 46\% drop in task accuracy on average — across various modalities, data sources, and task difficulty. The results indicate the need to develop optimization strategies to identify appropriate LLM agents and retrievers for efficient execution of CASs over enterprise data.},
booktitle = {Proceedings of the Conference on Governance, Understanding and Integration of Data for Effective and Responsible AI},
pages = {16–25},
numpages = {10},
keywords = {Benchmark, Compound AI Systems., Data Discovery, LLMs},
location = {Santiago, AA, Chile},
series = {GUIDE-AI '24}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and individual differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{wu2024exploring,
  title={Exploring large language model for graph data understanding in online job recommendations},
  author={Wu, Likang and Qiu, Zhaopeng and Zheng, Zhi and Zhu, Hengshu and Chen, Enhong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={8},
  pages={9178--9186},
  year={2024}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}




@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@article{yuan2024rigorllm,
  title={Rigorllm: Resilient guardrails for large language models against undesired content},
  author={Yuan, Zhuowen and Xiong, Zidi and Zeng, Yi and Yu, Ning and Jia, Ruoxi and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2403.13031},
  year={2024}
}



@article{zhu2024knowagent,
  title={Knowagent: Knowledge-augmented planning for llm-based agents},
  author={Zhu, Yuqi and Qiao, Shuofei and Ou, Yixin and Deng, Shumin and Zhang, Ningyu and Lyu, Shiwei and Shen, Yue and Liang, Lei and Gu, Jinjie and Chen, Huajun},
  journal={arXiv preprint arXiv:2403.03101},
  year={2024}
}


@misc{compound-ai-blog,
  title={The Shift from Models to Compound AI Systems},
  author={Matei Zaharia and Omar Khattab and Lingjiao Chen and Jared Quincy Davis
          and Heather Miller and Chris Potts and James Zou and Michael Carbin
          and Jonathan Frankle and Naveen Rao and Ali Ghodsi},
  howpublished={\url{https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/}},
  year={2024}
}

@inproceedings{chen2023symphony,
  title={Symphony: Towards Natural Language Query Answering over Multi-modal Data Lakes.},
  author={Chen, Zui and Gu, Zihui and Cao, Lei and Fan, Ju and Madden, Samuel and Tang, Nan},
  booktitle={CIDR},
  year={2023}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}


@article{yoran2023making,
  title={Making retrieval-augmented language models robust to irrelevant context},
  author={Yoran, Ori and Wolfson, Tomer and Ram, Ori and Berant, Jonathan},
  journal={arXiv preprint arXiv:2310.01558},
  year={2023}
}


@article{ram2023context,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={1316--1331},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@inproceedings{topsakal2023creating,
  title={Creating large language model applications utilizing langchain: A primer on developing llm apps fast},
  author={Topsakal, Oguzhan and Akinci, Tahir Cetin},
  booktitle={International Conference on Applied Engineering and Natural Sciences},
  volume={1},
  number={1},
  pages={1050--1056},
  year={2023}
}


@article{wang2023gemini,
  title={Gemini in reasoning: Unveiling commonsense in multimodal large language models},
  author={Wang, Yuqing and Zhao, Yun},
  journal={arXiv preprint arXiv:2312.17661},
  year={2023}
}

@article{li2023self,
  title={Self-checker: Plug-and-play modules for fact-checking with large language models},
  author={Li, Miaoran and Peng, Baolin and Galley, Michel and Gao, Jianfeng and Zhang, Zhu},
  journal={arXiv preprint arXiv:2305.14623},
  year={2023}
}

@inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}


@article{li2023survey,
  title={A survey on fairness in large language models},
  author={Li, Yingji and Du, Mengnan and Song, Rui and Wang, Xin and Wang, Ying},
  journal={arXiv preprint arXiv:2308.10149},
  year={2023}
}



@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}



@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}







@article{valmeekam2024planbench,
  title={Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change},
  author={Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{wang2024exploring,
  title={Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning},
  author={Wang, Yiqi and Chen, Wentao and Han, Xiaotian and Lin, Xudong and Zhao, Haiteng and Liu, Yongfei and Zhai, Bohan and Yuan, Jianbo and You, Quanzeng and Yang, Hongxia},
  journal={arXiv preprint arXiv:2401.06805},
  year={2024}
}



@article{zhao2024large,
  title={Large language models as commonsense knowledge for large-scale task planning},
  author={Zhao, Zirui and Lee, Wee Sun and Hsu, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{rajani2019explain,
  title={Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
  author={Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4932--4942},
  year={2019}
}


@inproceedings{wu2024exploring,
  title={Exploring large language model for graph data understanding in online job recommendations},
  author={Wu, Likang and Qiu, Zhaopeng and Zheng, Zhi and Zhu, Hengshu and Chen, Enhong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={8},
  pages={9178--9186},
  year={2024}
}

@inproceedings{fang2023recruitpro,
  title={Recruitpro: A pretrained language model with skill-aware prompt learning for intelligent recruitment},
  author={Fang, Chuyu and Qin, Chuan and Zhang, Qi and Yao, Kaichun and Zhang, Jingshuai and Zhu, Hengshu and Zhuang, Fuzhen and Xiong, Hui},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3991--4002},
  year={2023}
}


@inproceedings{fu2024drive,
  title={Drive like a human: Rethinking autonomous driving with large language models},
  author={Fu, Daocheng and Li, Xin and Wen, Licheng and Dou, Min and Cai, Pinlong and Shi, Botian and Qiao, Yu},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={910--919},
  year={2024}
}


@article{xie2024travelplanner,
  title={Travelplanner: A benchmark for real-world planning with language agents},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Zhu, Tinghui and Lou, Renze and Tian, Yuandong and Xiao, Yanghua and Su, Yu},
  journal={arXiv preprint arXiv:2402.01622},
  year={2024}
}


@article{abd2023large,
  title={Large language models in medical education: opportunities, challenges, and future directions},
  author={Abd-Alrazaq, Alaa and AlSaad, Rawan and Alhuwail, Dari and Ahmed, Arfan and Healy, Padraig Mark and Latifi, Syed and Aziz, Sarah and Damseh, Rafat and Alrazak, Sadam Alabed and Sheikh, Javaid and others},
  journal={JMIR Medical Education},
  volume={9},
  number={1},
  pages={e48291},
  year={2023},
  publisher={JMIR Publications Inc., Toronto, Canada}
}


@article{jeon2023large,
  title={Large language models in education: A focus on the complementary relationship between human teachers and ChatGPT},
  author={Jeon, Jaeho and Lee, Seongyong},
  journal={Education and Information Technologies},
  volume={28},
  number={12},
  pages={15873--15892},
  year={2023},
  publisher={Springer}
}



@article{styles2024workbench,
  title={WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting},
  author={Styles, Olly and Miller, Sam and Cerda-Mardini, Patricio and Guha, Tanaya and Sanchez, Victor and Vidgen, Bertie},
  journal={arXiv preprint arXiv:2405.00823},
  year={2024}
}

@article{liu2024agentlite,
  title={AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System},
  author={Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Yang, Liangwei and Liu, Zuxin and Tan, Juntao and Choubey, Prafulla K and Lan, Tian and Wu, Jason and Wang, Huan and others},
  journal={arXiv preprint arXiv:2402.15538},
  year={2024}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th annual acm symposium on user interface software and technology},
  pages={1--22},
  year={2023}
}

@article{saeed2023querying,
  title={Querying large language models with SQL},
  author={Saeed, Mohammed and De Cao, Nicola and Papotti, Paolo},
  journal={arXiv preprint arXiv:2304.00472},
  year={2023}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{zhang2023ecoassistant,
  title={Ecoassistant: Using llm assistant more affordably and accurately},
  author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},
  journal={arXiv preprint arXiv:2310.03046},
  year={2023}
}

@article{xie2023openagents,
  title={Openagents: An open platform for language agents in the wild},
  author={Xie, Tianbao and Zhou, Fan and Cheng, Zhoujun and Shi, Peng and Weng, Luoxuan and Liu, Yitao and Hua, Toh Jing and Zhao, Junning and Liu, Qian and Liu, Che and others},
  journal={arXiv preprint arXiv:2310.10634},
  year={2023}
}

@article{wu2023autogen,
  title={Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}

@article{chen2023autoagents,
  title={Autoagents: A framework for automatic agent generation},
  author={Chen, Guangyao and Dong, Siwei and Shu, Yu and Zhang, Ge and Sesay, Jaward and Karlsson, B{\"o}rje F and Fu, Jie and Shi, Yemin},
  journal={arXiv preprint arXiv:2309.17288},
  year={2023}
}

@article{zhou2023agents,
  title={Agents: An open-source framework for autonomous language agents},
  author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Li, Long and Wu, Jialong and Wang, Tiannan and Qiu, Shi and Zhang, Jintian and Chen, Jing and Wu, Ruipu and Wang, Shuai and others},
  journal={arXiv preprint arXiv:2309.07870},
  year={2023}
}

@article{chen2023agentverse,
  title={Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents},
  author={Chen, Weize and Su, Yusheng and Zuo, Jingwei and Yang, Cheng and Yuan, Chenfei and Qian, Chen and Chan, Chi-Min and Qin, Yujia and Lu, Yaxi and Xie, Ruobing and others},
  journal={arXiv preprint arXiv:2308.10848},
  year={2023}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{lu2024chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{beurer2023prompting,
  title={Prompting is programming: A query language for large language models},
  author={Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
  journal={Proceedings of the ACM on Programming Languages},
  volume={7},
  number={PLDI},
  pages={1946--1969},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{cai2023large,
  title={Large language models as tool makers},
  author={Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny},
  journal={arXiv preprint arXiv:2305.17126},
  year={2023}
}

@article{hu2023chatdb,
  title={Chatdb: Augmenting llms with databases as their symbolic memory},
  author={Hu, Chenxu and Fu, Jie and Du, Chenzhuang and Luo, Simian and Zhao, Junbo and Zhao, Hang},
  journal={arXiv preprint arXiv:2306.03901},
  year={2023}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{paranjape2023art,
  title={Art: Automatic multi-step reasoning and tool-use for large language models},
  author={Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
  journal={arXiv preprint arXiv:2303.09014},
  year={2023}
}

@article{hao2024toolkengpt,
  title={Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings},
  author={Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{liang2024taskmatrix,
  title={Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis},
  author={Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and others},
  journal={Intelligent Computing},
  volume={3},
  pages={0063},
  year={2024},
  publisher={AAAS}
}

@article{wang2024tools,
  title={What are tools anyway? a survey from the language model perspective},
  author={Wang, Zhiruo and Cheng, Zhoujun and Zhu, Hao and Fried, Daniel and Neubig, Graham},
  journal={arXiv preprint arXiv:2403.15452},
  year={2024}
}

@article{wang2024chain,
  title={Chain-of-table: Evolving tables in the reasoning chain for table understanding},
  author={Wang, Zilong and Zhang, Hao and Li, Chun-Liang and Eisenschlos, Julian Martin and Perot, Vincent and Wang, Zifeng and Miculicich, Lesly and Fujii, Yasuhisa and Shang, Jingbo and Lee, Chen-Yu and others},
  journal={arXiv preprint arXiv:2401.04398},
  year={2024}
}

@article{liu2024declarative,
  title={A Declarative System for Optimizing AI Workloads},
  author={Liu, Chunwei and Russo, Matthew and Cafarella, Michael and Cao, Lei and Chen, Peter Baille and Chen, Zui and Franklin, Michael and Kraska, Tim and Madden, Samuel and Vitagliano, Gerardo},
  journal={arXiv preprint arXiv:2405.14696},
  year={2024}
}

@article{cao2020kqa,
  title={KQA pro: A dataset with explicit compositional programs for complex question answering over knowledge base},
  author={Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang},
  journal={arXiv:2007.03875},
  year={2020}
}

@article{eltabakh2023cross,
  title={Cross Modal Data Discovery over Structured and Unstructured Data Lakes},
  author={Eltabakh, Mohamed Y and Kunjir, Mayuresh and Elmagarmid, Ahmed K and Ahmad, Mohammad Shahmeer},
  journal={Proceedings of the VLDB Endowment},
  volume={16},
  number={11},
  pages={3377--3390},
  year={2023},
  publisher={VLDB Endowment}
}

@article{khattab2023dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and and others},
  journal={arXiv preprint arXiv:2310.03714},
  year={2023}
}

@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@software{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}

@inproceedings{miller2013graph,
  title={Graph database applications and concepts with Neo4j},
  author={Miller, Justin J},
  booktitle={Proceedings of the southern association for information systems conference, Atlanta, GA, USA},
  volume={2324},
  number={36},
  pages={141--147},
  year={2013}
}

@book{bradshaw2019mongodb,
  title={MongoDB: the definitive guide: powerful and scalable data storage},
  author={Bradshaw, Shannon and Brazil, Eoin and Chodorow, Kristina},
  year={2019},
  publisher={O'Reilly Media}
}

@article{stonebraker1986design,
  title={The design of Postgres},
  author={Stonebraker, Michael and Rowe, Lawrence A},
  journal={ACM Sigmod Record},
  volume={15},
  number={2},
  pages={340--355},
  year={1986},
  publisher={ACM New York, NY, USA}
}

@article{izacard2022atlas,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and others},
  journal={arXiv:2208.03299},
  year={2022}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and others},
  journal={NeurIPS},
  year={2024}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and and others},
  journal={Science},
  year={2022},
  publisher={American Association for the Advancement of Science}
}


@article{yang2024harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and others},
  journal={TKDD},
  year={2024},
  publisher={ACM New York, NY}
}


@article{xi2023rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Zhiheng and others},
  journal={arXiv:2309.07864},
  year={2023}
}


@article{zhang2023multimodal,
  title={Multimodal chain-of-thought reasoning in language models},
  author={Zhang, Zhuosheng and others},
  journal={arXiv:2302.00923},
  year={2023}
}




@article{li2023enhancing,
  author = {Xue Li and Weibin Zeng and Zhibin Wang and Diwen Zhu and Jingbo Xu and Wenyuan Yu and Jingren Zhou},
  title = {Enhancing Data Lakes with GraphAr: Efficient Graph Data Management with a Specialized Storage Scheme},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2312.09577},
  doi = {10.48550/ARXIV.2312.09577},
  eprinttype = {arXiv},
  eprint = {2312.09577},
  biburl = {https://dblp.org/rec/journals/corr/abs-2312-09577.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{petroni2021kilt,
    title = "{KILT}: a Benchmark for Knowledge Intensive Language Tasks",
    author = {Petroni, Fabio  and
      others},
    booktitle = "NAACL",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "ACL",
    pages = "2523--2544",
    abstract = "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at \url{https://github.com/facebookresearch/KILT}.",
}




@inproceedings{feng2020scalable,
  title={Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering},
  author={Feng, Yanlin and Chen, Xinyue and Lin, Bill Yuchen and Wang, Peifeng and Yan, Jun and Ren, Xiang},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1295--1309},
  year={2020}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{yang2018hotpotqa,
  title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2369--2380},
  year={2018}
}

@inproceedings{cao2022kqa,
  title={KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base},
  author={Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6101--6119},
  year={2022}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@inproceedings{yasunaga2021qa,
  title={QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering},
  author={Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={535--546},
  year={2021}
}

@inproceedings{
li2023camel,
title={{CAMEL}: Communicative Agents for ''Mind'' Exploration of Large Language Model Society},
author={Li, Guohao  and others},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=3IyL2XWDkG}
}

@article{xi2023rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal={arXiv preprint arXiv:2309.07864},
  year={2023}
}


@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}


@article{wu2023autogen,
  title={Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and others},
  journal={arXiv:2308.08155},
  year={2023}
}


@article{zhao2023verify,
  title={Verify-and-edit: A knowledge-enhanced chain-of-thought framework},
  author={Zhao, Ruochen and Li, Xingxuan and Joty, Shafiq and Qin, Chengwei and Bing, Lidong},
  journal={arXiv preprint arXiv:2305.03268},
  year={2023}
}

@article{wu2309hayate,
  title={Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2023b. Less is more for long document summary evaluation by llms},
  author={Wu, Yunshu},
  journal={arXiv preprint arXiv:2309.07382}
}

@inproceedings{maekawa2024retrieval,
  title={Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models},
  author={Maekawa, Seiji and Iso, Hayate and Gurajada, Sairam and Bhutani, Nikita},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={5506--5521},
  year={2024}
}

@article{asai2023self,
  title={Self-rag: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2310.11511},
  year={2023}
}

@inproceedings{maekawa2024retrieval,
  title={Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models}, 
  author={Maekawa, Seiji  and
          others},
  booktitle = {NAACL},
  month = {June},
  year = {2024}
}

@inproceedings{wu-etal-2024-less,
    title = "Less is More for Long Document Summary Evaluation by {LLM}s",
    author = "Wu, Yunshu  and
      others",
    booktitle = "EACL",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "ACL",
    pages = "330--343",
    abstract = "Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the \textit{Lost-in-the-Middle} problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.",
}

@inproceedings{zhang24coling,
title = {{XATU}: {A} {F}ine-grained {I}nstruction-based {B}enchmark for {E}xplainable {T}ext {U}pdates},
author = {Zhang, Haopeng  and
          others},
booktitle = {LREC-COLING},
month = {May},
year = {2024}
}

@article{kim2023llm,
  title={An LLM compiler for parallel function calling},
  author={Kim, Sehoon and others},
  journal={arXiv:2312.04511},
  year={2023}
}

@inproceedings{vertesi2011value,
  title={The value of data: considering the context of production in data economies},
  author={Vertesi, Janet and Dourish, Paul},
  booktitle={Proceedings of the ACM 2011 conference on Computer supported cooperative work},
  pages={533--542},
  year={2011}
}

@article{patel2019bridging,
  title={Bridging data silos using big data integration},
  author={Patel, Jayesh},
  journal={International Journal of Database Management Systems},
  volume={11},
  number={3},
  pages={01--06},
  year={2019}
}

@article{comfort2005risk,
  title={Risk, security, and disaster management},
  author={Comfort, Louise K},
  journal={Annu. Rev. Polit. Sci.},
  volume={8},
  pages={335--356},
  year={2005},
  publisher={Annual Reviews}
}

@inproceedings{10.1145/3211954.3211955,
author = {Seabolt, Ed and Kandogan, Eser and Roth, Mary},
title = {Contextual Intelligence for Unified Data Governance},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211955},
doi = {10.1145/3211954.3211955},
abstract = {Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in today's data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for 'contextual intelligence', where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {2},
numpages = {9},
keywords = {Analytics, Context, Data Governance, Graph},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@article{luo2024bge,
  title={BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models},
  author={Luo, Kun and Liu, Zheng and Xiao, Shitao and Liu, Kang},
  journal={arXiv:2402.11573},
  year={2024}
}

@inproceedings{fernandez2016towards,
  title={Towards large-scale data discovery: position paper},
  author={Fernandez, Raul Castro and Abedjan, Ziawasch and Madden, Samuel and Stonebraker, Michael},
  booktitle={Proceedings of the Third International Workshop on Exploratory Search in Databases and the Web},
  pages={3--5},
  year={2016}
}

@inproceedings{fernandez2018aurum,
  title={Aurum: A data discovery system},
  author={Fernandez, Raul Castro and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
  booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)},
  pages={1001--1012},
  year={2018},
  organization={IEEE}
}

@inproceedings{sarthi2023raptor,
  title={RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval},
  author={Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016}
}

@inproceedings{nambiar2006making,
  title={The Making of TPC-DS.},
  author={Nambiar, Raghunath Othayoth and Poess, Meikel},
  booktitle={VLDB},
  volume={6},
  pages={1049--1058},
  year={2006}
}

@inproceedings{elsahar2018t,
  title={T-rex: A large scale alignment of natural language with knowledge base triples},
  author={Elsahar, Hady and Vougiouklis, Pavlos and Remaci, Arslen and Gravier, Christophe and Hare, Jonathon and Laforest, Frederique and Simperl, Elena},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@inproceedings{bhagavatula2013methods,
  title={Methods for exploring and mining tables on wikipedia},
  author={Bhagavatula, Chandra Sekhar and Noraset, Thanapon and Downey, Doug},
  booktitle={Proceedings of the ACM SIGKDD workshop on interactive data exploration and analytics},
  pages={18--26},
  year={2013}
}

@misc{compound-ai-blog,
  title={The Shift from Models to Compound AI Systems},
  author={Zaharia, Matei  and others},
  howpublished={\url{https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/}},
  year={2024}
}

 
@misc{Basketball,
  title={Basketball Statistics and History},
  author={Sports Reference LLC.},
  howpublished={\url{https://www.basketball-reference.com/}},
  year={2022}
}

@article{wang2023dbcopilot,
  title={DBCopilot: Scaling Natural Language Querying to Massive Databases},
  author={Wang, Tianshu and Lin, Hongyu and Han, Xianpei and Sun, Le and Chen, Xiaoyang and Wang, Hao and Zeng, Zhenyu},
  journal={arXiv:2312.03463},
  year={2023}
}

@inproceedings{zhao2021knowledge,
  title={Knowledge graphs enhanced neural machine translation},
  author={Zhao, Yang and Zhang, Jiajun and Zhou, Yu and Zong, Chengqing},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={4039--4045},
  year={2021}
}

@book{tong2015elasticsearch,
  title={Elasticsearch: The Definitive Guide},
  author={Tong, Zachary},
  year={2015},
  publisher={O'Reilly}
}

@article{Fan2022SemanticsawareDD,
author = {Fan, Grace and Wang, Jin and Li, Yuliang and Zhang, Dan and Miller, Ren\'{e}e J.},
title = {Semantics-Aware Dataset Discovery from Data Lakes with Contextualized Column-Based Representation Learning},
year = {2023},
issue_date = {March 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3587136.3587146},
doi = {10.14778/3587136.3587146},
abstract = {Dataset discovery from data lakes is essential in many real application scenarios. In this paper, we propose Starmie, an end-to-end framework for dataset discovery from data lakes (with table union search as the main use case). Our proposed framework features a contrastive learning method to train column encoders from pre-trained language models in a fully unsupervised manner. The column encoder of Starmie captures the rich contextual semantic information within tables by leveraging a contrastive multi-column pre-training strategy. We utilize the cosine similarity between column embedding vectors as the column unionability score and propose a filter-and-verification framework that allows exploring a variety of design choices to compute the unionability score between two tables accordingly. Empirical results on real table benchmarks show that Starmie outperforms the best-known solutions in the effectiveness of table union search by 6.8 in MAP and recall. Moreover, Starmie is the first to employ the HNSW (Hierarchical Navigable Small World) index to accelerate query processing of table union search which provides a 3,000X performance gain over the linear scan baseline and a 400X performance gain over an LSH index (the state-of-the-art solution for data lake indexing).},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {1726–1739},
numpages = {14}
}

@article{tus2018Nargesian,
author = {Nargesian, Fatemeh and Zhu, Erkang and Pu, Ken Q. and Miller, Ren\'{e}e J.},
title = {Table union search on open data},
year = {2018},
issue_date = {March 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3192965.3192973},
doi = {10.14778/3192965.3192973},
abstract = {We define the table union search problem and present a probabilistic solution for finding tables that are unionable with a query table within massive repositories. Two tables are unionable if they share attributes from the same domain. Our solution formalizes three statistical models that describe how unionable attributes are generated from set domains, semantic domains with values from an ontology, and natural language domains. We propose a data-driven approach that automatically determines the best model to use for each pair of attributes. Through a distribution-aware algorithm, we are able to find the optimal number of attributes in two tables that can be unioned. To evaluate accuracy, we created and open-sourced a benchmark of Open Data tables. We show that our table union search outperforms in speed and accuracy existing algorithms for finding related tables and scales to provide efficient search over Open Data repositories containing more than one million attributes.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {813–825},
numpages = {13}
}

@article{khatiwada2023santos,
  title={Santos: Relationship-based semantic table union search},
  author={Khatiwada, Aamod and Fan, Grace and Shraga, Roee and Chen, Zixuan and Gatterbauer, Wolfgang and Miller, Ren{\'e}e J and Riedewald, Mirek},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={1},
  pages={1--25},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{tang2023verifai,
  title={VerifAI: Verified Generative AI},
  author={Tang, Nan and Yang, Chenyu and Fan, Ju and Cao, Lei},
  journal={arXiv:2307.02796},
  year={2023}
}

@article{srinivas2023lakebench,
  title={LakeBench: Benchmarks for Data Discovery over Data Lakes},
  author={Srinivas, Kavitha and Dolby, Julian and Abdelaziz, Ibrahim and Hassanzadeh, Oktie and Kokel, Harsha and Khatiwada, Aamod and Pedapati, Tejaswini and Chaudhury, Subhajit and Samulowitz, Horst},
  journal={arXiv:2307.04217},
  year={2023}
}

@article{saad2023ares,
  title={Ares: An automated evaluation framework for retrieval-augmented generation systems},
  author={Saad-Falcon, Jon and Khattab, Omar and Potts, Christopher and Zaharia, Matei},
  journal={arXiv:2311.09476},
  year={2023}
}

@inproceedings{Petroni2020KILTAB,
  title={KILT: a Benchmark for Knowledge Intensive Language Tasks},
  author={Petroni, Fabio and others},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221507798}
}

@incollection{gomez2017enterprise,
  title={Enterprise knowledge graph: An introduction},
  author={Gomez-Perez, Jose Manuel and Pan, Jeff Z and Vetere, Guido and Wu, Honghan},
  booktitle={Exploiting linked data and knowledge graphs in large organisations},
  pages={1--14},
  year={2017},
  publisher={Springer}
}

@article{es2023ragas,
  title={Ragas: Automated evaluation of retrieval augmented generation},
  author={Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  journal={arXiv:2309.15217},
  year={2023}
}

@article{vrandevcic2014wikidata,
  title={Wikidata: a free collaborative knowledgebase},
  author={Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal={Communications of the ACM},
  volume={57},
  number={10},
  pages={78--85},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@article{zhongSeq2SQL2017,
  author    = {Victor Zhong and
               Caiming Xiong and
               Richard Socher},
  title     = {Seq2SQL: Generating Structured Queries from Natural Language using
               Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1709.00103},
  year      = {2017}
}

@article{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{kambhampati2024llms,
  title={LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks},
  author={Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Stechly, Kaya and Verma, Mudit and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  journal={arXiv preprint arXiv:2402.01817},
  year={2024}
}
@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}
@article{valmeekam2023planning,
  title={On the planning abilities of large language models-a critical investigation},
  author={Valmeekam, Karthik and Marquez, Matthew and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={75993--76005},
  year={2023}
}

@article{kim2023llmcompiler,
  title={An LLM Compiler for Parallel Function Calling},
  author={Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv},
  year={2023}
}