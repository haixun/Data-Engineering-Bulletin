
@article{DBLP:journals/corr/abs-2402-05391,
  title={Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey},
  author={Chen, Zhuo and Zhang, Yichi and Fang, Yin and Geng, Yuxia and Guo, Lingbing and Chen, Xiang and Li, Qian and Zhang, Wen and Chen, Jiaoyan and Zhu, Yushan and others},
  journal={arXiv preprint arXiv:2402.05391},
  year={2024}
}


@misc{liu2024chipnemo,

      title={ChipNeMo: Domain-Adapted LLMs for Chip Design},
      author={Mingjie Liu et al.},
      year={2024},
      eprint={2311.00176},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bolton2024biomedlm,
      title={BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text}, 
      author={Elliot Bolton and Abhinav Venigalla and Michihiro Yasunaga and David Hall and Betty Xiong and Tony Lee and Roxana Daneshjou and Jonathan Frankle and Percy Liang and Michael Carbin and Christopher D. Manning},
      year={2024},
      eprint={2403.18421},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{9499743,
  author={Tan, Zhanhong and Cai, Hongyu and Dong, Runpei and Ma, Kaisheng},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  title={NN-Baton: DNN Workload Orchestration and Chiplet Granularity Exploration for Multichip Accelerators},
  year={2021},
  volume={},
  number={},
  pages={1013-1026},
}


@misc{diao2023mixtureofdomainadapters,
      title={Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories}, 
      author={Shizhe Diao and Tianyang Xu and Ruijia Xu and Jiawei Wang and Tong Zhang},
      year={2023},
      eprint={2306.05406},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ge2021largeea,
  title={LargeEA: Aligning entities for large-scale knowledge graphs},
  author={Ge, Congcong and Liu, Xiaoze and Chen, Lu and Zheng, Baihua and Gao, Yunjun},
  journal={Proceedings of the VLDB Endowment},
  volume={15},
  number={2},
  pages={237--245},
  year={2021}
}

@inproceedings{liu2023unsupervised,
  title={Unsupervised entity alignment for temporal knowledge graphs},
  author={Liu, Xiaoze and Wu, Junyang and Li, Tianyi and Chen, Lu and Gao, Yunjun},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={2528--2538},
  year={2023}
}

@inproceedings{ge2021make,
  title={Make it easy: An effective end-to-end entity alignment framework},
  author={Ge, Congcong and Liu, Xiaoze and Chen, Lu and Zheng, Baihua and Gao, Yunjun},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={777--786},
  year={2021}
}

@article{guo2024distributed,
  title={Distributed representations of entities in open-world knowledge graphs},
  author={Guo, Lingbing and Chen, Zhuo and Chen, Jiaoyan and Zhang, Yichi and Sun, Zequn and Bo, Zhongpu and Fang, Yin and Liu, Xiaoze and Chen, Huajun and Zhang, Wen},
  journal={Knowledge-Based Systems},
  pages={111582},
  year={2024},
  publisher={Elsevier}
}

@misc{chen2024exploring,
      title={Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs}, 
      author={Zhikai Chen and Haitao Mao and Hang Li and Wei Jin and Hongzhi Wen and Xiaochi Wei and Shuaiqiang Wang and Dawei Yin and Wenqi Fan and Hui Liu and Jiliang Tang},
      year={2024},
      eprint={2307.03393},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2024blendfilter,
      title={BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering}, 
      author={Haoyu Wang and Tuo Zhao and Jing Gao},
      year={2024},
      eprint={2402.11129},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{gao2022clusterea,
  title={Clusterea: Scalable entity alignment with stochastic training and normalized mini-batch similarities},
  author={Gao, Yunjun and Liu, Xiaoze and Wu, Junyang and Li, Tianyi and Wang, Pengfei and Chen, Lu},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={421--431},
  year={2022}
}
@inproceedings{DBLP:conf/nips/SchaefferMK23,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{DBLP:journals/corr/abs-2311-06503,
  title={Knowledgeable preference alignment for llms in domain-specific question answering},
  author={Zhang, Yichi and Chen, Zhuo and Fang, Yin and Cheng, Lei and Lu, Yanxi and Li, Fangming and Zhang, Wen and Chen, Huajun},
  journal={arXiv preprint arXiv:2311.06503},
  year={2023}
}

@inproceedings{DBLP:conf/aaai/FangZYZD0Q0FC22,
  title={Molecular contrastive learning with chemical element knowledge graph},
  author={Fang, Yin and Zhang, Qiang and Yang, Haihong and Zhuang, Xiang and Deng, Shumin and Zhang, Wen and Qin, Ming and Chen, Zhuo and Fan, Xiaohui and Chen, Huajun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={36},
  number={4},
  pages={3968--3976},
  year={2022}
}

@inproceedings{DBLP:conf/semweb/0007CGPYC21,
  title={Zero-shot visual question answering using knowledge graph},
  author={Chen, Zhuo and Chen, Jiaoyan and Geng, Yuxia and Pan, Jeff Z and Yuan, Zonggang and Chen, Huajun},
  booktitle={The Semantic Web--ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event, October 24--28, 2021, Proceedings 20},
  pages={146--162},
  year={2021},
  organization={Springer}
}

@inproceedings{fang2023domain,
title={Domain-Agnostic Molecular Generation with Self-feedback},
author={Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@article{weller2023according,
  title={" According to..." Prompting Language Models Improves Quoting from Pre-Training Data},
  author={Weller, Orion and Marone, Marc and Weir, Nathaniel and Lawrie, Dawn and Khashabi, Daniel and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2305.13252},
  year={2023}
}

@inproceedings{fang2023mol,
  title={Mol-instructions: A large-scale biomolecular instruction dataset for large language models},
  author={Fang, Yin and Liang, Xiaozhuan and Zhang, Ningyu and Liu, Kangwei and Huang, Rui and Chen, Zhuo and Fan, Xiaohui and Chen, Huajun},
  journal={arXiv preprint arXiv:2306.08018},
  year={2023}
}

@inproceedings{DBLP:conf/icde/00070HCGYBZYSWY23,
  title={Tele-knowledge pre-training for fault analysis},
  author={Chen, Zhuo and Zhang, Wen and Huang, Yufeng and Chen, Mingyang and Geng, Yuxia and Yu, Hongtao and Bi, Zhen and Zhang, Yichi and Yao, Zhen and Song, Wenting and others},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)},
  pages={3453--3466},
  year={2023},
  organization={IEEE}
}

@inproceedings{DBLP:conf/jist/0007HCGFP0Z22,
  title={Lako: Knowledge-driven visual question answering via late knowledge-to-text injection},
  author={Chen, Zhuo and Huang, Yufeng and Chen, Jiaoyan and Geng, Yuxia and Fang, Yin and Pan, Jeff Z and Zhang, Ningyu and Zhang, Wen},
  booktitle={Proceedings of the 11th International Joint Conference on Knowledge Graphs},
  pages={20--29},
  year={2022}
}

@article{DBLP:journals/corr/abs-2310-06671,
  title={Making large language models perform better in knowledge graph completion},
  author={Zhang, Yichi and Chen, Zhuo and Zhang, Wen and Chen, Huajun},
  journal={arXiv preprint arXiv:2310.06671},
  year={2023}
}

@article{DBLP:journals/natmi/FangZZCZSFC23,
  title={Knowledge graph-enhanced molecular contrastive learning with functional prompt},
  author={Fang, Yin and Zhang, Qiang and Zhang, Ningyu and Chen, Zhuo and Zhuang, Xiang and Shao, Xin and Fan, Xiaohui and Chen, Huajun},
  journal={Nature Machine Intelligence},
  volume={5},
  number={5},
  pages={542--553},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{wang2023survey,
  title={Survey on factuality in large language models: Knowledge, retrieval and domain-specificity},
  author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and others},
  journal={arXiv preprint arXiv:2310.07521},
  year={2023}
}

@article{chen2017multilingual,
  title={Multilingual knowledge graph embeddings for cross-lingual knowledge alignment},
  author={Chen, Muhao and Tian, Yingtao and Yang, Mohan and Zaniolo, Carlo},
  journal={arXiv preprint arXiv:1611.03954},
  year={2016}
}

@article{dong2015data,
  title={Multilingual knowledge graph embeddings for cross-lingual knowledge alignment},
  author={Chen, Muhao and Tian, Yingtao and Yang, Mohan and Zaniolo, Carlo},
  journal={arXiv preprint arXiv:1611.03954},
  year={2016}
}

@inproceedings{chen2023felm,
title={{FELM}: Benchmarking Factuality Evaluation of Large Language Models},
author={Shiqi Chen and Yiran Zhao and Jinghan Zhang and I-Chun Chern and Siyang Gao and Pengfei Liu and Junxian He},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023}
}

@inproceedings{wang2023evaluating,
title={Evaluating Open-{QA} Evaluation},
author={Cunxiang Wang and Sirui Cheng and Qipeng Guo and Yuanhao Yue and Bowen Ding and Zhikun Xu and Yidong Wang and Xiangkun Hu and Zheng Zhang and Yue Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023}
}

@article{EvaluationSurvey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2023},
  publisher={ACM New York, NY}
}

@article{tian2023finetuning,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
}

@inproceedings{feng-etal-2023-factkb,
  title={FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge},
  author={Feng, Shangbin and Balachandran, Vidhisha and Bai, Yuyang and Tsvetkov, Yulia},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={933--952},
  year={2023}
}

@inproceedings{tam-etal-2023-evaluating,
  title={Evaluating the factual consistency of large language models through news summarization},
  author={Tam, Derek and Mascarenhas, Anisha and Zhang, Shiyue and Kwan, Sarah and Bansal, Mohit and Raffel, Colin},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={5220--5255},
  year={2023}
}

@inproceedings{min-etal-2023-factscore,
  title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
  author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={12076--12100},
  year={2023}
}

@inproceedings{manakul-etal-2023-selfcheckgpt,
  title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9004--9017},
  year={2023}
}

@article{pezeshkpour2023measuring,
  title={Measuring and modifying factual knowledge in large language models},
  author={Pezeshkpour, Pouya},
  journal={arXiv preprint arXiv:2306.06264},
  year={2023}
}

@inproceedings{de-cao-etal-2021-editing,
  title={Editing Factual Knowledge in Language Models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={6491--6506},
  year={2021}
}

@article{varshney2023stitch,
  title={A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation},
  author={Varshney, Neeraj and Yao, Wenlin and Zhang, Hongming and Chen, Jianshu and Yu, Dong},
  journal={arXiv preprint arXiv:2307.03987},
  year={2023}
}

@article{chern2023factool,
  title={FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
  author={Chern, I and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei and others},
  journal={arXiv preprint arXiv:2307.13528},
  year={2023}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@inproceedings{auer2007dbpedia,
  title={Dbpedia: A nucleus for a web of open data},
  author={Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle={International Semantic Web Conference},
  pages={722--735},
  year={2007},
  organization={Springer}
}

@article{NaturalQuestions,
  title={Natural Questions: A Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press-Journals}
}

@inproceedings{bollacker2008freebase,
  title={Freebase: a collaboratively created graph database for structuring human knowledge},
  author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle={Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  pages={1247--1250},
  year={2008}
}

@article{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}


@inproceedings{ScienceQA,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022}
}



@inproceedings{chen2020recall,
  title={Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting},
  author={Chen, Sanyuan and Hou, Yutai and Cui, Yiming and Che, Wanxiang and Liu, Ting and Yu, Xiangzhan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7870--7881},
  year={2020}
}

@article{zhai2023investigating,
  title={Investigating the Catastrophic Forgetting in Multimodal Large Language Models},
  author={Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu, Qing and Lee, Yong Jae and Ma, Yi},
  journal={arXiv preprint arXiv:2309.10313},
  year={2023}
}

@article{wang2022preserving,
  title={Preserving In-Context Learning ability in Large Language Model Fine-tuning},
  author={Wang, Yihan and Si, Si and Li, Daliang and Lukasik, Michal and Yu, Felix and Hsieh, Cho-Jui and Dhillon, Inderjit S and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2211.00635},
  year={2022}
}

@article{liu2023we,
  title={We're Afraid Language Models Aren't Modeling Ambiguity},
  author={Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A and Choi, Yejin},
  journal={arXiv preprint arXiv:2304.14399},
  year={2023}
}

@article{berglund2023reversal,
  title={The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}

@article{huang2022towards,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}


@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhou2023dont,
  title={Don't Make Your LLM an Evaluation Benchmark Cheater},
  author={Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  journal={arXiv preprint arXiv:2311.01964},
  year={2023}
}

@article{LLMSurvey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{chatgpt,
  title={Introducing chatgpt},
  author={OpenAI},
  year={2022},
  journal={https://openai.com/blog/chatgpt}
}

@article{gallegos2023bias,
    title={Bias and Fairness in Large Language Models: A Survey},
    author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
    journal={arXiv preprint arXiv:2309.00770},
    year={2023}
}

@inproceedings{tan2023chatgpt,
  title={Can ChatGPT replace traditional KBQA models? An in-depth analysis of the question answering performance of the GPT LLM family},
  author={Tan, Yiming and Min, Dehai and Li, Yu and Li, Wenbo and Hu, Nan and Chen, Yongrui and Qi, Guilin},
  booktitle={International Semantic Web Conference},
  pages={348--367},
  year={2023},
  organization={Springer}
}

@article{yao2023editing,
  title={Editing large language models: Problems, methods, and opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}



@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{TempQuestions,
  title={Tempquestions: A benchmark for temporal question answering},
  author={Jia, Zhen and Abujabal, Abdalghani and Saha Roy, Rishiraj and Str{\"o}tgen, Jannik and Weikum, Gerhard},
  booktitle={Companion Proceedings of the The Web Conference 2018},
  pages={1057--1062},
  year={2018}
}

@article{kotha2023understanding,
  title={Understanding catastrophic forgetting in language models via implicit inference},
  author={Kotha, Suhas and Springer, Jacob Mitchell and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2309.10105},
  year={2023}
}

@article{goodfellow2015empirical,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@inproceedings{TruthfulQA,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3214--3252},
  year={2022}
}

@article{C-Eval,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{BigBench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@article{MMLU,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{AGIEval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}

@software{Open-LLM-Leaderboard,
  author = {Daniel Park},
  title = {Open-LLM-Leaderboard-Report},
  url = {https://github.com/dsdanielpark/Open-LLM-Leaderboard-Report},
  year = {2023}
}


@article{Pinocchio,
  title={Do Large Language Models Know about Facts?},
  author={Hu, Xuming and Chen, Junzhe and Li, Xiaochuan and Guo, Yufei and Wen, Lijie and Yu, Philip S and Guo, Zhijiang},
  journal={arXiv preprint arXiv:2310.05177},
  year={2023}
}


@inproceedings{yin-etal-2023-large,
  title={Do Large Language Models Know What They Don’t Know?},
  author={Yin, Zhangyue and Sun, Qiushi and Guo, Qipeng and Wu, Jiawen and Qiu, Xipeng and Huang, Xuan-Jing},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={8653--8665},
  year={2023}
}

@article{dao2023flashattention2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@inproceedings{liu-etal-2022-p,
  title={P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={61--68},
  year={2022}
}

@article{wang2024myanswer,
  title={" My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models},
  author={Wang, Xinpeng and Ma, Bolei and Hu, Chengzhi and Weber-Genzel, Leon and R{\"o}ttger, Paul and Kreuter, Frauke and Hovy, Dirk and Plank, Barbara},
  journal={arXiv preprint arXiv:2402.14499},
  year={2024}
}

@article{GPT4,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv},
  year={2023}
}

@article{GPT-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{GPT2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{GPT,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}


@inproceedings{azaria-mitchell-2023-internal,
  title={The Internal State of an LLM Knows When It’s Lying},
  author={Azaria, Amos and Mitchell, Tom},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={967--976},
  year={2023}
}

@article{kasai2022realtimeqa,
  title={RealTime QA: What's the Answer Right Now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{vu2023freshllms,
  title={Freshllms: Refreshing large language models with search engine augmentation},
  author={Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and others},
  journal={arXiv preprint arXiv:2310.03214},
  year={2023}
}

@inproceedings{HaluEval,
  title={Halueval: A large-scale hallucination evaluation benchmark for large language models},
  author={Li, Junyi and Cheng, Xiaoxue and Zhao, Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{TQ,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@article{sun2019rotate,
  title={Rotate: Knowledge graph embedding by relational rotation in complex space},
  author={Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},
  journal={arXiv preprint arXiv:1902.10197},
  year={2019}
}

@inproceedings{yasunaga2022deep,
  title={Deep bidirectional language-knowledge graph pretraining},
  author={Yasunaga, Michihiro and Bosselut, Antoine and Ren, Hongyu and Zhang, Xikun and Manning, Christopher D and Liang, Percy S and Leskovec, Jure},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37309--37323},
  year={2022}
}

@article{liang2023holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{gemmateam2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{ren2023investigating,
  title={Investigating the factual knowledge boundary of large language models with retrieval augmentation},
  author={Ren, Ruiyang and Wang, Yuhao and Qu, Yingqi and Zhao, Wayne Xin and Liu, Jing and Tian, Hao and Wu, Hua and Wen, Ji-Rong and Wang, Haifeng},
  journal={arXiv preprint arXiv:2307.11019},
  year={2023}
}

@article{luo2023reasoning,
  title={Reasoning on graphs: Faithful and interpretable large language model reasoning},
  author={Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  journal={arXiv preprint arXiv:2310.01061},
  year={2023}
}

@article{kim2023kggpt,
  title={KG-GPT: A general framework for reasoning on knowledge graphs using large language models},
  author={Kim, Jiho and Kwon, Yeonsu and Jo, Yohan and Choi, Edward},
  journal={arXiv preprint arXiv:2310.11220},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{jiang-etal-2023-reasoninglm,
  title={ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph},
  author={Jiang, Jinhao and Zhou, Kun and Zhao, Wayne Xin and Li, Yaliang and Wen, Ji-Rong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3721--3735},
  year={2023}
}

@article{zhang2024knowledge,
  title={Knowledge Graph Enhanced Large Language Model Editing},
  author={Zhang, Mengqi and Ye, Xiaotian and Liu, Qiang and Ren, Pengjie and Wu, Shu and Chen, Zhumin},
  journal={arXiv preprint arXiv:2402.13593},
  year={2024}
}

@article{yang2024kgrank,
  title={KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques},
  author={Yang, Rui and Liu, Haoran and Zeng, Qingcheng and Ke, Yu He and Li, Wanxin and Cheng, Lechao and Chen, Qingyu and Caverlee, James and Matsuo, Yutaka and Li, Irene},
  journal={arXiv preprint arXiv:2403.05881},
  year={2024}
}

@inproceedings{suchanek2007yago,
  title={Yago: a core of semantic knowledge},
  author={Suchanek, Fabian M and Kasneci, Gjergji and Weikum, Gerhard},
  booktitle={Proceedings of the 16th international conference on World Wide Web},
  pages={697--706},
  year={2007}
}

@inproceedings{carlson2010toward,
  title={Toward an architecture for never-ending language learning},
  author={Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam and Mitchell, Tom},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={24},
  pages={1306--1313},
  year={2010}
}

@article{sun2023head,
  title={Head-to-tail: How knowledgeable are large language models (llm)? AKA will llms replace knowledge graphs?},
  author={Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
  journal={arXiv preprint arXiv:2308.10168},
  year={2023}
}

@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  pages={151--175},
  year={2010},
  publisher={Springer}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@misc{CommonsenseQA,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00937}, 
}

@misc{lin2022truthfulqameasuringmodelsmimic,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.07958}, 
}