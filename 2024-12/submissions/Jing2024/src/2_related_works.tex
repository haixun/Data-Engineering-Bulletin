\vspace{-2mm}
\section{Related Work}
\vspace{-2mm}
\paragraph{\cz{Factuality Issue of LLMs}}
Factuality issue~\cite{wang2023survey, zhang2023siren}, is the issue that LLMs may produce content inconsistent with established facts. As outlined in~\cite{wang2023survey}, this issue may be due to: \cz{\textbf{\textit{(\rmnum{1})}}} LLMs lacking expertise in specific domains~\cite{ScienceQA,bolton2024biomedlm}; \cz{\textbf{\textit{(\rmnum{2})}}} LLMs' unawareness of recent developments or changes~\cite{TempQuestions,yao2023editing}; \cz{\textbf{\textit{(\rmnum{3})}}} LLMs not retaining~\cite{wang2023evaluating,TQ,NaturalQuestions} or forgetting~\cite{goodfellow2015empirical,kotha2023understanding,wang2022preserving, chen2020recall, zhai2023investigating} knowledge from its training corpus; and \cz{\textbf{\textit{(\rmnum{4})}}} LLMs failing to reason with the knowledge they possess~\cite{liu2023we, berglund2023reversal,tan2023chatgpt}. The factuality issue has been addressed by various works, by incorporating Retrieval Augmented Generation (RAG)~\cite{lewis2020retrieval, wang2024blendfilter}, fine-tuning~\cite{tian2023finetuning}, and knowledge-enhanced models~\cite{feng-etal-2023-factkb,diao2023mixtureofdomainadapters}. 
To summarize, these approaches integrate other knowledge sources into the LLMs' training process or use them to augment the models' knowledge base, thus alleviating the factuality issue. Our work differ from them in that we evaluate the factuality of LLMs, rather than providing factuality enhancement methods.


\paragraph{\cz{Factuality Evaluation of LLMs}}
The expanding use of LLMs across various domains necessitates the assurance of their output's accuracy and reliability. 
A range of benchmarks and evaluation methodologies for assessing large language models (LLMs) are proposed.
These works primarily focus on evaluating the factuality, truthfulness, reasoning capabilities, and adaptability to new information of LLMs. MMLU~\cite{MMLU} and TruthfulQA \cite{TruthfulQA} aim to measure the factuality and truthfulness of LLMs across diverse tasks, while C-Eval \cite{C-Eval} focuses on the Chinese context, assessing models' knowledge of Chinese culture and laws.
%  BigBench  \cite{BigBench} challenges LLMs with tasks beyond current capabilities, and HaluEval \cite{HaluEval}, SelfAware \cite{yin-etal-2023-large}.
% , and the Pinocchio \cite{Pinocchio} benchmark explore models' propensity for generating hallucinations, their self-awareness, and their reasoning skills, respectively. REALTIMEQA  \cite{kasai2022realtimeqa} and FreshQA  \cite{vu2023freshllms} introduce dynamic benchmarks that test LLMs on current events and up-to-date knowledge.
 There are also works \cite{sun2023head, liang2023holistic} that propose factuality evaluation using subsets of KGs. %
However, selecting subsets of KGs to test LLMs can introduce selection bias. For example, random sampling can focus more on a few popular domains or subjects more densely connected with others, thus not showing LLMs' factuality on diversified topics.
Our work addresses this limitation by %
proposing a resource-efficient method to evaluate the factuality of LLMs which allows evaluations on whole KGs instead of subsets, thus providing a more diversified and comprehensive evaluation of the LLMs' factuality, enabling an extensive assessment of LLM's factuality and reasoning abilities in a way that existing individual benchmarks do not as they only focus on specific aspects. 


\paragraph{\cz{Using KGs in LLMs}}
\cz{KGs} are structured representations of factual knowledge, typically in the form of (head, relation, tail) triples.  There are lots of efforts in constructing \cite{auer2007dbpedia}, and reasoning~\cite{bordes2013translating} on KGs. This has made KGs an indispensable resource of factual knowledge for AI tasks. 
Currently, the most common way of integrating KGs with LLMs is using KGs as an external knowledge source to enhance LLM performance by pre-training, fine-tuning, or in-context learning~\cite{yasunaga2022deep,jiang-etal-2023-reasoninglm, zhang2024knowledge,kim2023kggpt,luo2023reasoning}. Our work is different from these works in that we use KGs to evaluate the factuality of LLMs, rather than enhancing the LLMs with KGs. 


