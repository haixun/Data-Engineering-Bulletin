\vspace{-0.3cm}
\section{Conclusions}
We introduce \GraphEval{}, an innovative approach for appraising the efficacy of LLMs against a voluminous test dataset derived from an extensive knowledge graph containing over 10 million facts, significantly mitigating the necessity for costly human intervention. \GraphEval{}, by embedding a judge module within the LLM itself, not only refines the evaluation process but also establishes a new benchmark for assessing the veracity of the information presented by these models. The empirical evidence from our experiments substantiates the judge model's proficiency in fact-checking, exhibiting a high degree of concordance with the accuracy of the LLM's outputs, and simultaneously diminishing the resources required for evaluation. The insights gleaned from our study shed light on the multifaceted performance of LLMs and lay the groundwork for future endeavors aimed at enhancing the reliability of their generated content. \wfj{Moreover, we consider extending this work to cross-lingual KGs to evaluate the performance of various LLMs in different languages.}

% \section*{Ethics Statement}
% We use publicly available knowledge graphs and large language models and do not collect any personal data. The DBpedia knowledge graph used is retrieved from Wikipedia, in which some content, although factually correct, may be offensive to certain readers. For example, some historical events damaging to certain races/countries.  
% We only evaluate the factuality of the LLMs, and do not use the LLMs for any other purposes. We hope that our work can contribute to the development of more reliable and trustworthy AI systems.

% \section*{Limitations}

% This research represents an initial foray into the realm of evaluating the factuality of LLMs by leveraging real-world-sized knowledge graphs, a critical step toward understanding and enhancing the reliability of LLM outputs. Amidst its pioneering efforts, several limitations have surfaced, sketching a roadmap for future investigative endeavors.
%  Firstly, the judge model, while efficient in this specific context, lacks versatility for other tasks and cannot generate text, suggesting an avenue for future research to enhance its functionality and application range. Secondly, the study relies solely on zero-shot evaluation, omitting potential improvements in LLM performance through methods like in-context few-shot learning, indicating a need for more comprehensive evaluation techniques. Additionally, the white-box evaluation method employed is restricted by its requirement for access to LLM hidden states, which is not universally available, pointing to the necessity for developing black-box evaluation strategies. Lastly, the research does not examine the method's efficacy on domain-specific knowledge graphs nor consider temporal relations, marking areas for future exploration to better understand and improve LLM factuality assessments.




