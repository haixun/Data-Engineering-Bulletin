
\newpage
\section{Appendix}


\input{submissions/Jing2024/src/experiments/judge_model}

\input{submissions/Jing2024/src/experiments/relation_type_ext.tex}

\input{submissions/Jing2024/src/experiments/correlation_analysis}

\input{submissions/Jing2024/src/experiments/knowledge_boundary}





\subsection{Detailed Settings}
\label{app:detailed_settings} 

\input{submissions/Jing2024/src/figures/relation_template_prompt.tex}
\input{submissions/Jing2024/src/tables/relation_templates.tex}   



\paragraph{Relation templates}
We use the relation templates to create queries for evaluating the models.
These templates are first generated by GPT with Web API in a few-shot manner, then manually curated to ensure the quality of the templates. We refer to Figure \ref{fig:relation_template_prompt} for the prompt used
for generating the relation templates. The prompt is designed to ask the model to generate a query for a given relation type. The model is asked to generate a query that can be used to judge the factuality of the relation type. We then manually curate the generated templates to ensure the quality of the templates. 
We refer to Table \ref{tab:relation_templates} for the curated relation templates.  Due to the large number of relation types in the DBpedia knowledge graph, we only showcase a few relation templates in the table, these templates are the most common relation types in the knowledge graph, sorted by the number of triples associated with the relation type.
 We can see that the relation templates are comprehensive and cover a wide range of topics. This can be seen as a source of multiple-domain knowledge for evaluating the LLMs.



\paragraph{Data and Model}
 We download the DBpedia data dump from \href{https://www.dbpedia.org/}{https://www.dbpedia.org/}. We use the turtle format of the DBpedia knowledge graph. 
 We directly use the LLaMA 2 and Gemma from the Hugging Face model hub. The model cards are
 \href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{meta-llama/Llama-2-7b-chat-hf}, \href{https://huggingface.com/meta-llama/Llama-2-13b-chat-hf}{ meta-llama/Llama-2-13b-chat-hf}, \href{https://huggingface.co/meta-llama/Llama-2-70b-chat-hf}{meta-llama/Llama-2-70b-chat-hf}, \href{https://huggingface.co/gemma-team/gemma-2b-chat-hf}{gemma-team/gemma-2b-chat-hf}, and \href{https://huggingface.co/gemma-team/gemma-7b-chat-hf}{gemma-team/gemma-7b-chat-hf}. 
 

\input{submissions/Jing2024/src/tables/instruction}

\paragraph{Instruction used for the LLaMA and Gemma models}
We report the instructions used for creating queries for the LLaMA and Gemma models. The instruction is designed to ask the model to judge whether the statement is true or false. We refer to Table \ref{tab:instruction} for the instruction used for creating queries. We use the same instruction for both the LLaMA and Gemma models with little modification to adjust the model's instruction format. With this instruction, the most frequent responses of LLMs are \textit{Yes, the statement is true}, \textit{No, the statement is false}, and \textit{I don't know}, with some variations on the suffix, mainly explaining the reason for the answer. This is what we expect from the LLMs when using a judge model (or the first-token logit as well), since the judge model doesn't use the LLM's response, but the hidden state of the LLM, which makes the consistency of the response format important. 


\subsection{Language Setting}

\xzrevision{As a framework, \GraphEval{} is not constrained by language, as long as the input is in the form of a knowledge graph. However, we did not conduct experiments on multilingual or cross-lingual datasets in the current work. Current experiments are conducted on English knowledge graphs.}
