
\subsection{Judge Model Analysis}
\label{app:judge_model}
We analyze the judge model's performance on the labeled validation set. 
We compare \GraphEval{}'s judge model by using the last token logit as the judge model. This is a common practice in evaluating LLMs, as the last token logit is the most common way to extract the hidden state of the LLMs. We also analyze the judge model with or without the prompt encoder (PE), as it may have a negative impact on the judge model's performance. We refer to Figure~\ref{fig:evaluation_scores} for the judge model's performance on the labeled validation set. 

\input{submissions/Jing2024/src/figures/peft_abla.tex}

\input{submissions/Jing2024/src/tables/abla_substitute.tex}

\input{submissions/Jing2024/src/tables/speed_test}

 \paragraph{Accuracy Analysis}
 The  \GraphEval{} model, both with and without Prompt Encoder (PE), consistently outperforms the score of using Last token logits in almost all configurations and metrics. This indicates the effectiveness of the  \GraphEval{} approach in capturing the nuances of the evaluation task.

 \paragraph{Ablation Study}
{\it On Prompt Encoder:}
As Figure~\ref{fig:evaluation_scores} shows, 
the comparison between models with and without PE indicates a slight performance variation. For \GraphEval{}, the presence of PE does not significantly alter the performance, suggesting that our method of evaluating LLMs is robust to the inclusion or exclusion of PE.
For the Last token logits method, removing PE generally results in a perturbation in performance. However, the  \GraphEval{} approach's consistency suggests a potentially different or more advanced mechanism of evaluation that is less dependent on PE.
{\it On Substitute Models:}
We also evaluate the judge model's performance on different LLMs as hidden state input. 
We refer to Table \ref{tab:abla_substitute} for the judge model's performance on different LLMs as hidden state input. We can see that, generally, when larger models are applied for feeding the hidden states, there is a slight increase in the fitting accuracy of the judge model. However, there is no significant difference in the judge model's performance.



\paragraph{Efficiency study}
We also analyze the judge model's efficiency by measuring the time it takes to make a prediction on one triple.
The speed of text generation refers to the average rate at which the LLM completes generating a response consisting of one sentence derived from a triple. It's important to recognize that the pace of text generation can vary with different prompts because the LLM may produce responses of varying lengths. Therefore, for a more consistent measure of text generation speeds, it's advisable to consider the rate of token generation. Despite this, our evaluation framework, \GraphEval{}, does not depend on text generation and operates on a triple-based unit. Consequently, we continue to use the triple as the unit of measurement for time.
We use the same hardware and software environment for all the experiments. We compare the average speed of the judge model with text generation. We report the time it takes to make a prediction in Table \ref{tab:speed_test}. 
 The attention implementation and precision are the same for text generation and for the judge model's input model. 
 We can see that the judge model is significantly faster than text generation. This indicates that the judge model is efficient in evaluating the LLMs. Also, benefiting from the substitute model, our evaluation speed and GPU requirement does not grow with the LLM size, which is an advantage for evaluating large LLMs.
 We also observe that, paradoxically, the Gemma 2B model operates slower than the Gemma 7B model, despite its smaller size. This counterintuitive result could be attributed to the implementation of Flash Attention 2. To draw a fair comparison, we documented the text generation speed on A6000 GPUs excluding Flash Attention 2, which is indicated within parentheses. The comparative data reveals that Gemma 2B is faster than Gemma 7B when Flash Attention 2 is not utilized. Notwithstanding this, Gemma 2B demonstrates enhanced performance when Flash Attention 2 is active. Therefore, for the sake of consistency, we have decided to maintain the results acquired with Flash Attention 2.
