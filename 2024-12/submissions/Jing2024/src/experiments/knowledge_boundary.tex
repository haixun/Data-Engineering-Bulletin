
\subsection{Knowledge Boundary Analysis}\label{app:knowledge_boundary}
\input{submissions/Jing2024/src/tables/knowledge_boundary}
\xzrevision{We analyze the knowledge boundaries of large language models (LLMs), as discussed in Section \ref{sub:performance_analysis} and \cite{ren2023investigating}, which suggest that larger models have a better understanding whether they know an answer or not. To investigate this hypothesis, we conduct experiments using the Llama 3 model series on two question-answering datasets: CommonsenseQA \cite{CommonsenseQA} and TruthfulQA \cite{TruthfulQA}. To assess whether an LLM understands its own knowledge boundaries, we directly elicit confidence scores for each answer through prompting, then we calculate the Expected Calibration Error (ECE). ECE measures the misalignment between the correctness of answers and the models' confidence. Mathematically, for LLM responses $\mathcal{A}$, ECE is defined as:}

\begin{equation} \text{ECE} = \frac{1}{|\mathcal{A}|}\sum_{a\in\mathcal{A}} \left|\mathbb{I}(a) - \text{conf}(a)\right|, \end{equation}

\xzrevision{where $\mathbb{I}(a)$ is an indicator function that outputs 1 if $a$ is correct and 0 otherwise, and $\text{conf}(a)$ denotes the confidence score assigned by the model. The experimental results are presented in Table \ref{tab:ece_results}.}

\xzrevision{The results in Table \ref{tab:ece_results} align with our previous hypothesis across different Llama 3 model sizes. For both CommonsenseQA and TruthfulQA, the ECE values decrease as model size increases, indicating better alignment between confidence and correctness in larger models. Specifically, for CommonsenseQA, the Llama 3 70B model achieves the lowest ECE (22.28\%), demonstrating superior calibration compared to smaller models like Llama 3 1B (54.65\%). Similarly, on TruthfulQA, the Llama 3 70B model achieves an ECE of 23.45\%, significantly outperforming the smaller Llama 3 1B model with an ECE of 58.35\%.}

\xzrevision{These findings align with the hypothesis that larger models are better calibrated in estimating their confidence, which can partially explain why larger models are more likely to answer ``I don't know'' when asked about a question as shown in Section \ref{sub:performance_analysis}.}

\xzrevision{Overall, \GraphEval{} aligns well with the results on TruthfulQA and CommonsenseQA, demonstrating that it effectively captures the model's factuality and informativeness across diverse knowledge domains. This alignment validates the robustness of our evaluation framework and confirms its consistency with established benchmarks for assessing LLM reasoning and truthfulness. }

