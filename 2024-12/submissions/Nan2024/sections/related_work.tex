%!TEX root = ../main.tex
\section{Related Works}
\label{sec:related}

\stitle{Retrieval Augmented Generation Question Answering.}
%
Large languaga models sometimes generate factually incorrect or misleading information, often due to a lack of real-time knowledge or limited access to external facts beyond their training data.
RAG-based Question Answering addresses this by integrating external knowledge retrieval into the generation process. By retrieving relevant document chunks through semantic search, RAG ensures that the model’s responses are grounded in accurate, real-world information, effectively reducing the likelihood of hallucinations.
Early approaches focused on jointly training the retriever and generator, ensuring that the retrieved content aligned with the generation model’s intent to provide more accurate answers~\cite{izacard2023atlas}. With the success of in-context learning, more recent work has treated the retriever as a separate module, directly providing retrieved information to the model via prompts~\cite{wang2023knowledgptenhancinglargelanguage}.
As retrieval technologies have advanced, RAG-based systems now support multimodal retrieval, enabling answers that draw from diverse data sources~\cite{chen2021open, chen-etal-2022-murag, luo-etal-2023-unifying}. 
%For example, OTT-QA~\cite{chen2021open}  retrieves both tables and text, MuRAG~\cite{chen-etal-2022-murag} integrates text and images, and MMQA~\cite{luo-etal-2023-unifying} combines text, tables, and images to handle complex queries.
% \yang{It's hard for us to compare Symphony with existing systems.}


\stitle{Trustworthiness of Large Language Models.}
%
The trustworthiness of LLMs is essential for their effective deployment in real-world applications. To assess LLM trustworthiness, researchers have proposed various approaches. For example, TrustLLM~\cite{huang2024trustllmtrustworthinesslargelanguage} provides a comprehensive framework for evaluating LLMs across different trust dimensions.
However, evaluating LLM trustworthiness remains challenging, with gaps in holistic assessment approaches. Some studies suggest that self-evaluation, where LLMs assess their confidence in the generated outputs, can help improve selective generation and mitigate inaccuracies~\cite{Ren2023SelfEvaluationIS}. Additionally, understanding the internal mechanisms of LLMs, such as the use of local intrinsic dimension (LID) for predicting truthfulness, has been proposed as a way to measure model reliability~\cite{Yin2024CharacterizingTI}.
In our work, we aim to improve the trustworthiness of LLMs through post-verification, ensuring that generated outputs are validated against reliable sources after generation to minimize inaccuracies and enhance their overall reliability.