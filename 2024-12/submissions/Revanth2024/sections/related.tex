\section{Related Work}
\label{sec:related-work}
\noindent \textbf{Pseudo-relevance feedback:} Our method has similarities with %the existing approach of 
Pseudo-Relevance Feedback (PRF) \cite{rocchio1971relevance, lv2009adaptive, li2022does} in IR: \cite{bendersky2011parameterized, xu2017quary} use the retrieved documents to improve sparse approaches via query expansion or query term reweighting, \cite{li2018nprf, zheng2020bert} score similarity between a target document and a top-ranked feedback document, while \cite{yu2021improving} train a separate query encoder that computes a new query embedding using the retrieved documents as additional input. In contrast, our approach does not require customized training feedback models or availability of explicit feedback data, as we improve the query vector by directly distilling from the reranker's output within an R\&R framework. %\pradeep{Why is our approach better?} 

Further, previous approaches to PRF have been dependent on the choice of retriever architecture and language; \cite{yu2021improving}'s PRF model is tied to the retriever used, \cite{chandradevan2022learning} explore cross-lingual relevance feedback, but require feedback documents in target language and thereby could only apply to three languages, while \cite{li2022interpolate} explore interpolating relevance feedback between dense and sparse approaches.
On the other hand, our approach is independent of the choice of the retriever and reranker architecture, and can be used for neural retrieval in any domain, language or modality. \\

\noindent \textbf{Distillation in Neural IR:} Existing approaches primarily leverage reranker feedback \textit{during training} of the dual-encoder retriever, to sample better negatives \cite{qu2021rocketqa}, for standard knowledge distillation of the cross-attention scores \cite{izacard2020distilling}, to train smaller and more efficient rankers by distilling larger models \cite{hofstatter2020improving}, or to align the geometry of dual-encoder embeddings with that from cross-encoders \cite{wang2021enhancing}. Instead, we leverage distillation at inference time, updating only the query representation to replicate the cross-encoderâ€™s scores for the corresponding test instance.
A key implication of this design choice is that unlike existing methods, we keep the retriever parameters unchanged, meaning \textsc{ReFIT} can be incorporated out-of-the-box into any neural R\&R framework. In contrast, extending training-time distillation to new languages or modalities would require re-training the bi-encoder.

More recently, \textsc{TouR}~\cite{sung2023optimizing} has proposed test-time optimization of query representations with two variants: \textsc{TouR}$_{\text{hard}}$ and  \textsc{TouR}$_{\text{soft}}$. 
\textsc{TouR}$_{\text{hard}}$ optimizes the marginal likelihood of a small set of (pseudo) positive contexts.
\textsc{ReFIT} shares similarities with \textsc{TouR}$_{\text{soft}}$, which uses the normalized scores of a cross-encoder over the retrieved results as soft labels.
Crucially, \textsc{TouR} relies on multiple iterations of relevance feedback via distillation, where each iteration runs until the top-1 retrieval result has the highest reranker score (in \textsc{TouR}$_{\text{soft}}$) or is a pseudo-positive (in \textsc{TouR}$_{\text{hard}}$).
This makes inference highly computationally expensive, as each additional iteration involves labeling top-$K$ retrieval results with a reranker and then retrieving again.
\textsc{ReFIT} improves efficiency over \textsc{TouR} by requiring only a single iteration of feedback that simply updates the query vector for longer, foregoing additional retrieval and reranking steps. More specifics on the inference process of the two methods can be found in \S{\ref{sec:tour_comparison}}.
\textsc{TouR} was evaluated only on English phrase and passage retrieval tasks, while we demonstrate \textsc{ReFIT}'s effectiveness in multidomain, multilingual and multimodal settings, with an empirical comparison with \textsc{TouR} in \S{\ref{sec:tour_comparison}}.