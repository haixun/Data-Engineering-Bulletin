@misc{touvron2023llama,
    archiveprefix = {arXiv},
    author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
    eprint = {2307.09288},
    primaryclass = {cs.CL},
    title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
    year = {2023}
}

@inproceedings{starteam,
title={Knowledge Graph Integration and Self-Verification for Comprehensive Retrieval-Augmented Generation},
author={Chenyuan Wu and Tingjia Shen and Ruiran Yan and Hao Wang and Zheng Liu and Zhen WANG and Defu Lian and Enhong Chen},
booktitle={2024 KDD Cup Workshop for Retrieval Augmented Generation},
year={2024},
url={https://openreview.net/forum?id=457wTt0ngj}
}

@article{future,
    author = {Xinxi Chen and Li Wang and Wei Wu and Qi Tang and Yiyao Liu},
    journal = {2024 KDD Cup Workshop for Retrieval Augmented Generation},
    title = {Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG},
    year = {2024}
}

@inproceedings{
dummy_model,
title={A Simple yet Effective Retrieval-Augmented Generation Framework for the Meta {KDD} Cup 2024},
author={Liyang He and Rui Li and Shuanghong Shen and Junyu Lu and Linbo Zhu and Yu Su and Zhenya Huang},
booktitle={2024 KDD Cup Workshop for Retrieval Augmented Generation},
year={2024},
url={https://openreview.net/forum?id=s9QAadOn6H}
}

@inproceedings{
bumblebee7,
title={{TCAF}: a Multi-Agent Approach of Thought Chain for Retrieval Augmented Generation},
author={Jun Zhao and Xiaojiang Liu},
booktitle={2024 KDD Cup Workshop for Retrieval Augmented Generation},
year={2024},
url={https://openreview.net/forum?id=nvjfWv7uGY}
}

@inproceedings{
dragonrangers,
title={{KDD} Cup Meta {CRAG} 2024 Technical Report: Three-step Question-Answering Framework},
author={Sungho Park and Jeongeum Seok and Jooyoung Lee and Joohyung Yun and Wonseok Lee},
booktitle={2024 KDD Cup Workshop for Retrieval Augmented Generation},
year={2024},
url={https://openreview.net/forum?id=G4Ei2QlKnv}
}

@misc{vslyu,
      title={WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs}, 
      author={Weijian Xie and Xuefeng Liang and Yuhui Liu and Kaihua Ni and Hong Cheng and Zetian Hu},
      year={2024},
      eprint={2408.07611},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.07611}, 
}

@article{electricsheep,
    author = {Ye Yuan and Chengwu Liu and Jingyang Yuan and Gongbo Sun and Siqi Li and Ming Zhang},
    journal = {2024 KDD Cup Workshop for Retrieval Augmented Generation},
    title = {A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning},
    year = {2024}
}

@article{mddh,
    title={MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering}, 
    author={Mitchell DeHaven},
    journal = {2024 KDD Cup Workshop for Retrieval Augmented Generation},
    year = {2024}
}

@article{apex,
      title={Revisiting the Solution of Meta KDD Cup 2024: CRAG}, 
      author={Jie Ouyang and Yucong Luo and Mingyue Cheng and Daoyu Wang and Shuo Yu and Qi Liu and Enhong Chen},
    journal = {2024 KDD Cup Workshop for Retrieval Augmented Generation},
    year = {2024}
}

@article{db3,
      title={Winning Solution For Meta KDD Cup' 24}, 
      author={Yikuan Xia and Jiazun Chen and Jun Gao},    journal = {2024 KDD Cup Workshop for Retrieval Augmented Generation},
    year = {2024}
}

@inproceedings{
riviera4,
title={{RAG} Approach Enhanced by Category Classification with {BERT}},
author={Yuki Taya and Daiki Ito and Shingo Maeda and Yusuke Hamano},
booktitle={2024 KDD Cup Workshop for Retrieval Augmented Generation},
year={2024},
url={https://openreview.net/forum?id=yzSkvdTYfm}
}


@misc{richardson2007beautiful,
  title={Beautiful soup documentation},
  author={Richardson, Leonard},
  year={2007},
  publisher={April}
}

@inproceedings{barbaresi2021trafilatura,
  title={Trafilatura: A web scraping library and command-line tool for text discovery and extraction},
  author={Barbaresi, Adrien},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
  pages={122--131},
  year={2021}
}

@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ni2021sentence,
  title={Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models},
  author={Ni, Jianmo and Abrego, Gustavo Hernandez and Constant, Noah and Ma, Ji and Hall, Keith B and Cer, Daniel and Yang, Yinfei},
  journal={arXiv preprint arXiv:2108.08877},
  year={2021}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{chen2024bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2402.03216},
  year={2024}
}

@misc{ms-marco,
    author = {ms-marco-MiniLM-L-12-v2},
    url = {https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2}
}

@article{yang2024crag,
    author = {Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and Gui, Rongze Daniel and Jiang, Ziran Will and Jiang, Ziyu and others},
    journal = {ArXiv preprint},
    title = {CRAG--Comprehensive RAG Benchmark},
    url = {https://arxiv.org/abs/2406.04744},
    volume = {abs/2406.04744},
    year = {2024}
}

@article{mavroudis2024langchain,
  title={LangChain},
  author={Mavroudis, Vasilios},
  year={2024},
  publisher={Preprints}
}

@article{panickssery2024llm,
    author = {Panickssery, Arjun and Bowman, Samuel R and Feng, Shi},
    journal = {ArXiv preprint},
    title = {Llm evaluators recognize and favor their own generations},
    url = {https://arxiv.org/abs/2404.13076},
    volume = {abs/2404.13076},
    year = {2024}
}

@misc{chatgpt2023,
    author = {{OpenAI}},
    howpublished = {\url{https://openai.com/index/chatgpt/}},
    note = {Accessed: 2024-06-04},
    title = {Chat{GPT}},
    year = {2023}
}

@article{usbeck2023qald,
    author = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and M{\"o}ller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and others},
    journal = {Semantic Web},
    number = {Preprint},
    pages = {1--15},
    publisher = {IOS Press},
    title = {QALD-10--The 10th challenge on question answering over linked data},
    year = {2023}
}

@article{bajaj2016ms,
    author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
    journal = {ArXiv preprint},
    title = {Ms marco: A human generated machine reading comprehension dataset},
    url = {https://arxiv.org/abs/1611.09268},
    volume = {abs/1611.09268},
    year = {2016}
}

@article{kwiatkowski2019natural,
    address = {Cambridge, MA},
    author = {Kwiatkowski, Tom  and
Palomaki, Jennimaria  and
Redfield, Olivia  and
Collins, Michael  and
Parikh, Ankur  and
Alberti, Chris  and
Epstein, Danielle  and
Polosukhin, Illia  and
Devlin, Jacob  and
Lee, Kenton  and
Toutanova, Kristina  and
Jones, Llion  and
Kelcey, Matthew  and
Chang, Ming-Wei  and
Dai, Andrew M.  and
Uszkoreit, Jakob  and
Le, Quoc  and
Petrov, Slav},
    doi = {10.1162/tacl_a_00276},
    journal = {Transactions of the Association for Computational Linguistics},
    pages = {452--466},
    publisher = {MIT Press},
    title = {Natural Questions: A Benchmark for Question Answering Research},
    url = {https://aclanthology.org/Q19-1026},
    volume = {7},
    year = {2019}
}

@inproceedings{chen2024benchmarking,
    author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    number = {16},
    pages = {17754--17762},
    title = {Benchmarking large language models in retrieval-augmented generation},
    volume = {38},
    year = {2024}
}

@article{vu2023freshllms,
    author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and others},
    journal = {ArXiv preprint},
    title = {Freshllms: Refreshing large language models with search engine augmentation},
    url = {https://arxiv.org/abs/2310.03214},
    volume = {abs/2310.03214},
    year = {2023}
}

@inproceedings{talmor2018web,
    address = {New Orleans, Louisiana},
    author = {Talmor, Alon  and
Berant, Jonathan},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    doi = {10.18653/v1/N18-1059},
    pages = {641--651},
    publisher = {Association for Computational Linguistics},
    title = {The Web as a Knowledge-Base for Answering Complex Questions},
    url = {https://aclanthology.org/N18-1059},
    year = {2018}
}

@inproceedings{lewis2020retrieval,
    author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
    url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
    year = {2020}
}

@inproceedings{guu2020retrieval,
    author = {Kelvin Guu and
Kenton Lee and
Zora Tung and
Panupong Pasupat and
Ming{-}Wei Chang},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/icml/GuuLTPC20.bib},
    booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
    pages = {3929--3938},
    publisher = {{PMLR}},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
    title = {Retrieval Augmented Language Model Pre-Training},
    url = {http://proceedings.mlr.press/v119/guu20a.html},
    volume = {119},
    year = {2020}
}

@inproceedings{izacard2020leveraging,
    address = {Online},
    author = {Izacard, Gautier  and
Grave, Edouard},
    booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
    doi = {10.18653/v1/2021.eacl-main.74},
    pages = {874--880},
    publisher = {Association for Computational Linguistics},
    title = {Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
    url = {https://aclanthology.org/2021.eacl-main.74},
    year = {2021}
}

@article{mallen2022not,
    author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
    journal = {ArXiv preprint},
    title = {When not to trust language models: Investigating effectiveness of parametric and non-parametric memories},
    url = {https://arxiv.org/abs/2212.10511},
    volume = {abs/2212.10511},
    year = {2022}
}

@article{sun2023head,
    author = {Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
    journal = {ArXiv preprint},
    title = {Head-to-tail: How knowledgeable are large language models (llm)? AKA will llms replace knowledge graphs?},
    url = {https://arxiv.org/abs/2308.10168},
    volume = {abs/2308.10168},
    year = {2023}
}

@inproceedings{yasunaga2021qa,
    address = {Online},
    author = {Yasunaga, Michihiro  and
Ren, Hongyu  and
Bosselut, Antoine  and
Liang, Percy  and
Leskovec, Jure},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    doi = {10.18653/v1/2021.naacl-main.45},
    pages = {535--546},
    publisher = {Association for Computational Linguistics},
    title = {{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering},
    url = {https://aclanthology.org/2021.naacl-main.45},
    year = {2021}
}

@inproceedings{sennrich-etal-2016-neural,
    address = {Berlin, Germany},
    author = {Sennrich, Rico  and
Haddow, Barry  and
Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
    publisher = {Association for Computational Linguistics},
    title = {Neural Machine Translation of Rare Words with Subword Units},
    url = {https://aclanthology.org/P16-1162},
    year = {2016}
}

@article{brown2020language,
  added-at = {2020-07-28T16:09:05.000+0200},
  author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  biburl = {https://www.bibsonomy.org/bibtex/27a2a9aee490ff30dd5b4d0470a8be8d8/albinzehe},
  interhash = {c02cbc3bfa91c08710d0db948c927dad},
  intrahash = {7a2a9aee490ff30dd5b4d0470a8be8d8},
  journal = {arXiv preprint arXiv:2005.14165},
  keywords = {gpt-3 kallimachos languagemodels proposal-knowledge transformer},
  timestamp = {2020-07-28T16:09:05.000+0200},
  title = {Language models are few-shot learners},
  year = 2020
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{statsqa,
  author       = {Yizhang Zhu and
                  Shiyin Du and
                  Boyan Li and
                  Yuyu Luo and
                  Nan Tang},
  title        = {Are Large Language Models Good Statisticians?},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@inproceedings{symphony,
  author       = {Zui Chen and
                  Zihui Gu and
                  Lei Cao and
                  Ju Fan and
                  Samuel Madden and
                  Nan Tang},
  title        = {Symphony: Towards Natural Language Query Answering over Multi-modal
                  Data Lakes},
  booktitle    = {{CIDR}},
  year         = {2023},
  url          = {https://www.cidrdb.org/cidr2023/papers/p51-chen.pdf},
}


@inproceedings{verifai,
  author       = {Nan Tang and
                  Chenyu Yang and
                  Ju Fan and
                  Lei Cao and
                  Yuyu Luo and
                  Alon Y. Halevy},
  title        = {VerifAI: Verified Generative {AI}},
  booktitle    = {{CIDR}}, 
  year         = {2024},
  url          = {https://www.cidrdb.org/cidr2024/papers/p5-tang.pdf},
}

@article{sun2024large,
author = {Sun, Yushi and Xin, Hao and Sun, Kai and Xu, Yifan Ethan and Yang, Xiao and Dong, Xin Luna and Tang, Nan and Chen, Lei},
title = {Are Large Language Models a Good Replacement of Taxonomies?},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681973},
doi = {10.14778/3681954.3681973},
abstract = {Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf. Our comprehensive experiments of eighteen LLMs under three prompting settings validate that LLMs perform miserably poorly in handling specialized taxonomies and leaf-level entities. Specifically, the QA accuracy of the best LLM drops by up to 30\% as we go from common to specialized domains and from root to leaf levels of taxonomies.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2919–2932},
numpages = {14}
}

@misc{bajaj2018ms,
      title={{MS MARCO}: A Human Generated MAchine Reading COmprehension Dataset}, 
      author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
      year={2018},
      eprint={1611.09268},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{rawte2023troubling,
  title={The Troubling Emergence of Hallucination in Large Language Models--An Extensive Definition, Quantification, and Prescriptive Remediations},
  author={Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, SM and Chadha, Aman and Sheth, Amit P and Das, Amitava},
  journal={arXiv preprint arXiv:2310.04988},
  year={2023}
}

@inproceedings{li2023chain,
  title={Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources},
  author={Li, Xingxuan and Zhao, Ruochen and Chia, Yew Ken and Ding, Bosheng and Joty, Shafiq and Poria, Soujanya and Bing, Lidong},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={15696--15707},
  year={2023},
  organization={PMLR}
}

@article{chen2023benchmarking,
  title={Benchmarking large language models in retrieval-augmented generation},
  author={Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  journal={arXiv preprint arXiv:2309.01431},
  year={2023}
}

@misc{brave24,
    title = {Brave {S}earch {API}},
    author={{Brave Software}},
    url = {https://search.brave.com/},
}

@misc{tpc,
    title = {TPC Benchmarks},
    author={tpc.org},
    url = {https://tpc.org/},
}

@inproceedings{bennett2007netflix,
    title = {The Netflix Prize},
    author = {James Bennett and Stan Lanning},
    booktitle = "SigKDD KDDCup",
    year = "2007"
}

@inproceedings{jia2009imagenet,
    author = "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li",
    title = "ImageNet: A Large-Scale Hierarchical Image Database",
    booktitle = "CVPR",
    year = "2009"
}

@inproceedings{thorne2018fever,
    author = "James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Arpit Mittal",
    title = "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
    booktitle = "ACL",
    year = 2018,
}

@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gao2024ragsurvey,
    title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
    author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
    jounral={arXiv preprint arXiv:2312.10997},
    year={2024}
}

@inproceedings{yasunaga-etal-2021-qa,
    title = "{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering",
    author = "Yasunaga, Michihiro  and
      Ren, Hongyu  and
      Bosselut, Antoine  and
      Liang, Percy  and
      Leskovec, Jure",
    year = "2021",
    publisher = "Association for Computational Linguistics",
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{lievin2024can,
  title={Can large language models reason about medical questions?},
  author={Li{\'e}vin, Valentin and Hother, Christoffer Egeberg and Motzfeldt, Andreas Geert and Winther, Ole},
  journal={Patterns},
  volume={5},
  number={3},
  year={2024},
  publisher={Elsevier}
}

@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
publisher = {Association for Computing Machinery},
volume = {55},
number = {12},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@misc{copilotpro2024,
  author = {{Microsoft Corporation}},
  title = {Copilot {P}ro},
  howpublished = {\url{https://copilot.cloud.microsoft/en-us/copilot-pro}},
  year = {2024},
  note = {Accessed: 2024-06-04}
}

@misc{perplexityai2023,
  author = {{Perplexity {AI}}},
  title = {Perplexity.ai},
  howpublished = {\url{https://www.perplexity.ai/}},
  note = {Accessed: 2024-06-04}
}

@misc{deepmind_gemini,
  author = {{Google DeepMind}},
  title = {Gemini {A}dvanced},
  howpublished = {\url{https://gemini.google.com/advanced}},
  note = {Accessed: 2024-06-04}
}

@misc{chatgptplus2023,
  author = {{OpenAI}},
  title = {Chat{GPT} {P}lus},
  howpublished = {\url{https://openai.com/index/chatgpt-plus/}},
  year = {2023},
  note = {Accessed: 2024-06-04}
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    month = jul,
    year = "2017",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{lfqa23,
author={Fangyuan Xu and Yixiao Song and Mohit Iyyer and Eunsol Choi},
Booktitle = {Association of Computational Linguistics},
Year = "2023",
Title={A Critical Evaluation of Evaluations for Long-form Question Answering},
}

@inproceedings{vu-moschitti-2021-ava,
    title = "{AVA}: an Automatic e{V}aluation Approach for Question Answering Systems",
    author = "Vu, Thuy  and
      Moschitti, Alessandro",
    year = "2021",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{chen-etal-2022-murag,
    title = "{M}u{RAG}: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
    author = "Chen, Wenhu  and
      Hu, Hexiang  and
      Chen, Xi  and
      Verga, Pat  and
      Cohen, William",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@article{achiam2023gpt,
  title={{GPT-4} technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{mallen2023,
    author = "Alex Mallen and Akari Asai and Victor Zhong and Rajarshi Das and Daniel Khashabi and Hannaneh Hajishirzi",
    title = "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
    booktitle = "ACL",
    year = 2023
}

@article{Schick2023ToolFormer,
    author = "Timo Schick and Jane Dwivedi-Yu and Roberto Dessi and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom",
    title = "Toolformer: Language Models Can Teach Themselves to Use Tools",
    journal = "arXiv",
    year = 2023
}

@article{gao2024llm,
  title={Llm-based nlg evaluation: Current status and challenges},
  author={Gao, Mingqi and Hu, Xinyu and Ruan, Jie and Pu, Xiao and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2402.01383},
  year={2024}
}

@article{tang2024multihop,
  title={Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries},
  author={Tang, Yixuan and Yang, Yi},
  journal={arXiv preprint arXiv:2401.15391},
  year={2024}
}

@article{pradeep2024ragnar,
  title={Ragnar$\backslash$" ok: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track},
  author={Pradeep, Ronak and Thakur, Nandan and Sharifymoghaddam, Sahel and Zhang, Eric and Nguyen, Ryan and Campos, Daniel and Craswell, Nick and Lin, Jimmy},
  journal={arXiv preprint arXiv:2406.16828},
  year={2024}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}


@inproceedings{xin2024journey,
author = {Dong, Xin Luna},
title = {The Journey to a Knowledgeable Assistant with Retrieval-Augmented Generation (RAG)},
year = {2024},
isbn = {9798400704222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626246.3655999},
doi = {10.1145/3626246.3655999},
abstract = {For decades, multiple communities (Database, Information Retrieval, Natural Language Processing, Data Mining, AI) have pursued the mission of providing the right information at the right time. Efforts span web search, data integration, knowledge graphs, question answering. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in comprehending and generating human language, revolutionizing techniques in every front. However, their inherent limitations such as factual inaccuracies and hallucinations make LLMs less suitable for creating knowledgeable and trustworthy assistants.This talk describes our journey in building a knowledgeable AI assistant by harnessing LLM techniques. We start with our findings from a comprehensive set of experiments to assess LLM reliability in answering factual questions and analyze performance variations across different knowledge types. Next, we describe our federated Retrieval-Augmented Generation (RAG) system that integrates external information from both the web and knowledge graphs for trustworthy text generation on real-time topics like stocks and sports, as well as on torso-to-tail entities like local restaurants. Additionally, we brief our explorations on extending our techniques towards multi-modal, contextualized, and personalized Q&A. We will share our techniques, our findings, and the path forward, high- lighting how we are leveraging and advancing the decades of work in this area.},
booktitle = {Companion of the 2024 International Conference on Management of Data},
pages = {3},
numpages = {1},
keywords = {RAG (retrieval augmented generation), data integration, generative AI, knowledge graph, question answering},
location = {Santiago AA, Chile},
series = {SIGMOD/PODS '24}
}


@misc{saadfalcon2023ares,
      title={ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems}, 
      author={Jon Saad-Falcon and Omar Khattab and Christopher Potts and Matei Zaharia},
      year={2023},
      eprint={2311.09476},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{es-etal-2024-ragas,
    title = "{RAGA}s: Automated Evaluation of Retrieval Augmented Generation",
    author = "Es, Shahul  and
      James, Jithin  and
      Espinosa Anke, Luis  and
      Schockaert, Steven",
    editor = "Aletras, Nikolaos  and
      De Clercq, Orphee",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-demo.16",
    pages = "150--158",
    abstract = "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",
}


@article{liu2024cofe,
  title={CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity},
  author={Liu, Jintao and Ding, Ruixue and Zhang, Linhao and Xie, Pengjun and Huang, Fie},
  journal={arXiv preprint arXiv:2410.12248},
  year={2024}
}

@misc{tonic_validate,
  author       = {Tonic AI},
  title        = {Tonic Validate},
  year         = 2023,
  url          = {https://github.com/TonicAI/tonic_validate},
  note         = {Accessed: 2024-11-10}
}

@article{han2024rag,
  title={Rag-qa arena: Evaluating domain robustness for long-form retrieval augmented question answering},
  author={Han, Rujun and Zhang, Yuhao and Qi, Peng and Xu, Yumo and Wang, Jenyuan and Liu, Lan and Wang, William Yang and Min, Bonan and Castelli, Vittorio},
  journal={arXiv preprint arXiv:2407.13998},
  year={2024}
}

@misc{zindi_rag_competition,
  title        = {Retrieval-Augmented Generation (RAG) for Public Services and Administration Tasks},
  howpublished = {\url{https://zindi.africa/competitions/retrieval-augmented-generation-rag-for-public-services-and-administration-tasks}},
  note         = {Accessed: 2024-11-10}
}

@misc{finance_rag_challenge,
  title        = {FinanceRAG Challenge: Retrieval-Augmented Generation (RAG) for Financial Documents},
  howpublished = {\url{https://finance-rag.com/}},
  note         = {Accessed: 2024-11-10}
}

@misc{trustbit_enterprise_rag_challenge,
  author       = {Trustbit},
  title        = {Enterprise RAG Challenge},
  year         = 2024,
  url          = {https://github.com/trustbit/enterprise-rag-challenge},
  note         = {Accessed: 2024-11-10}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}