\section{Discussion and future work}
\label{sec:futurework}
The KDD Cup CRAG 2024 competition established a benchmark for evaluating Retrieval-Augmented Generation (RAG) systems with three challenging tasks: retrieval summarization, knowledge graph and web retrieval, and end-to-end RAG. Questions covered various domains, dynamism levels, types, and entity popularity. Key learnings from hosting the challenge (Section  \ref{sec:hosting}) emphasized the need for standardized retrieval content and retrieval access, controlled model constraints, maintaining a private test set, and mixed automated and human evaluation strategies to ensure fairness and manage cost.

CRAG highlighted persistent challenges in RAG systems. First, reducing retrieval noises from the web and from KG is critical as irrelevant or contradicting information degrades answer quality. Second, handling real-time or fast-changing questions remains difficult due to the need for nuanced temporal reasoning. Third, questions that require aggregation, multi-hop reasoning or post-processing are challenging as they require complex logic to integrate knowledge across multiple sources. Lastly, reducing hallucinations in summarization remains essential. To address the above challenges, winning teams leveraged a variety of solutions (Section \ref{sec:winning-solutions}). For example, developing multi-step pipelines to parse, chunk, and rank web contents reliably, creating regularized APIs to enhance KG retrieval expressiveness, incorporating external libraries to improve temporal reasoning, leveraging chain-of-thought techniques to break down complex queries, and fine-tuning LLM and implementing confidence estimation component to reduce hallucinations. The practical applications of these findings have significant implications for improvements in industry-grade RAG systems.

The CRAG competition highlights several promising directions for future RAG research: Enhancing web retrieval quality via better handling of structured and semi-structured data, improving temporal reasoning for real-time queries, and refining methods for multi-hop and aggregation tasks can improve response reliability. Additionally, targeted fine-tuning may be essential to effectively reduce hallucinations. Looking ahead, future CRAG-style competitions could expand the question sets to include multimodal, multi-turn questions, integrating text with images or structured data for a more realistic RAG environment. As researchers pursue these directions, they will also need to address key scalability challenges in building RAG systems that can handle real-time and multimodal data, including ensuring data consistency across diverse sources and achieving low-latency processing of real-time multimodal data. By tackling these challenges, researchers can unlock the full potential of RAG systems and improve response reliability.