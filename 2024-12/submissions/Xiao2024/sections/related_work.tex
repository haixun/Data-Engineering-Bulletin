\section{Related work}
\label{sec:relatedwork}
\subsection{RAG Benchmarks}
We compare and highlight the strengths and limitations of existing retrieval-augmented generation (RAG) benchmarks across several critical dimensions, as demonstrated in table~\ref{tab:benchmark_comparison} in CRAG~\cite{yang2024crag}. QALD-10~\cite{usbeck2023qald} focuses primarily on knowledge graph search over Wikidata but does not incorporate web retrieval, limiting its capacity to test models in unstructured environments. It lacks mock APIs or dynamic question capabilities, reducing its ability to simulate real-world, evolving knowledge scenarios. MS MARCO~\cite{bajaj2016ms}, while widely used for open-domain question answering, primarily emphasizes passage retrieval and lacks integration with knowledge graphs. It also fails to test for long-tail facts, as it predominantly handles popular, factoid-based information sourced from common queries, limiting its coverage of less frequent, tail facts. Natural Questions~\cite{kwiatkowski2019natural} focuses on more complex and long-form queries but remains heavily constrained to Wikipedia-based knowledge and lacks dynamic or API-driven retrieval. RGB~\cite{chen2024benchmarking}, on the other hand, introduces a focus on long-tail facts but does not fully test retrieval beyond specific domains, missing integration with KG searches or dynamic question handling. FreshLLMs~\cite{vu2023freshllms} excels in testing models’ ability to retrieve and update real-time knowledge but remains narrow, lacking domain diversity and focus on structured retrieval such as knowledge graphs. 

Despite having smaller question size than MS MARCO and NQ, CRAG~\cite{yang2024crag} stands out by combining web retrieval, knowledge graph search, and mock APIs, simulating diverse retrieval environments. It goes beyond Wikipedia, tackling dynamic questions and ensuring comprehensive coverage of both torso and tail facts across multiple domains, making it a more robust and versatile benchmark for evaluating next-generation RAG systems.



\begin{table*}[t]
\small
  \caption{Comparing CRAG to existing benchmarks for factual question answering.\label{tab:benchmark_comparison}}
  \centering
  \begin{tabular}{lccccccc}
    \toprule
    Benchmark & \makecell{Web \\ retrieval} & \makecell{KG \\ search} & \makecell{Mock \\ API} & \makecell{Dynamic \\ question} & \makecell{Torso and \\ tail facts} & \makecell{Beyond \\ Wikipedia} & \makecell{Question \\ size} \\
    \midrule
    QALD-10~\cite{usbeck2023qald} & \color{red}\xmark & \color{green}\checkmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & 0.8K  \\
    MS MARCO~\cite{bajaj2016ms} & \color{green}\checkmark & \color{red}\xmark & \color{red}\xmark  & not explicitly & not explicitly & \color{green}\checkmark & 100K  \\
    NQ~\cite{kwiatkowski2019natural} & \color{green}\checkmark & \color{red}\xmark & \color{red}\xmark & not explicitly & not explicitly & \color{red}\xmark & 323K \\
    RGB~\cite{chen2023benchmarking} & \color{green}\checkmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{green}\checkmark & 1K \\
    FreshLLM~\cite{vu2023freshllms} & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{green}\checkmark & \color{red}\xmark & \color{green}\checkmark & 0.6K \\
    CRAG~\cite{yang2024crag} & \color{green}\checkmark & \color{green}\checkmark & \color{green}\checkmark & \color{green}\checkmark & \color{green}\checkmark & \color{green}\checkmark  & 4.4K\\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{RAG Systems}
RAG systems have evolved significantly over time, focusing on improving large language models (LLMs) by integrating retrieval mechanisms for enhanced knowledge access. Talmor et al.~\cite{talmor2018web} pioneered the use of the web as a knowledge base to answer complex questions, emphasizing retrieval for reasoning tasks. Lewis et al.~\cite{lewis2020retrieval} formally introduced RAG, combining dense retrieval with generative models to solve knowledge-intensive NLP tasks, setting a strong foundation for subsequent models. REALM~\cite{guu2020retrieval} improved this by incorporating retrieval into pre-training, making retrieval part of both the learning and fine-tuning process, which was further extended by Fusion-in-Decoder (FiD)~\cite{izacard2020leveraging}, fusing multiple retrieved documents into the generation process for improved question answering. Mallen et al.~\cite{mallen2022not} explored the effectiveness of parametric and non-parametric memory mechanisms to determine when models should rely on internal versus external knowledge. Similarly, Sun et al.~\cite{sun2023head} investigated whether LLMs could replace traditional knowledge graphs, assessing how well LLMs store and retrieve structured knowledge. QA-GNN~\cite{yasunaga2021qa} integrated retrieval-augmented methods with reasoning from knowledge graphs, combining structured and unstructured knowledge for better question answering. Meanwhile, Mallen et al.~\cite{mallen2022not} focused on trust in LLMs, analyzing the limitations of parametric knowledge and advocating for non-parametric memory integration. Recently, FreshLLMs~\cite{vu2023freshllms} refreshed LLM knowledge using search engine augmentation to maintain up-to-date responses. These works collectively contribute to a growing body of research that refines LLM performance by bridging the gap between parametric knowledge and external, retrievable information.

\subsection{RAG System Evaluation}
In recent developments for evaluating RAG systems, multiple frameworks have proposed solutions aimed at addressing key challenges like retrieval relevance, truthfulness, and the efficiency of the evaluation process. ARES~\cite{saadfalcon2023ares} offers an automated evaluation approach by generating synthetic data and fine-tuning lightweight language models, allowing for scalable assessments of retrieval relevance, answer faithfulness and answer relevance. However, ARES may face issues with domain adaptation, as its synthetic data may not always represent the nuances of diverse real-world datasets. RAGAS~\cite{es-etal-2024-ragas} introduces a reference-free evaluation method, providing metrics for context relevance without relying on ground truth annotations, reducing human labeling costs. Nevertheless, its heuristic-based measures may struggle with capturing deeper semantic nuances and can exhibit biases in complex scenarios. CoFE-RAG~\cite{liu2024cofe} performs an exhaustive evaluation across the entire RAG pipeline, employing multi-granularity keyword-based assessment of the retrieved context. RAG-QA Arena~\cite{han2024rag} proposes a pairwise preference evaluation framework with truthfulness as the primary criterion, leveraging ground truth to gauge the evaluation system quality.

\subsection{RAG Competitions}
More recently, the TREC 2024 RAG Track is proposed, featuring Ragnarök~\cite{pradeep2024ragnar}, an open-source, end-to-end evaluation framework with MS MARCO V2.1 collection and industrial baselines like OpenAI's GPT-4o and Cohere's Command R+. The framework provides a web-based interface for benchmarking pairwise RAG systems through a RAG battle arena.

Various domain-specific competitions, such as the Zindi RAG challenge~\cite{zindi_rag_competition} for public services, the FinanceRAG competition~\cite{finance_rag_challenge}, and the Trustbit Enterprise RAG Challenge~\cite{trustbit_enterprise_rag_challenge}, push the boundaries of RAG applications. These challenges emphasize tailored RAG systems that retrieve domain-specific information and generate context-aware responses, demonstrating the potential of RAG to enhance service delivery in public administration, financial analysis, and business document comprehension.