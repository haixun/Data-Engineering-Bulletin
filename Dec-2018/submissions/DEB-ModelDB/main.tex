\documentclass[11pt]{article}
\usepackage{deauthor}
\usepackage{times,graphicx,color,xspace}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{subfig}
\newcommand{\mpv}[1]{\textcolor{blue}{mpv: #1}}
\newcommand{\mlwfs}{ML workflows\xspace}
\newcommand{\mdb}{{\sc ModelDB}\xspace}
\newcommand{\mlwf}{ML workflow\xspace}
\newcommand{\wf}{workflow\xspace}
\newcommand{\wfs}{workflows\xspace}
\newcommand{\Dss}{Data scientists\xspace}
\newcommand{\Ds}{Data scientist\xspace}
\newcommand{\dss}{data scientists\xspace}
\newcommand{\ds}{data scientist\xspace}
\newcommand{\mldevs}{ML developers\xspace}
\newcommand{\mldev}{ML developer\xspace}
\newcommand{\IWE}{We\xspace}
\newcommand{\iwe}{we\xspace}

\title{\mdb: Opportunities and Challenges in Managing Machine Learning Models}
\author{
  Manasi Vartak\\
  MIT CSAIL \\
  \texttt{mvartak@csail.mit.edu}
  \and
  Samuel Madden\\
  MIT CSAIL \\
  \texttt{madden@csail.mit.edu}
}

\begin{document}

\maketitle
\begin{abstract}
Machine learning applications have become ubiquitous in a variety of domains.
Powering each of these ML applications are one or more machine learning models that are used to make key decisions or compute key quantities.
The life-cycle of an ML model starts with data processing, going on to feature engineering, model experimentation, deployment, and maintenance.
We call the process of tracking a model across all phases of its life-cycle as {\bf model management}.
In this paper, we discuss the need for model management systems, describe \mdb, the first open-source model management system developed at MIT, and discuss the opportunities and challenges in managing models. 
\end{abstract}

\section{Introduction}
\label{sec:model-management}

Machine learning has become ubiquitous in a variety of applications including voice assistants, self-driving cars, and recommendation systems.
Each ML-based application employs one or more machine learning models that are used to make key decisions or compute key quantities such as recognizing spoken words, detecting a pedestrian on the road, and identifying the best products to recommend.
Models are to ML-based applications what databases are to stateful web-applications; they are key for the correct functioning of these applications.
Consequently, just like we have database management systems (or DBMSs) to manage state in applications, we find the need for centralized systems that manage models, i.e., {\it model management systems}.

% While a DBMS serves the purpose of managing data throughout its life-cycle (i.e., creation, querying, update, and delete); a model management system serves the purpose of storing and querying {\it metadata} about models throughout their life-cycle.

% We begin with a brief overview of a typical model life-cycle and use it to motivate the need for model management.
To understand the requirements of a model management system, we begin with a brief overview of the life-cycle of a machine learning model.
The life-cycle of an ML model can be considered to consist of five phases, namely: 
(1) {\em Data Preparation} - Obtaining the training and test data to develop a model; 
(2) {\em Feature Engineering} - Identifying or creating the appropriate descriptors from the input data (i.e., features) to be used by the model; 
(3) {\em Experimentation} - Experimenting with different models on the training and test data and choosing the best; 
(4) {\em Deployment} - Deploying the chosen model in a live system; 
(5) {\em Maintenance} - Monitoring the live model performance and updating the model as needed.
% Since developing a high quality machine learning model is heavily dependent on data and the task, many of these steps are iterative.
At every phase of the model life-cycle and during each iteration, certain pieces of metadata are essential for ensuring the key requirements at that phase.
For example, for Phases 1-2, the key requirement for a model in that phase is reproducibility, i.e., enough data about the data preparation and feature engineering must be tracked so that the said features can be re-created exactly.
Similarly, for Phase 3, the key requirement, along with reproducibility, is experiment tracking so that a \dss can choose the best model.
For Phases 4-5, the key requirements deal with making model-related data widely accessible and ensuring that key metrics for the models are collected regularly.

\newpage

% Model management, at this time, is a loosely defined term used to capture different operations one might wish to perform when building or operationalizing machine learning models.
We define a model management system as one that follows a model (whether deployed or not) throughout these five phases of its life-cycle and captures relevant metadata at each step.
For instance, since the key requirement during the Experimentation phase is to enable the \dss to choose the best model, metadata captured in this phases includes items such as performance metrics obtained by the model, hyperparameter values used during training, etc.
In contrast, for a model in the Deployment phase, metadata might include the version of the model, where it is deployed, and how to query it.
Although one can imagine a model management system that stores models and supports prediction serving operations, the heterogeneity in models, as well as their hardware and software requirements make such a solution sub-optimal.
Therefore, we take the view that model management systems are best suited to store key metadata about models throughout their life-cycle.
Thus, {\bf we define a model management system as a system that tracks metadata about models through the five phases of their life-cycle}.

% As machine learning deployments are becoming ubiquitous, various systems have been
% developed in academia and in industry to manage the life-cycle of \mlwfs.
% To the best of our knowledge, \mdb is the first open-source system for model
% management.

Given the rapid proliferation of machine learning applications, systems have been proposed in academia as well as industry to address different aspects of the model management problem.
In this paper, we focus on \mdb, the first open-source system we developed at MIT for model management.
Other academic systems that seek to address similar problems include the ModelHub system~\cite{modelhub} (to explore deep learning architectures and efficiently store network weights), 
ProvDB~\cite{provdb} (to manage 
metadata collected via collaborative data science), Ground~\cite{hellerstein2017ground} (to provide a common
framework for tracking data origin and use via generic abstractions), and the work on model selection management systems by Kumar et. al.~\cite{msms}.

One of the earliest commercial model management system was the SAS Model Manager which tracked models built and deployed within the SAS platform~\cite{sas-model-manager}.
Today, most proprietary ML platforms such as the Michelangelo platform at Uber~\cite{michelangelo} and FBLearner at Facebook~\cite{fblearner} include a model management or model repository component.
Similarly, vast majority of data science teams end up re-building a model management system to suit their needs.
In~\cite{sculley2014high-interest-cc}, Sculley et. al. elegantly present the
challenges with building and productionizing at Google and highlight the need to manage ``pipeline jungles'' and model configurations.
% Two of the problems highlighted in~\cite{sculley2014high-interest-cc} are the 
% technical debt arising from ``pipeline jungles'' and large amount of configurations.
% \mdb takes first steps in addressing these problems by providing a central
% repository where pipelines (i.e., \wfs) and their configurations are stored in
% a standardized format.
% Different companies have since published the architecture of their proprietary
% ML platforms such as the Michelangelo platform at Uber~\cite{michelangelo} and FBLearner at Facebook~\cite{fblearner}.
% These architectures have a model repository as a centerpiece.
% Similarly, in~\cite{schibsted-versioning-pipelines} authors describe a 
% proprietary pipeline versioning system developed to track ML pipelines. 
% Unlike \mdb, this system performs versioning at the pipeline-stage granularity 
% and versions not only the operators used at each stage but also the intermediate
% datasets produced by each stage. 

Model management systems are also closely related to workflow and experiment management systems such as Kepler~\cite{kepler}, Taverna workbench~\cite{taverna},
Galaxy~\cite{galaxy}, VisTrails~\cite{bavoil2005vistrails, callahan2006vistrails, callahan2006workflows} as well as recent workflow engines tailored for data processing such as Apache Airflow~\cite{airflow} and
Luigi~\cite{luigi}.
While workflow systems can address some of the model management needs during the first three phases of the model life-cycle, these systems require extensions to support the deployment and maintenance phases of the ML model life-cycle.
% A hybrid approach is used by new \wf system such as Apache Airflow~\cite{airflow} and
% Luigi~\cite{luigi} where \wfs are defined in code but the steps must follow a uniform interface.
% For example, \wfs in Apache Airflow are DAGs of Tasks where a Task implements the
% common Operator interface.
% Every execution of the DAG creates a Run that is executed with the particular 
% task instances.
% Defining \wfs in code enables \mldevs to use their preferred ML or data processing
% libraries while still making it easy to track and run \wfs.


% A major drawback of VisTrails and most of the scientific workflow systems 
% described above is that they require scientists to use a system-specific \wf
% definition interface (GUI or otherwise) that is separate from their scientific
% development environment.
% This is, in fact, the solution also adopted by some commercial ML systems including
% Microsoft Azure ML~\cite{azure-ml} and the SeaHorse product from DeepSense~\cite{seahorse}.
% From our interviews with dozens of data scientists, however, we learned that 
% \dss found the use of a standalone workflow management systems (particularly 
% GUI-based) extremely restrictive.
% \Dss want the freedom to write workflows in their ML environment of 
% choice and the ability to use new ML techniques without waiting
% for the \wf management system to reflect these updates.
% Moreover, we found that \dss were unwilling to switch to new tools for one piece 
% of functionality (however key the functionality may be).
% Thus we found that while a standalone workflow specification system would be 
% much easier to build, it would have immense difficulty getting adoption with \mldevs. 
% As a result, a significant part of the effort in designing and implementing \mdb
% was directed towards passively collecting \mlwfs without requiring \dss to 
% change their modeling processes.

The rest of the paper is organized as follows.
In Section~\ref{sec:motivation}, we describe the motivation behind model management systems; in Section~\ref{sec:challenges}, we describe the challenges faced in building model management systems; in Section~\ref{sec:modeldb}, we describe the \mdb system developed at MIT.
We conclude the paper in Section~\ref{sec:future} with a discussion of how we see the need for model management evolving in the future.

% A key part of this paper seeks to define what model management means to data science and machine learning practitioners and how that translates to a need for a model management system.
% Models are to ML-based applications what databases are to web-applications \mpv{??}.
% They are key for the correct functioning of these applications.
% Consequently, just like we have database management systems (or DBMSs), we find the need for systems that manage models.
% While a DBMS serves the purpose of storing, querying, and updating data; a model management system serves the purpose of storing metadata about models, optionally the models, enable querying of the metadata and models.
% Unlike database systems, model management systems are more concerned with storing model metadata as opposed to the models themselves.
% The reason is that machine learning models are often extremely diverse and do not follow a fixed schema.
% In this paper, we focus on \mdb, a system we developed at MIT to address the problem of model management while building an ML model.
% We provide a perspective on the challenges in model management and how we see the need for model management evolve in the future.


\section{Need for Model Management}
\label{sec:motivation}

To understand the need for model management, we studied modeling workflows across
many companies and research labs ranging in size and complexity of their machine learning applications.
Across the different phases of the model life-cycle, the need for model management was apparent in three key areas: managing modeling experiments, enabling reproducibility and collaboration, and the need for governance.

The first and primary area where the need for model management is evident is during the Experimentation phase of the model life-cycle.
The empirical nature of machine learning model development means that \dss and \mldevs experiment with hundreds of models before identifying one that meets some acceptance criteria.
This is particularly true when the \dss performs hyperparameter optimization on a model.
Data about previously built \mlwfs is necessary to inform the next set of \mlwfs and experiments, and also simply to identify the best model produced so far.
Consequently, tracking these experiments becomes of paramount importance.
The absence of an experiment tracking system leads to models and experiments being {\it lost} and valuable time and resources being spent in reproducing the models.
For example, one \mldev at a large tech company related how she had spent over a week just re-running a modeling experiment another employee had previously conducted since the experimental setup and results were not recorded.

A second related need for model management becomes evident when a previously built model needs to be reproduced from scratch.
For example, perhaps a new version of a model produces erroneous results and therefore the model must be reverted to an older version.
If the previous model object has been removed, re-creating it requires accurate information about what data was used, how it was processed, libraries and versions used, and details of how the model was trained (including random seeds).
A similar need becomes evident when there is a discrepancy between an offline and live model or when a model has to be updated with new data.
Similarly, when data science teams want to collaborate on a particular model or build on top of each other's work (e.g., use the results of one model as input to another), the lack of a centralized repository of models hampers sharing of information about what models exist and how to integrate a given models in a product or business process.
    
Another requirement for model management arises from the need to provide governance around what models are being used to make automated decisions. 
For non-regulated industries, this governance may only be required to provide the business insight into what models are being used in a product or business.
However, for many regulated industries, particularly in banking and healthcare, government regulations require that any models used to make automated decisions be documented and available for audits. 
Moreover, government legislation now being implemented (e.g., the GDPR regulations in the European Union~\cite{gdpr}) require that companies be able to explain any decisions made without human intervention, 
making it essential to record all models used across a business and provide complete provenance information about how the model was generated and used.


\section{Challenges in Model Management}
\label{sec:challenges}

As defined above, model management covers the entire life-cycle of models starting with data-processing, feature engineering, model experimentation, deployment, and maintenance.
Across all the above stages, {\it diversity} and {\it heterogeneity} in ML environments and frameworks are the key challenges in consistently capturing metadata across the modeling lifecycle.

For example, during data processing (or feature engineering), we want to track the
transformations applied to data so that they can be accurately reproduced later.
% features that are created and used in a model.
Since feature processing may be done in very different languages and compute environments (e.g., Spark, Teradata, HBase), ensuring that data processing can be accurately recorded across these environments is challenging.
Moreover, even within a single environment, ensuring high coverage for all the ways in which a data transformation may be applied (e.g., applying a one-hot-encoding operation in pandas) is challenging unless this functionality has been built into the system from the ground-up.

Similarly, during the experimentation phase, there is a large diversity in machine learning environments and libraries in different languages (e.g., scikit-learn, Tensorflow, PyTorch in Python; R; H2O framework in Java).
Each library has a unique way of defining models (e.g., graphs in Tensorflow vs. plain objects in scikit-learn) and their associated attributes.
Consequently, one representation cannot capture models built across all frameworks.
The diversity in machine learning frameworks also translates to diversity in deployment methods for each model.
For example, while TF-serving is a popular means to serve Tensorflow models, a Flask-based deployment method is most popular for scikit-learn models.
Capturing the relevant deployment properties for the above methods therefore also becomes a challenge.

Once a model is deployed, while some properties are common to all models (e.g., latency of predictions, number of requests), many properties are application dependent (e.g., application-specific metrics to be monitored) and might require input from disparate systems (e.g., prediction storage systems).
As a result, one generic solution is unlikely to meet the requirements for all applications and will instead requires extensible code.

In addition to the challenges introduced due to the diversity of ML environments, two overarching requirements emerge from requirements for model management: first, \dss want a model management solution that minimizes the developer intervention and requires minimal changes to the current developer workflow.
For example, many \dss are (understandably) unwilling to choose a new ML environment or significantly change their modeling workflow to account for model management.
Second, \dss want a {\it vendor-neutral} model management system so that users are not tied into one particular provider or framework and can instead focus on their modeling task.

\section{\mdb}
\label{sec:modeldb}

Building an ML model for real-world applications is an iterative process. 
\Dss and \mldevs experiment with tens to hundreds of models before identifying 
one that meets some acceptance criteria on model performance.
 % (e.g., mean squared error of predictions should be $<$0.05).
For example, top competitors in the Kaggle competition to predict Zillow Home prices~\cite{zillow} made more than 250 submissions (and therefore built at least as many models), while those in the Toxic Comment classification competition~\cite{toxic-comments} made over 400 submissions.
As an example of actual experimentation performed during model building, Listing~\ref{lst:versioning_2} reproduces code comments by an expert Kaggle competitor (ranked 500) written in order to track models built for the Zillow Price Prediction Challenge.

\begin{figure}[tb] 
\centerline{
\includegraphics[trim=0mm 0mm 0mm 0mm,
width=0.85\textwidth,clip=true]{figs/listing.png}}
\caption{Model Versioning comments by Kaggle competitor}
\label{lst:versioning_2}
\end{figure}

% \lstset{basicstyle=\tiny}
% \begin{lstlisting}[basicstyle=\small\ttfamily,caption={Model versioning comments by a Kaggle competitor},label={lst:versioning_2},language=Python]
% # version 61
% #   Drop fireplacecnt and fireplaceflag, following Jayaraman:
% #     https://www.kaggle.com/valadi/xgb-w-o-outliers-lgb-with-outliers-combo-tune5

% # version 60
% #   Try BASELINE_PRED=0.0115, since that's the actual baseline from
% #     https://www.kaggle.com/aharless/oleg-s-original-better-baseline

% # version 59
% #   Looks like 0.0056 is the optimum BASELINE_WEIGHT

% # versions 57, 58
% #   Playing with BASELINE_WEIGHT parameter:
% #     3 values will determine quadratic approximation of optimum

% ...

% # version 49
% #   My latest quadratic approximation is concave, so I'm just taking
% #     a shot in the dark with lgb_weight=.3

% # version 45
% #   Increase lgb_weight to 0.25 based on new quadratic approximation.
% #   Based on scores for versions 41, 43, and 44, the optimum is 0.261
% #     if I've done the calculations right.
% #   I'm being conservative and only going 2/3 of the way there.
% #   (FWIW my best guess is that even this will get a worse score,
% #    but you gotta pay some attention to the math.)

% # version 44
% #   Increase lgb_weight to 0.23, per Nikunj's suggestion, even though
% #     my quadratic approximation said I was already at the optimum
% \end{lstlisting}

% # version 55
% #   OK, it doesn't get the same result, but I also get a different result
% #     if I fork the earlier version and run it again.
% #   So something weird is going on (maybe software upgrade??)
% #   I'm just going to submit this version and make it my new benchmark.

% # version 53
% #   Re-parameterize ensemble (should get same result).

% # version 51
% #   Quadratic approximation based on last 3 submissions gives 0.3533
% #     as optimal lgb_weight.  To be slightly conservative,
% #     I'm rounding down to 0.35

% # version 50
% #   Quadratic approximation based on last 3 submissions gives 0.3073 
% #     as optimal lgb_weight

As we can see from this listing, a \ds typically tests a large number of model versions before identifying the best one.
Moreover, although \dss (and data science teams) build many tens to hundreds of models
when developing an ML application, they currently have no way to keep track of all 
the models they have built.
Consequently, insights are lost, models cannot be reproduced, collaboration becomes challenging, and model governance becomes challenging.

To address these challenges, we developed a system at MIT called \mdb~\cite{modeldb-hilda}. 
\mdb is the first open-source machine learning model management system and currently focuses on tracking models during the experimentation phase.
\mdb automatically tracks models as they are built, records 
provenance information for each step in the pipeline used to generate the model, stores this 
data in a standard format, makes it available for querying via an API and
a visual interface.
In the following sections, we describe the architecture of \mdb, how data may be logged into the system and different ways of querying the data.
We finish with a brief discussion of current work on extending \mdb.

% To address the problem of \mlwf management, 
% we propose a novel system called \mdb.
% \mdb is a centralized repository of models that tracks what models were built, how they were built (including pre-processing steps), and how they performed.
% To provide this functionality, 

% in their native environment (e.g., spark.ml or scikit-learn), indexes them intelligently, and allows flexible exploration of models via an API as well as a visual interface. 

\subsection{\mdb Architecture}

Figure \ref{fig:mdb_arch} shows the high-level architecture of our system. 
\mdb consists of three key components: native client libraries for different machine learning environments, a backend that stores
model data, and a web-based visualization interface. 
Client libraries are responsible for automatically extracting models and pipelines from code
and passing them to the \mdb backend. 
\mdb client libraries are currently available for scikit-learn, spark.ml, and along with a {\it Light} Python API that can be used in any Python-based machine learning environment.
This means that \mldevs can continue to build models and perform 
experimentation these environments while the native libraries
passively capture \wf information.
The \mdb backend exposes a thrift\footnote{https://thrift.apache.org/} 
interface to allow clients in different 
languages to communicate with the \mdb backend. 
% \mdb stores models and pipelines as a sequence of actions (as opposed to states) and uses a branching model of history to track the changes in models over time~\cite{Derthick01enhancingdata}. \mpv{FIX}
%Together, these abstractions can provide a chronicle of the users' model building operations in a format that can be easily shared, queried and visualized. 
The backend can use a variety of storage systems to store the model metadata.
% For ease of implementation, we chose to store \mdb generated model data in a
% relational database whereas for model metadata that can have a flexible schema 
% (e.g., user-specified key-value pairs or Light API specifications), we chose to use a key-value store.
The third component of \mdb, the visual interface, provides an easy-to-navigate layer on top of the backend storage system that permits visual exploration and analyses of model data.

\begin{figure}[tb] 
\centerline{
\hbox{\resizebox{10cm}{!}{\includegraphics[trim=0mm 0mm 0mm 0mm,
clip=true]{figs/mdb_arch.png}}}}
\caption{ModelDB Architecture}
\label{fig:mdb_arch}
\end{figure}

\subsection{Client Libraries}
Many existing workflow management programs (e.g. VisTrails~\cite{callahan2006vistrails}) require that the user create a workflow in advance, usually by means of a GUI. 
However, the \dss we interviewed overwhelmingly concurred that GUIs restricted their flexibility in defining pipelines and that it was difficult to specify pipelines beforehand because they changed constantly. 
Moreover, we found that \dss were unwilling to change their preferred ML environment for a workflow management system. Therefore, our primary design constraint while creating the \mdb client libraries was to minimize any changes the \ds would need to make both to code and the existing modeling process. To meet this constraint, we chose to make model logging accessible directly through code (as opposed to a GUI) and to build native logging libraries for different ML environment. The spark.ml and scikit-learn libraries are architected such that \dss can use the environments for analysis exactly as they normally would and the library transparently and automatically logs the relevant data to the backend.

\subsection{\mdb Frontend}
\label{sec:mdb_frontend}

\mdb captures a large amount of metadata about models.
To support easy access to this data, \mdb provides a visual interface.
We provide three key views for exploring the data stored in \mdb.
A user starts with the projects summary page (Fig.~\ref{fig:project-summary}) 
that provides a high level overview of all the projects in the system.
The user can then click on a particular project to see the \wfs for that
project.
We present models via two key views; the first view presents two visualizations, a timeline visualization (Fig.~\ref{fig:mdb_model_timeline}) showing the evolution of the model metrics over time as well as a custom visualization interface for model meta-analyses.
The second view is a tabular view of all the models in a particular project (Fig.~\ref{fig:tabular-view}) along with interactions such as filtering, sorting, grouping, and search.
From any of the above interfaces, the \mldev can drill-down into a single model to visualize the model pipeline that was automatically inferred by the client library.
% (Fig.~\ref{fig:mdb_model_pipeline}) 

\begin{figure*}
  \centering
  \includegraphics[trim=0mm 0mm 0mm 0mm,
  clip=true, width=\textwidth]{figs/mdb_dashboard.png}
  \caption{\label{fig:project-summary} Projects Summary View}
\end{figure*}

% \begin{figure*}
% \centering
% \begin{subfig}
%   \centering
%   \includegraphics[width=0.48\textwidth]{figs/mdb_tabular.png}
% \end{subfig}%
% \begin{subfigure}
%   \centering
%   \includegraphics[width=0.48\textwidth]{figs/mdb_model_pipeline.png}
% \end{subfigure}%
% \caption{\zillow pipelines}
% \label{fig:storage_sizes_trad}
% \end{figure*}

\begin{figure*}
  \centering
  \includegraphics[trim=0mm 0mm 0mm 0mm,
  clip=true, width=0.8\textwidth]{figs/mdb_model_timeline.png}
  \caption{\label{fig:mdb_model_timeline} Timeline Visualization}
\end{figure*}

% \begin{figure*}
%   \centering
%   \includegraphics[trim=0mm 0mm 0mm 0mm,
%   clip=true, width=\textwidth]{figures/mdb_model_timeline_drilldown.png}
%   \caption{\label{fig:mdb_model_timeline_drilldown} Model Timeline Drilldown 
%   (reproduced from~\cite{weiwei})}
% \end{figure*}

% \begin{figure*}
%   \centering
%   \includegraphics[trim=0mm 0mm 0mm 0mm,
%   clip=true, width=\textwidth]{figures/mdb_custom_visualizations.png}
%   \caption{\label{fig:mdb_custom_visualizations} Custom Visualizations
%   (reproduced from~\cite{weiwei})}
% \end{figure*}



\begin{figure*}
  \centering
  \includegraphics[trim=0mm 0mm 0mm 0mm,
  clip=true, width=0.8\textwidth]{figs/mdb_tabular.png}
  \caption{\label{fig:tabular-view} Models View}
\end{figure*}

% \begin{figure*}
%   \centering
%   \includegraphics[trim=0mm 0mm 0mm 0mm,
%   clip=true, width=\textwidth]{figures/mdb_model_filtering.png}
%   \caption{\label{fig:mdb_model_filtering} Model Filtering
%   (reproduced from~\cite{weiwei})}
% \end{figure*}

% \begin{figure*}
%   \centering
%   \includegraphics[trim=0mm 0mm 0mm 0mm,
%   clip=true, width=0.75\textwidth]{figs/mdb_model_pipeline.png}
%   \caption{\label{fig:mdb_model_pipeline} Model Pipeline View}
% \end{figure*}

\subsection{\mdb Adoption and Future Work}

We officially released \mdb as an open-source model management system in 
Feb. 2017 and since then there has been a large amount of interest and adoption of \mdb.
Specifically, over the last year, our GitHub repository~\cite{mdb-repo} 
has garnered $>$ 500 stars, has been cloned over a thousand times, and has been
forked $>$100 times.
\mdb has been tested at multiple small and large companies, and is deployed in
various settings.
\mdb has also served as inspiration for other model management systems such as~\cite{mlflow}.

Based on the feedback from current \mdb users and broader data science needs, we are currently expanding \mdb to capture metadata not only from the Experimentation phase, but also the Deployment and Monitoring phases.

\section{Evolution of Model Management Systems}
\label{sec:future}

Current model management systems have focused on keeping track of modeling experiments and, to some extent, keeping track of deployed model versions. 
However, as machine learning models proliferate into every key business process and product, we posit that the task of managing the life-cycle of models will become as key as managing the life-cycle of code.
Just as version control systems such as SVN and Git made source code development and collaboration more robust, we envision that model management systems will serve a similar purpose in the future.
We imagine model management systems to become the system of record for all models and model-related information. 
Consequently, we expect model management systems to evolve in the following directions.

{\bf Model Data Pipelines.} For many machine learning applications, the ultimate performance of the machine learning model depends on the features or data attributes used by the model.
As a result, when a model is to be reproduced, it requires accurate records of the data and transformations used to produce features.
We expect model management systems to evolve to accurately record data versions (train and test) as well data transformations that are used to generate features.
This metadata may come from a separate data processing or workflow system as opposed to being generated by the model management system, however, the model management system would track all the metadata required to create the model end-to-end.

{\bf Model Interoperability.} One of the key challenges in model management described before is the diversity in ML models and frameworks.
While some frameworks support rapid development, others might be more suitable for deployment in production settings.
Given the rapid proliferation of ML frameworks, we expect model management systems to support, if not provide, a level of interoperability between different ML environments and frameworks (e.g., PMML~\cite{pmml} and ONNX~\cite{onnx} provide a good start).
This will enable \dss to pick and choose the best framework for each phase of the model life-cycle.

{\bf Model Testing.} As more decisions and business logic get delegated to machine learning models, the importance of testing models (similar to code testing) will increase and will become a key part of managing the model life-cycle.
For instance, defining unit tests for models and their input data will become commonplace.
Similarly, we expect integration tests with model outputs to become more prevalent as model predictions get used as input for other models.
Finally, we note that as adversarial attacks on models increase, testing of models and edge cases will become key, requiring the development of new techniques to prevent adversarial attacks (e.g., ~\cite{adversarialICLR2015
,feinman2017detectAdversarial}). 

{\bf Model Monitoring.} While model testing takes place before a model is deployed, model monitoring takes place once the model is deployed in a live system.
Model monitoring today is largely limited to monitoring system-level metrics such as the number of prediction requests, latency, and compute usage.
However, we are already seeing the need for data-level monitoring of models (e.g., as noted in~\cite{sculley2014high-interest-cc}) to ensure that the offline and live data fed to a model is similar.
We expect model management systems to encompass model monitoring modules to ensure continued model health, triggering alerts and actions as appropriate.

{\bf Model Interpretability and Fairness.} 
As models are used for automated decision making in regulated industries and increasingly used by non-technical users, explaining the results of models will become a key aspect of the management of deployed models (as evidenced by the rich research on interpretability~\cite{shap,DoshiKim2017Interpretability,svcca}).
We view a model management system as the system of record for all models and, therefore, the logical gateway for model interpretability and understanding.
In the future, we therefore expect model management systems to expose interpretability functionality for every recorded model.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we discussed the need for model management systems that track model metadata throughout the model life-cycle.
We described \mdb, an open-source model management system developed at MIT that focuses on the Experimentation phase of the model life-cycle and is being extended to Deployment and Maintenance.
Finally, we described the challenges and opportunities in model management in the future.

\begin{thebibliography}{10} 
\itemsep=1pt 
\begin{small}
\bibitem{bavoil2005vistrails}
Louis Bavoil, Steven~P Callahan, Patricia~J Crossno, Juliana Freire, Carlos~E
  Scheidegger, Cl{\'a}udio~T Silva, and Huy~T Vo.
\newblock Vistrails: Enabling interactive multiple-view visualizations.
\newblock In {\em Visualization, 2005. VIS 05. IEEE}, pages 135--142. IEEE,
  2005.

\bibitem{callahan2006workflows}
Steven~P Callahan, Juliana Freire, Emanuele Santos, Carlos~E Scheidegger,
  Claudio~T Silva, and Huy~T Vo.
\newblock Managing the evolution of dataflows with vistrails.
\newblock In {\em Data Engineering Workshops, 2006. Proceedings. 22nd
  International Conference on}, pages 71--71. IEEE, 2006.

\bibitem{callahan2006vistrails}
Steven~P Callahan, Juliana Freire, Emanuele Santos, Carlos~E Scheidegger,
  Cl{\'a}udio~T Silva, and Huy~T Vo.
\newblock Vistrails: visualization meets data management.
\newblock In {\em Proceedings of the 2006 ACM SIGMOD international conference
  on Management of data}, pages 745--747. ACM, 2006.

\bibitem{toxic-comments}
Kaggle competition.
\newblock Toxic comment classification challenge.
\newblock
  \url{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge}.

\bibitem{zillow}
Kaggle Competition.
\newblock Zillow prize: Zillowâ€™s home value prediction (zestimate).
\newblock \url{https://www.kaggle.com/c/zillow-prize-1}.

\bibitem{DoshiKim2017Interpretability}
Been Doshi-Velez, Finale;~Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock In {\em eprint arXiv:1702.08608}, 2017.

\bibitem{fblearner}
Facebook Engineering.
\newblock Introducing fblearner flow: Facebook's ai backbone.
\newblock
  \url{https://code.facebook.com/posts/1072626246134461/introducing-fblearner-flow-facebook-s-ai-backbone/}.

\bibitem{michelangelo}
Uber Engineering.
\newblock Meet michelangelo: Uber's machine learning platform.
\newblock \url{https://eng.uber.com/michelangelo/}.

\bibitem{feinman2017detectAdversarial}
Reuben Feinman, Ryan~R Curtin, Saurabh Shintre, and Andrew~B Gardner.
\newblock Detecting adversarial samples from artifacts.
\newblock {\em arXiv preprint}, 2017.

\bibitem{airflow}
Apache~Software Foundation.
\newblock Airflow.

\bibitem{mdb-repo}
MIT~DB Group.
\newblock Modeldb.
\newblock \url{https://github.com/mitdbg/modeldb}, 2017.

\bibitem{pmml}
Alex Guazzelli, Wen-Ching Lin, and Tridivesh Jena.
\newblock {\em PMML in Action: Unleashing the Power of Open Standards for Data
  Mining and Predictive Analytics}.
\newblock CreateSpace, Paramount, CA, 2010.

\bibitem{hellerstein2017ground}
Joseph~M Hellerstein, Vikram Sreekanti, Joseph~E Gonzalez, James Dalton, Akon
  Dey, Sreyashi Nag, Krishna Ramachandran, Sudhanshu Arora, Arka Bhattacharyya,
  Shirshanka Das, et~al.
\newblock Ground: A data context service.

\bibitem{msms}
Arun Kumar, Robert McCann, Jeffrey Naughton, and Jignesh~M. Patel.
\newblock Model selection management systems: The next frontier of advanced
  analytics.
\newblock {\em SIGMOD Rec.}, 44(4):17--22, May 2016.

\bibitem{kepler}
Bertram Lud{\"a}scher, Ilkay Altintas, Chad Berkley, Dan Higgins, Efrat Jaeger,
  Matthew Jones, Edward~A Lee, Jing Tao, and Yang Zhao.
\newblock Scientific workflow management and the kepler system.
\newblock {\em Concurrency and Computation: Practice and Experience},
  18(10):1039--1065, 2006.

\bibitem{shap}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pages
  4768--4777. Curran Associates, Inc., 2017.

\bibitem{modelhub}
H.~Miao, A.~Li, L.~S. Davis, and A.~Deshpande.
\newblock Modelhub: Deep learning lifecycle management.
\newblock In {\em 2017 IEEE 33rd International Conference on Data Engineering
  (ICDE)}, pages 1393--1394, April 2017.

\bibitem{provdb}
Hui Miao, Amit Chavan, and Amol Deshpande.
\newblock Provdb: Lifecycle management of collaborative analysis workflows.
\newblock 2017.

\bibitem{mlflow}
MLFlow.
\newblock Mlflow.
\newblock \url{https://github.com/mlflow/mlflow}.

\bibitem{gdpr}
Official~Journal of~the European~Union.
\newblock General data protection regulation.
\newblock
  \url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32016R0679}.

\bibitem{onnx}
ONNX.
\newblock Onnx: Open neural network exchange.
\newblock \url{https://github.com/onnx/onnx}.

\bibitem{svcca}
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 6078--6087. Curran Associates, Inc., 2017.

\bibitem{sas-model-manager}
SAS.
\newblock Sas model manager.
\newblock \url{https://www.sas.com/en_us/software/model-manager.html}.

\bibitem{sculley2014high-interest-cc}
D~Sculley, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, and Michael Young.
\newblock Machine learning: The high-interest credit card of technical debt.

\bibitem{luigi}
Spotify.
\newblock Luigi.
\newblock \url{https://github.com/spotify/luigi}.

\bibitem{galaxy}
The~Galaxy Team and Community.
\newblock The galaxy project.
\newblock \url{https://galaxyproject.org/}.

\bibitem{modeldb-hilda}
Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah
  Husnoo, Samuel Madden, and Matei Zaharia.
\newblock Modeldb: A system for machine learning model management.
\newblock In {\em Proceedings of the Workshop on Human-In-the-Loop Data
  Analytics}, HILDA '16, pages 14:1--14:3, New York, NY, USA, 2016. ACM.

\bibitem{taverna}
Katherine Wolstencroft, Robert Haines, Donal Fellows, Alan Williams, David
  Withers, Stuart Owen, Stian Soiland-Reyes, Ian Dunlop, Aleksandra Nenadic,
  Paul Fisher, et~al.
\newblock The taverna workflow suite: designing and executing workflows of web
  services on the desktop, web or in the cloud.
\newblock {\em Nucleic acids research}, 41(W1):W557--W561, 2013.


\end{small}
\end{thebibliography}
\end{document}